"""
Training data services for DeepEarth ML pipelines.

Provides efficient batch loading of multimodal biodiversity data for PyTorch training.
Supports both direct Python import and Flask API integration for maximum flexibility.

    üî¨ Research Data ‚îÄ‚îÄ‚ñ∫ üß† ML Tensors ‚îÄ‚îÄ‚ñ∫ üöÄ Training Pipeline
    
    OBSERVATION_ID ‚Üí Species, Images, Coordinates, Time, Embeddings
"""

import logging
import numpy as np
import torch
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime
import pandas as pd

logger = logging.getLogger(__name__)


def get_training_batch(
    cache, 
    observation_ids: List[str],
    include_vision: bool = True,
    include_language: bool = True,
    device: str = 'cpu'
) -> Dict[str, Any]:
    """
    üéØ Core Training Data Pipeline
    
    Efficiently loads multimodal training data for a batch of observations.
    Returns PyTorch-ready tensors optimized for ML training workflows.
    
    Args:
        cache: UnifiedDataCache instance for data access
        observation_ids: List of OBSERVATION_IDs in format "{gbif_id}_{image_idx}"  
        include_vision: Whether to load vision embeddings (8√ó24√ó24√ó1408)
        include_language: Whether to load language embeddings (S√ó7168)
        device: PyTorch device for tensor placement ('cpu', 'cuda')
        
    Returns:
        Dict containing:
            'species': List[str] - Latin species names
            'image_urls': List[str] - Direct image URLs  
            'locations': torch.Tensor - GPS coordinates [N, 3] (lat, lng, elevation)
            'timestamps': torch.Tensor - Time data [N, 6] (year, month, day, hour, min, sec)
            'language_embeddings': torch.Tensor - Species embeddings [N, S, 7168] 
            'vision_embeddings': torch.Tensor - Vision embeddings [N, 8, 24, 24, 1408]
            'metadata': Dict - Additional metadata for debugging
            
    Raises:
        ValueError: If observation_ids format is invalid or data is missing
        RuntimeError: If batch loading fails
    """
    logger.info(f"Loading training batch: {len(observation_ids)} observations")
    
    # Parse OBSERVATION_IDs to extract GBIF IDs and image indices
    gbif_ids, image_indices = _parse_observation_ids(observation_ids)
    
    # Load observations data for the batch
    observations = _load_observations_batch(cache, gbif_ids)
    
    # Initialize result containers
    batch_data = {
        'species': [],
        'image_urls': [],
        'locations': [],
        'timestamps': [],
        'metadata': {
            'observation_ids': observation_ids,
            'gbif_ids': gbif_ids,
            'batch_size': len(observation_ids),
            'loaded_at': datetime.now().isoformat()
        }
    }
    
    # Process each observation in batch
    for i, (obs_id, gbif_id, img_idx) in enumerate(zip(observation_ids, gbif_ids, image_indices)):
        
        obs_row = observations[observations['gbif_id'] == gbif_id].iloc[0]
        
        # 1. Species (Latin name)
        species_name = obs_row['taxon_name']
        batch_data['species'].append(species_name)
        
        # 2. Image URL (specific image for this OBSERVATION_ID)
        image_urls = obs_row['image_urls']
        if isinstance(image_urls, np.ndarray) and len(image_urls) >= img_idx:
            image_url = image_urls[img_idx - 1]  # img_idx is 1-based
        else:
            image_url = image_urls[0] if len(image_urls) > 0 else ""
        batch_data['image_urls'].append(image_url)
        
        # 3. Location (Latitude, Longitude, Elevation)
        lat = float(obs_row['latitude'])
        lng = float(obs_row['longitude']) 
        # Note: Elevation not currently in dataset, defaulting to 0.0
        elevation = 0.0  # TODO: Add elevation data when available
        batch_data['locations'].append([lat, lng, elevation])
        
        # 4. Time (Year, Month, Day, Hour, Minute, Second)
        timestamp = [
            int(obs_row.get('year', 0)),
            int(obs_row.get('month', 0)), 
            int(obs_row.get('day', 0)),
            int(obs_row.get('hour', 0)),
            int(obs_row.get('minute', 0)),
            int(obs_row.get('second', 0))
        ]
        batch_data['timestamps'].append(timestamp)
    
    # Convert lists to tensors
    batch_data['locations'] = torch.tensor(batch_data['locations'], dtype=torch.float32, device=device)
    batch_data['timestamps'] = torch.tensor(batch_data['timestamps'], dtype=torch.int32, device=device)
    
    # 5. Species embeddings (Language)
    if include_language:
        batch_data['language_embeddings'] = _load_language_embeddings_batch(
            cache, observations, gbif_ids, device
        )
    
    # 6. Vision embeddings
    if include_vision:
        batch_data['vision_embeddings'] = _load_vision_embeddings_batch(
            cache, observations, gbif_ids, device
        )
    
    logger.info(f"Successfully loaded batch with {len(batch_data['species'])} observations")
    return batch_data


def create_observation_id(gbif_id: int, image_index: int = 1) -> str:
    """
    üè∑Ô∏è OBSERVATION_ID Creation
    
    Creates standardized OBSERVATION_ID from GBIF ID and image index.
    Format: "{gbif_id}_{image_idx}" for human-readable identification.
    
    Args:
        gbif_id: GBIF occurrence identifier
        image_index: Image number for this observation (1-based)
        
    Returns:
        str: Formatted OBSERVATION_ID
    """
    return f"{gbif_id}_{image_index}"


def parse_observation_id(observation_id: str) -> Tuple[int, int]:
    """
    üîç OBSERVATION_ID Parsing
    
    Extracts GBIF ID and image index from OBSERVATION_ID string.
    
    Args:
        observation_id: Formatted OBSERVATION_ID string
        
    Returns:
        Tuple[int, int]: (gbif_id, image_index)
        
    Raises:
        ValueError: If observation_id format is invalid
    """
    try:
        parts = observation_id.split('_')
        if len(parts) != 2:
            raise ValueError(f"Invalid OBSERVATION_ID format: {observation_id}")
        
        gbif_id = int(parts[0])
        image_index = int(parts[1])
        
        if image_index < 1:
            raise ValueError(f"Image index must be >= 1, got: {image_index}")
            
        return gbif_id, image_index
        
    except (ValueError, IndexError) as e:
        raise ValueError(f"Failed to parse OBSERVATION_ID '{observation_id}': {e}")


def get_available_observation_ids(
    cache,
    has_vision: bool = True,
    has_language: bool = True,
    limit: Optional[int] = None
) -> List[str]:
    """
    üìã Available Observations Discovery
    
    Returns list of all valid OBSERVATION_IDs in the dataset that meet criteria.
    Useful for train/test split generation and batch sampling.
    
    Args:
        cache: UnifiedDataCache instance
        has_vision: Only include observations with vision embeddings
        has_language: Only include observations with language embeddings  
        limit: Maximum number of observation_ids to return
        
    Returns:
        List[str]: Available OBSERVATION_IDs
    """
    observations = cache.load_observations()
    
    # Apply filters
    if has_vision:
        observations = observations[observations['has_vision'] == True]
    
    if has_language:
        # Language embeddings should be available for all observations
        observations = observations[observations['language_embedding'].notna()]
    
    observation_ids = []
    
    for _, row in observations.iterrows():
        gbif_id = row['gbif_id']
        
        # For now, assume 1 image per observation (can extend later)
        # TODO: Parse actual number of images when multi-image support is added
        num_images = 1
        
        for img_idx in range(1, num_images + 1):
            obs_id = create_observation_id(gbif_id, img_idx)
            observation_ids.append(obs_id)
            
            if limit and len(observation_ids) >= limit:
                break
        
        if limit and len(observation_ids) >= limit:
            break
    
    logger.info(f"Found {len(observation_ids)} available observation IDs")
    return observation_ids


def _parse_observation_ids(observation_ids: List[str]) -> Tuple[List[int], List[int]]:
    """Parse batch of OBSERVATION_IDs into GBIF IDs and image indices."""
    gbif_ids = []
    image_indices = []
    
    for obs_id in observation_ids:
        gbif_id, img_idx = parse_observation_id(obs_id)
        gbif_ids.append(gbif_id)
        image_indices.append(img_idx)
    
    return gbif_ids, image_indices


def _load_observations_batch(cache, gbif_ids: List[int]) -> pd.DataFrame:
    """Load observations DataFrame for batch of GBIF IDs."""
    observations = cache.load_observations()
    
    # Filter to requested GBIF IDs
    batch_obs = observations[observations['gbif_id'].isin(gbif_ids)]
    
    if len(batch_obs) != len(set(gbif_ids)):
        missing_ids = set(gbif_ids) - set(batch_obs['gbif_id'].values)
        logger.warning(f"Missing observations for GBIF IDs: {missing_ids}")
    
    return batch_obs


def _load_language_embeddings_batch(
    cache, 
    observations: pd.DataFrame, 
    gbif_ids: List[int],
    device: str
) -> torch.Tensor:
    """Load language embeddings for batch of observations."""
    embeddings = []
    
    for gbif_id in gbif_ids:
        obs_row = observations[observations['gbif_id'] == gbif_id].iloc[0]
        lang_emb = obs_row['language_embedding']
        
        if isinstance(lang_emb, np.ndarray):
            embeddings.append(lang_emb)
        else:
            # Fallback: load via taxon_id
            taxon_id = obs_row['taxon_id']
            lang_emb = cache.get_language_embedding(taxon_id)
            embeddings.append(lang_emb)
    
    # Stack into batch tensor [N, 7168]
    # Note: For now assuming single token per species. Multi-token support can be added later.
    embeddings_tensor = torch.tensor(np.stack(embeddings), dtype=torch.float32, device=device)
    
    return embeddings_tensor


def _load_vision_embeddings_batch(
    cache, 
    observations: pd.DataFrame,
    gbif_ids: List[int],
    device: str
) -> torch.Tensor:
    """Load vision embeddings for batch of GBIF IDs using optimized batch loading."""
    embeddings = []
    errors_count = 0
    missing_count = 0
    
    # Use optimized batch loading if available
    if hasattr(cache, 'mmap_loader') and cache.mmap_loader:
        try:
            # Single optimized batch query
            batch_embeddings = cache.mmap_loader.get_vision_embeddings_batch(gbif_ids)
            
            for gbif_id in gbif_ids:
                if gbif_id in batch_embeddings:
                    # Reshape to 4D: [8, 24, 24, 1408]
                    vision_emb_4d = cache.loader.reshape_vision_embedding(batch_embeddings[gbif_id])
                    embeddings.append(vision_emb_4d)
                else:
                    # Create zero tensor for missing embeddings
                    zero_emb = np.zeros((8, 24, 24, 1408), dtype=np.float32)
                    embeddings.append(zero_emb)
                    missing_count += 1
                    
            # Log summary for batch method
            if missing_count > 0:
                success_count = len(gbif_ids) - missing_count
                logger.info(f"Batch vision embedding summary: {success_count}/{len(gbif_ids)} successful, {missing_count} missing")
                
        except Exception as e:
            logger.error(f"Batch loading failed, falling back to individual loading: {e}")
            # Fall back to individual loading
            return _load_vision_embeddings_individual(cache, observations, gbif_ids, device)
    else:
        # Fall back to individual loading if no mmap loader
        return _load_vision_embeddings_individual(cache, observations, gbif_ids, device)
    
    # Stack into batch tensor [N, 8, 24, 24, 1408]
    embeddings_tensor = torch.tensor(np.stack(embeddings), dtype=torch.float32, device=device)
    
    return embeddings_tensor


def _load_vision_embeddings_individual(
    cache, 
    observations: pd.DataFrame,
    gbif_ids: List[int],
    device: str
) -> torch.Tensor:
    """Individual vision embedding loading (fallback method)."""
    embeddings = []
    errors_count = 0
    missing_count = 0
    
    for i, gbif_id in enumerate(gbif_ids):
        try:
            # Get taxon_id for this observation
            obs_row = observations[observations['gbif_id'] == gbif_id].iloc[0]
            taxon_id = obs_row['taxon_id']
            
            # Load raw embedding with both required parameters
            vision_emb = cache.get_vision_embedding(gbif_id, taxon_id)
            
            if vision_emb is not None:
                # Reshape to 4D: [8, 24, 24, 1408]
                vision_emb_4d = cache.loader.reshape_vision_embedding(vision_emb)
                embeddings.append(vision_emb_4d)
            else:
                # Create zero tensor for missing embeddings
                zero_emb = np.zeros((8, 24, 24, 1408), dtype=np.float32)
                embeddings.append(zero_emb)
                missing_count += 1
                # Rate-limit missing embedding warnings
                if missing_count <= 3:
                    logger.warning(f"Missing vision embedding for GBIF ID {gbif_id}, using zeros")
                elif missing_count == 4:
                    logger.warning(f"Additional missing embeddings detected, suppressing further warnings for this batch...")
        
        except Exception as e:
            errors_count += 1
            # Rate-limit error messages to prevent log flooding
            if errors_count <= 3:
                logger.error(f"Failed to load vision embedding for GBIF ID {gbif_id}: {e}")
            elif errors_count == 4:
                logger.error(f"Multiple embedding errors detected, suppressing further error messages for this batch...")
            
            # Create zero tensor as fallback
            zero_emb = np.zeros((8, 24, 24, 1408), dtype=np.float32)
            embeddings.append(zero_emb)
        
        # Progress indicator for large batches
        if len(gbif_ids) > 32 and (i + 1) % 16 == 0:
            success_rate = ((len(gbif_ids) - errors_count - missing_count) / len(gbif_ids)) * 100
            logger.info(f"Vision embedding progress: {i+1}/{len(gbif_ids)} ({success_rate:.1f}% success rate)")
    
    # Summary logging
    if errors_count > 0 or missing_count > 0:
        success_count = len(gbif_ids) - errors_count - missing_count
        logger.warning(f"Vision embedding individual summary: {success_count}/{len(gbif_ids)} successful, "
                      f"{missing_count} missing, {errors_count} errors")
    
    # Stack into batch tensor [N, 8, 24, 24, 1408]
    embeddings_tensor = torch.tensor(np.stack(embeddings), dtype=torch.float32, device=device)
    
    return embeddings_tensor