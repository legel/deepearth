# Pix4D to NerfStudio with Ecodash Data

The primary goal of this document is to detail the current strategy and lessons learned in moving Pix4D data to Nerfstudio. Not everything is currently working and this document will be updated over time.

## Current Workflow

1. **Preprocessing:**

    a. We assume data has been collected from multiple sources (e.g., drone imagery in different passes with different parameters alongside handheld imagery).

    b. We assume that the data has been processed in Pix4D, which does a global alignment of the images using RTK GNSS and image matching. In this optimization, Pix4D (like other structure-from-motion approaches) generates a point cloud.

    c. We export the Pix4D data to the Open Photogrammetry Format (OPF, https://github.com/Pix4D/opf-spec) using the Pix4D export tool.

2. **Format conversion:**

    a. We then use the python library `pyopf` (https://github.com/Pix4D/pyopf) from Pix4D to convert the OPF data into a format that Nerfstudio can understand (primarily based on the `colmap` directory structure).

    ```bash
    pip install pyopf
    opf2nerf project.opf --out-dir out_dir/ --nerfstudio
    ```

    b. Currently, nerfstudio cannot handle this much data. So, we form a "lite" version of the dataset by selecting a subset of images. This lite version only contains one trace and the camera intrinsic parameters are assumed constant (which is not true in reality, but is a simplification for the default Nerfstudio models that we are looking to remove in the near future).

    ```bash
    python make_lite_transforms.py path/to/transforms.json images/subfolder
    ```

    such as

    ```bash
    python make_lite_transforms.py dixhite_nerfstudio_lite/transforms.json images/18b6eeedec0ad1afdb2a4c6194a1cb12
    ```

3. **Run the Nerfstudio Engine:**

    a. The two main models used are `splatfacto` (i.e., the default 3D Gaussian Splatting model) and `nerfacto` (i.e., the default NeRF model). These can be trained with the `ns-train` command with toggles for camera pose optimization, enabling bilateral grids, and other parameters.

    *Important:* the splats are current initialized from random and the optimization is definitely non-convex, which means that you probably want to train multiple times. Seeing a PSNR change of 5dB is not uncommon between training runs. (In the future, the splats will be initialized from the point cloud generated by Pix4D.) We recommend running with `wandb` (Weights & Biases) to track the training runs and visualize the results and to keep the visualization off during training to speed up the process and reduce memory usage.

    ```bash
    ns-train splatfacto --data path/to/folder/with/json_and_images/ --vis wandb --project-name DixHite --experiment-name method-data-subset
    ```

    such as

    ```bash
    ns-train splatfacto --data ../open-photogrammetry-format/dixhite_nerfstudio_lite/ --vis wandb --project-name DixHite --experiment-name splatfacto-DH-lite-18b6e --pipeline.datamanager.camera-res-scale-factor 0.5
    ```
    or

    ```bash
    ns-train nerfacto --data ../open-photogrammetry-format/dixhite_nerfstudio_lite/ --vis wandb --project-name DixHite --experiment-name nerfacto-DH-lite-18b6e
    ```

    b. The `ns-viewer` command can be used to visualize the results. This is useful for debugging and understanding how well the model has learned the scene. Although the metrics in the training logs are useful, visual inspection is often required since all of the eval images are "in distribution".

    ```bash
    ns-viewer --load-config path/to/config.yml
    ```

    such as

    ```bash
    ns-viewer --load-config outputs/splatfacto-DH-lite-18b6e/splatfacto/2025-06-18_103321/config.yml
    ```

4. **Export:**

    a. The trained models can be exported to various formats with the `ns-export` command. Generally, we export the `splatfacto` model to the `ply` splat format expected in online viewers. The `nerfacto` model is more difficult to export.

    ```bash
    ns-export gaussian-splat --load-config path/to/config.yml
    ```

    such as 

    ```bash
    ns-export gaussian-splat --load-config outputs/splatfacto-DH-lite-18b6e/splatfacto/2025-06-18_103321/config.yml
    ```

    b. Depending on the use case, you may want to export a _cropped_ version of the model. This can be done by specifying a bounding box in the `ns-export` command. Often, it can be easiest to set the bounding box in the `ns-viewer` and then copy the bounding box parameters to the export command.

    ```bash
    ns-export gaussian-splat --load-config path/to/config.yml --obb_center ... --obb_rotation ... --obb_scale ...
    ```

    such as

    ```bash
    ns-export gaussian-splat --load-config outputs/splatfacto-DH-lite-18b6e/splatfacto/2025-06-18_103321/config.yml --output-dir exports/splat/ --obb_center 0.0000000000 0.0000000000 -0.5000000000 --obb_rotation -0.3600000000 0.1500000000 0.0000000000 --obb_scale 2.0000000000 2.0000000000 0.5500000119
    ```

5. **Post-processing:**

    a. We recommend using SuperSplat (https://playcanvas.com/products/supersplat, specifically https://superspl.at/editor/) to finish editing the splats. Specifically, you can further crop the splats and remove outliers.


## Lessons Learned
- **Use OPF and PyOPF:** Prior to using the OPF format, we were attempting to parse the data from the Pix4D logs into nerfstudio, which is tedious and error-prone. The OPF format is a standardized way to store photogrammetry data and is supported by Pix4D, which makes it easier to work with. The `pyopf` library provides a convenient way to convert the OPF data to the Nerfstudio format.
    - *Future work:* Not all of the OPF data is currently transferred to the Nerfstudio format, some of which is helpful for georeferencing. As an example, below is a pose entry using `pyopf`. Notice the lack of latitude, longitude, and altitude information or the coordinate reference system (CRS) information. While this info is not helpful for a _general_ radiance field, it is surely helpful for a radiance field that is georeferenced. 

    ```json
    {
        "file_path": "images/18b6eeedec0ad1afdb2a4c6194a1cb12/DJI_20250116144128_0001_V.JPG",
        "sharpness": 497.9324916303854,
        "transform_matrix": [
            [
                -0.9999940971720697,
                -0.003434130143462031,
                0.00011122578452617957,
                -1.5746158720105796
            ],
            [
                0.003433953743571987,
                -0.999992904076043,
                -0.0015491156343917956,
                0.9166514329169863
            ],
            [
                0.00011654485997224369,
                -0.001548724546029606,
                0.9999987939340608,
                0.9774399072593736
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0
            ]
        ],
        "camera_model": "OPENCV",
        "camera_angle_x": 1.2365496407798062,
        "camera_angle_y": 0.9793051369679774,
        "fl_x": 3711.4736242481904,
        "fl_y": 3711.4736242481904,
        "k1": 0.0068451060448535975,
        "k2": -0.01720111628858226,
        "k3": 0.035920615364993885,
        "k4": 0,
        "p1": -0.001854310406489115,
        "p2": -0.00021452167090679883,
        "is_fisheye": false,
        "cx": 2630.6611799725556,
        "cy": 1927.3369352072907,
        "w": 5280,
        "h": 3956,
        "aabb_scale": 16
    }
    ```



- **Data Size:** The GPU will struggle with large datasets, which is the case for us since we have many high resolution images. Especially for prototyping, we recommend creating a "lite" version of the dataset by selecting a subset of images to work with initially. For splatfacto, we also have to either downscale the images to a lower resolution or cap the number of splats to not exceed the GPU memory, which is undesireable in the long-term, but fine for prototyping. 

    - *Future work:* We are working on an efficient way to handle large datasets. Specifically, in `ns-train` there is a flag for `--pipeline.datamanager.cache-images`:

    ```
    --pipeline.datamanager.cache-images {cpu,gpu,disk}
    │
    │     Where to cache images in memory.
    │
    │     - If "cpu", caches images on cpu RAM as pytorch tensors.
    │     - If "gpu", caches images on device as pytorch tensors.
    │     - If "disk", keeps images on disk which conserves memory. Datamanager will use parallel dataloader
    │     (default: gpu)
    ```

- **Camera Intrinsics:** The camera intrinsics are assumed constant in the current workflow, which is not true in reality. The image intrinsics are generally quite close within a single data collection pass: if you collect drone imagery via `DJI Smart Oblique` it is quite likely that the intrinsics will be similar. But, the intrinsics may vary the next time you run it. The intrinsics are _definitely_ different between devices.

    - *Future work:* It is definitely possible to use the different intrinsics (e.g., like NeRF-In-The-Wild) and I seem to recall that nerfstudio used be able to look at the transforms file directly, but this seems to not be the case anymore. We will look into this in the future.

    As an example, these are the camera intrinsics for one drone pass (ignoring the transform matrix for succinctness):
    ```json
    {
        "file_path": "images/7c377562c94dd8bb34397505c9c9a8e8/DJI_20250116144945_0001_V.JPG",
        "sharpness": 578.1971481888955,
        "transform_matrix": [[], [], [], []],
        "camera_model": "OPENCV",
        "camera_angle_x": 1.2353634462972731,
        "camera_angle_y": 0.9782629285891327,
        "fl_x": 3716.1379634705486,
        "fl_y": 3716.1379634705486,
        "k1": -0.10841462700837586,
        "k2": 0.0033175741653546586,
        "k3": -0.019128156099607264,
        "k4": 0,
        "p1": 2.8716084987264094e-06,
        "p2": -7.955413792490326e-05,
        "is_fisheye": false,
        "cx": 2627.7909343926726,
        "cy": 1925.531579643719,
        "w": 5280,
        "h": 3956,
        "aabb_scale": 16
    }
    ```

    versus another drone pass (similar numbers, but not the same):

    ```json
    {
        "file_path": "images/18b6eeedec0ad1afdb2a4c6194a1cb12/DJI_20250116144128_0001_V.JPG",
        "sharpness": 497.9324916303854,
        "transform_matrix": [[], [], [], []],
        "camera_model": "OPENCV",
        "camera_angle_x": 1.2365496407798062,
        "camera_angle_y": 0.9793051369679774,
        "fl_x": 3711.4736242481904,
        "fl_y": 3711.4736242481904,
        "k1": 0.0068451060448535975,
        "k2": -0.01720111628858226,
        "k3": 0.035920615364993885,
        "k4": 0,
        "p1": -0.001854310406489115,
        "p2": -0.00021452167090679883,
        "is_fisheye": false,
        "cx": 2630.6611799725556,
        "cy": 1927.3369352072907,
        "w": 5280,
        "h": 3956,
        "aabb_scale": 16
    }
    ```

    versus a handheld pass, which is does not have the intrinsics in the file, but the intrinsics are included at the top of the file. The numbers are quite different, as expected:

    ```json
    {
        "camera_model": "OPENCV",
        "camera_angle_x": 1.2922413358739202,
        "camera_angle_y": 0.8023549843539378,
        "fl_x": 2800.6853747491177,
        "fl_y": 2800.6853747491177,
        "k1": 0.16751758001629213,
        "k2": -0.43694085113871817,
        "k3": 0.3572620582476267,
        "k4": 0,
        "p1": 0.0,
        "p2": 0.0,
        "is_fisheye": false,
        "cx": 2132.989062410066,
        "cy": 1185.9892300680767,
        "w": 4224,
        "h": 2376,
        "aabb_scale": 16,
        "frames": [ 
            {
                "file_path": "images/ef9926668c953a314a74e175dcfff7eb/Image_000002.jpg",
            "sharpness": 2895.427335672406,
            "transform_matrix": [[], [], [], []]
            }
        ]
    }
    ```

- **Camera Pose Optimization:** The camera poses are assumed to be accurate, which is generally the case with Pix4D (since it uses RTK GNSS for georeferencing and ground control markers). Indeed the results seem worse when camera pose optimization is enabled during training. 
    - *Future work:* The general Nerfstudio assumption on camera pose optimization is that the poses are quite inaccurate meaning that a lot of modification may be needed. However, this is not the case for us. Instead, it may be helpful to postpone camera pose optimization until after the model has been (mostly) trained. Nerfstudio handles the camera pose optimization with the `--pipeline.model.camera-optimizer.mode` flag:

    ```
    --pipeline.model.camera-optimizer.mode {off,SO3xR3,SE3}
    │     Pose optimization strategy to use. If enabled, we recommend SO3xR3.
    │     (default: off)
    ```

    We can some control over the exponential decay scheduler as well with the `optimizers.camera-opt.optimizer` options:

    ```
    --optimizers.camera-opt.optimizer.lr FLOAT
    │     The learning rate to use. (default: 0.0001)
    --optimizers.camera-opt.optimizer.eps FLOAT
    │     The epsilon value to use. (default: 1e-15)
    --optimizers.camera-opt.optimizer.max-norm {None}|FLOAT
    │     The max norm to use for gradient clipping. (default: None)
    --optimizers.camera-opt.optimizer.weight-decay FLOAT
    │     The weight decay to use. (default: 0)
    ```

    In principle, we could define a custom scheduler as well and extend the `splatfacto` or `nerfacto` models to support this. However, this is not currently implemented. See: https://github.com/nerfstudio-project/nerfstudio/blob/main/nerfstudio/configs/method_configs.py  

    Note that nerfstudio saves everything in the pytorch checkpoint file. So, the optimized camera poses would likely need to be extracted from the checkpoint file after training. 

- **Point Cloud Initialization:** The splats are currently initialized from random, which is not ideal. The "default" nerfstudio workflow goes through COLMAP which saves the point cloud in a specific way. Historically, NeRFs did not use the point cloud initialization, which may explain why it is not transferred in the `pyopf` conversion.
    - *Future work:* The OPF point cloud is current stored in a few ways (see the directory structure below). Likely, we want to initialize the splats from the sparse point cloud data in the `sparse` directory. Nerfstudio expects the point cloud as a ply file rather than glb (https://pix4d.github.io/opf-spec/specification/point_cloud.html). It seems theoretically possible to convert to ply with `pyopf` via the following command:

    ```bash
    opf2ply path_to/project.opf --out-dir your_output_dir
    ```

- **Local Coordinate System:** Nerfstudio auto-rotates, auto-scales, and auto-centers the scene by default based on the camera poses. Usually we do not want this since the Pix4D data is already in a good local coordinate system. However, for `nerfacto`, scale is often crucial for the model to learn the scene because of how it handles "near" and "far" distances. The `splatfacto` model does not have this issue since it uses a directly explicit representation.
    - *Current solution:* Nerfstudio has a few (rather hidden) flags to fix this, but it requires some preamble: When passing the data to `ns-train`, we can specify the `dataparser` (https://github.com/nerfstudio-project/nerfstudio/tree/main/nerfstudio/data/dataparsers). The default is the nerfstudio (i.e., `nerfstudio-data` at https://github.com/nerfstudio-project/nerfstudio/blob/main/nerfstudio/data/dataparsers/nerfstudio_dataparser.py). This data parser has some additional flags, of which include:

    ```
    --scene-scale FLOAT
    │   How much to scale the region of interest by. (default: 1.0)
    --orientation-method {pca,up,vertical,none}
    │   The method to use for orientation. (default: up)
    --center-method {poses,focus,none}
    │   The method to use to center the poses. (default: poses)
    --auto-scale-poses {True,False}
    │   Whether to automatically scale the poses to fit in +/- 1 bounding box. (default: True)
    ```

    To avoid the auto-rotation, auto-scaling, and auto-centering, we can use the following command:

    ```bash
    splatfacto --data ../open-photogrammetry-format/dixhite_nerfstudio_lite/ --vis wandb --project-name DixHite --experiment-name splatfacto-DH-lite-18b6e --pipeline.datamanager.camera-res-scale-factor 0.5 nerfstudio-data --orientation-method none --center-method none --auto-scale-poses False
    ```

    Note that the final transform matrix is stored in `dataparser_transforms.json` in the same directory as the `config.yml` file generated at the start of training. If the parameters are set correctly, the transform matrix should be the identity matrix. 

## OPF Directory Structure

Directory Structure from OPF seen in practice for the Ecodash data. Note that the OPF specification has more information: https://github.com/Pix4D/opf-spec.
```
├── computed_depth_maps
│   # Contains .tiff depth maps computed from the images (no lidar)
├── dense
│   # Contains dense point cloud data as `.glbin` and `.glb` files
├── depth
│   # Contains depth point cloud data as `.glbin` and `.glb` files
├── fused
│   # Contains fused point cloud data as `.glbin` and `.glb` files
├── images
│   ├── 18b6eeedec0ad1afdb2a4c6194a1cb12
│   │   # Images from one drone pass
│   ├── 7c377562c94dd8bb34397505c9c9a8e8
│   │   # Images from another drone pass
│   └── ef9926668c953a314a74e175dcfff7eb
│       # Images from a handheld pass
└── sparse
    # Contains sparse point cloud data as `.glbin` and `.glb` files
```

In addition to these directories, we have the following files in the top-level directory:

```
├── 2d_segment_graphs.json
    # A JSON file with confidence scores between graph edges and vertices for different camera ids.
│
├── 3d_segment_graphs.json
    # A JSON file (structured differently than the 2d_segment_graphs.json) with confidence scores between graph edges and vertices for different camera ids.
│
├── calibrated_cameras.json
    # A JSON file with the calibrated camera parameters including id (id and sensor id) and extrinsics (as three orientation angles, unspecified). The end of the file includes three intrinsic parameters sets with distortion type, focal length, and principal point.
│
├── calibrated_intersection_tie_points.json
    # A JSON file with the camera coordinates of calibrated marks.
│
├── calibration_settings.json
    # A JSON file with the hyperparameter settings used in Pix4D.
│
├── camera_list.json
    # A JSON file matching camera ids to their corresponding image files.
│
├── computed_depth_maps.json
    # A JSON file matching camera ids to their corresponding depth maps.
│
├── features.bin
    # Binary file unopened yet.
│
├── gps_bias.json
    # A JSON file with the GPS bias values (rotation, scale, translation), though set to identity.
│
├── input_cameras.json
    # A JSON file with more detailed camera parameters including id (id and sensor id), pixel range, geolocation (latitude, longitude, altitude, coordinate reference system (CRS), and the sigma uncertainties), height above takeoff, orientation (specified as yaw, pitch, roll with uncertainties), and the timestamp. The end of the file includes three intrinsic parameters sets with band wavelengths & weights, image size, distortion type, focal length, principal point, and pixel size.
│
├── input_intersection_tie_points.json
    # A JSON file similar to `calibrated_intersection_tie_points.json`.
│
├── matches.bin
    # Binary file unopened yet.
│
├── originalMatches.bin
    # Binary file unopened yet.
│
├── polygonal_meshes.json
    # A JSON file with the mesh edges, vertices, and faces. Though, this is a sparse mesh.
│
├── projected_input_cameras.json
    # A JSON file that has the camera poses in a new reference frame (no intrinsics).
│
├── project.opf
    # A JSON file as a directory tree
│
├── reoptimize_settings.json
    # A JSON file with the hyperparameter settings used in Pix4D for reoptimization.
│
├── scene_reference_frame.json
    # A JSON file with the conversion from the georeferenced coordinate system to the local coordinate system used in Pix4D (i.e., with a UTM shift and CRS description).
│
└── vanishing_points.json
    # A JSON file with three points, presumably per axis.
```


