════════════════════════════════════════════════════════════════════════════
                    DeepEarth Production Training
════════════════════════════════════════════════════════════════════════════
Experiment: deepearth_flowering_production
Date: Mon Sep 29 09:02:25 UTC 2025
════════════════════════════════════════════════════════════════════════════
Checking shared memory...
Current shared memory: 27G

GPU Configuration:
  Available GPUs: 1
NVIDIA L4, 23034 MiB, 550.90.07

Using single GPU training
Started GPU monitoring (PID: 1459804)

════════════════════════════════════════════════════════════════════════════
Launching training with command:
/usr/bin/python3 -u /opt/ecodash/deepearth/train_deepearth_verbose.py --config /opt/ecodash/deepearth/models/flowering/flowering_light.yaml --data_dir /opt/ecodash/deepearth/models/flowering/data --output_dir /opt/ecodash/deepearth/experiments --epochs 10 --batch_size 32 --learning_rate 1e-3 --gradient_accumulation 2 --num_workers 4 --save_every 5 --mixed_precision
════════════════════════════════════════════════════════════════════════════

Using device: cuda

====================================================================================================
DEEPEARTH VERBOSE TRAINING DIAGNOSTICS
====================================================================================================
Batch size: 32
Context window: 16
Total tokens per batch: 512

📚 Loading dataset...

======================================================================
Angiosperm Dataset Bridge Initialization
======================================================================
Data directory: /opt/ecodash/deepearth/models/flowering/data
[CoordinateTransformer] Initialized with time range [1900, 2100]

✓ Loading preprocessed data from cache: cache/flowering_dataset/processed_data.pt
  Loaded 561,381 samples from cache
✅ Loaded 561,381 samples, 2,784,856 observations

🎼 Encoder configurations (Symphony of Experts):
  [0] alphaearth: 64D → PerceiverProjection → 500D
  [2] bioclip2: 768D → PerceiverProjection → 500D
  [3] phenovision: 1D → PerceiverProjection → 500D
  [1] earth4d: 4D → PerceiverProjection → 500D

🏗️ Initializing model...

======================================================================
Initializing DeepEarth Perceiver
======================================================================
Universal token dimension: 256
  → Spacetime: 128D
  → Data: 118D
  → Metadata: 6D
  → Mask: 4D
  → Position: 0D

Initializing Earth4D spatiotemporal encoder...

================================================================================
EARTH4D RESOLUTION SCALE TABLE
================================================================================

SPATIAL ENCODER (XYZ):
Level  Grid Res     Meters/Cell     KM/Cell     
--------------------------------------------------
0      16           796.4km         796.38      
1      32           398.2km         398.19      
2      64           199.1km         199.09      
3      128          99.5km          99.55       
4      256          49.8km          49.77       
5      512          24.9km          24.89       
6      1024         12.4km          12.44       
7      2048         6.2km           6.22        
8      4096         3.1km           3.11        
9      8192         1.6km           1.56        
10     16384        777.7m          0.778       
11     32768        388.9m          0.389       
12     65536        194.4m          0.194       
13     131072       97.21m          0.097       
14     262144       48.61m          0.049       
15     524288       24.30m          0.024       
16     1048576      12.15m          0.012       
17     2097152      6.076m          0.006       
18     4194304      3.038m          0.003       
19     8388608      1.519m          0.002       
20     16777216     0.7595m         0.0008      
21     33554432     0.3797m         0.0004      
22     67108864     0.1899m         0.0002      
23     134217728    0.0949m         0.0001      

TEMPORAL ENCODERS (XYT, YZT, XZT):
Level  Grid Res     Seconds/Cell    Days/Cell   
--------------------------------------------------
0      8            3944700.0       45.66       
1      16           1972350.0       22.83       
2      32           986175.0        11.41       
3      64           493087.5        5.71        
4      128          246543.8        2.85        
5      256          123271.9        1.43        
6      512          61635.9         0.71        
7      1024         30818.0         0.36        
8      2048         15409.0         0.18        
9      4096         7704.5          0.09        
10     8192         3852.2          0.04        
11     16384        1926.1          0.02        
12     32768        963.1           0.01        
13     65536        481.5           0.01        
14     131072       240.8           0.00        
15     262144       120.4           0.00        
16     524288       60.2            0.00        
17     1048576      30.1            0.00        
18     2097152      15.0            0.00        

HASH TABLE CONFIGURATION:
  Spatial: 2^23 = 8,388,608 entries
  Temporal: 2^18 = 262,144 entries
  Total capacity: 9,175,040 entries

ACTUAL PARAMETERS (MEMORY FOOTPRINT):
  Spatial encoders: 340,336,640 params = 1298.28 MB
  Temporal encoders: 25,390,080 params = 96.86 MB
  Total: 365,726,720 params = 1395.14 MB
  During training (4x): ~5580.55 MB
  Earth4D raw output dimension: 162
  ⚡ Skipping PerceiverProjector for Earth4D - using native 162D directly
  Adjusted spacetime_dim to 162D
  Recalculated data_dim: 84D
  ✓ Earth4D initialized (spacetime: 162D, data: 84D)

Initializing multimodal fusion network...

======================================================================
Initializing Multimodal Fusion Network
======================================================================
Universal dimension: 84
Number of encoders: 4

  Encoder 0 (alphaearth):
    Input dimension: 64
    ✓ Created bidirectional projections

  Encoder 2 (bioclip2):
    Input dimension: 768
    ✓ Created bidirectional projections

  Encoder 3 (phenovision):
    Input dimension: 1
    ✓ Created bidirectional projections

  Encoder 1 (earth4d):
    Input dimension: 4
    → Earth4D handled by spacetime encoder (trainable)

======================================================================


Initializing metadata embeddings...
  → Dataset: 10 → 2D
  → Modality: 10 → 2D
  → Encoder: 10 → 2D
  → Mask patterns: 32 → 4D

Initializing positional encodings...
  → Context position: disabled
  → Modality position: disabled

Configuring Perceiver architecture...
  → Encoder: 2 blocks, 64 latents
  → Decoder: Cross-attention reconstruction

Configuring loss functions...
  → Component loss weights: [1.0, 1.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612]

======================================================================
DeepEarth Perceiver initialized successfully
Total parameters: 370,803,897
======================================================================

  Total parameters: 370.8M
  Earth4D parameters: 365.7M (98.6%)

🏃 Starting training on 2,227,884 observations...
  Learning rate: 0.001
  Mixed precision: True

====================================================================================================
STARTING TRAINING EPOCH
====================================================================================================

================================================================================
BATCH 1/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 253 tokens (49.4%)
    BioCLIP: 112 tokens (21.9%)
    Encoder_0: 146 tokens (28.5%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.111328125, 'data': 0.130859375, 'dataset': 0.017578125, 'modality': 0.0546875, 'encoder': 0.072265625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.9664

📉 LOSS BREAKDOWN:
  Total loss: 0.966413

  Universal Space Losses (256D token space):
    spacetime_loss: 0.046589
    data_loss: 0.407087
    dataset_loss: 0.033773
    encoder_loss: 0.038653

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.209502
    phenovision_modality_loss: 0.147085
    bioclip_modality_loss: 0.354792
    alphaearth_modality_loss: 0.467211

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=57
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=67
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=28
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=37

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0061, -0.0310, -0.0232, 0.0003]
    Predicted: [-0.0597, -0.0737, -0.1897, 0.0428]
    Δ (Delta): [-0.0536, -0.0427, -0.1665, 0.0426]
    MAPE: 4384.03%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.053, -0.92 , -0.197, -1.054,  0.389, -0.081, -0.282,  0.09 ]
    Predicted:
      [-0.14 ,  0.011, -0.114,  0.388, -0.098,  0.131,  0.169,  0.239]
    Δ (Delta):
      [-0.193,  0.931,  0.083,  1.442, -0.487,  0.212,  0.451,  0.149]
    MAPE: 170.00%, RMSE: 0.6613
    🌸 Flowering Probability:
      Original:  0.053
      Predicted: -0.140
      Δ (Delta): -0.193

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 5.859505

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.015358
    Max norm: 0.018048
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.023556
    Max norm: 0.290244
    Zero gradient ratio: 15.56%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.86s
  Backward pass: 13.16s
  Ratio (back/fwd): 4.6x

🔧 PARAMETER UPDATES:
  other: mean_change=0.02085763, max_change=0.09042532
  earth4d: mean_change=0.10023502, max_change=0.10994882
  multimodal: mean_change=0.03373665, max_change=1.08425677
  perceiver: mean_change=0.06469154, max_change=0.25585458

================================================================================
BATCH 2/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 250 tokens (48.8%)
    BioCLIP: 143 tokens (27.9%)
    Encoder_0: 118 tokens (23.0%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.107421875, 'data': 0.12890625, 'dataset': 0.017578125, 'modality': 0.056640625, 'encoder': 0.044921875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 1.0437

📉 LOSS BREAKDOWN:
  Total loss: 1.043654

  Universal Space Losses (256D token space):
    spacetime_loss: 0.047639
    data_loss: 0.324802
    dataset_loss: 0.019612
    encoder_loss: 0.025312

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.045723
    phenovision_modality_loss: 0.207894
    bioclip_modality_loss: 0.730637
    alphaearth_modality_loss: 0.385767

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=55
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=66
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=29
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=23

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 5):
    Original:  [-0.0104, -0.0187, -0.0366, 0.0214]
    Predicted: [0.1566, -0.1204, -0.1087, 0.2324]
    Δ (Delta): [0.1670, -0.1017, -0.0722, 0.2110]
    MAPE: 832.59%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 11):
    Original (first 8 dims):
      [ 0.454, -1.246, -1.459,  0.35 ,  0.073,  0.573, -0.595,  0.786]
    Predicted:
      [-0.04 ,  0.177,  0.12 ,  0.229,  0.14 , -0.223, -0.291, -0.164]
    Δ (Delta):
      [-0.494,  1.423,  1.579, -0.121,  0.068, -0.796,  0.304, -0.95 ]
    MAPE: 96.24%, RMSE: 0.8951
    🌸 Flowering Probability:
      Original:  0.454
      Predicted: -0.040
      Δ (Delta): -0.494

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 6.690344

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.009502
    Max norm: 0.010118
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.023258
    Max norm: 0.314675
    Zero gradient ratio: 15.19%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.92s
  Backward pass: 13.00s
  Ratio (back/fwd): 4.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.01443446, max_change=0.06234387
  earth4d: mean_change=0.09132547, max_change=0.10158998
  multimodal: mean_change=0.02308401, max_change=0.64785862
  perceiver: mean_change=0.04575599, max_change=0.18131660

================================================================================
BATCH 3/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 112 tokens (21.9%)
    AlphaEarth: 267 tokens (52.1%)
    Encoder_0: 131 tokens (25.6%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.083984375, 'data': 0.162109375, 'dataset': 0.021484375, 'modality': 0.064453125, 'encoder': 0.0390625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.7195

📉 LOSS BREAKDOWN:
  Total loss: 0.719512

  Universal Space Losses (256D token space):
    spacetime_loss: 0.049673
    data_loss: 0.246387
    dataset_loss: 0.043843
    encoder_loss: 0.020752

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.007864
    bioclip_modality_loss: 0.244282
    alphaearth_modality_loss: 0.324276
    phenovision_modality_loss: 0.263855

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=43
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=83
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=33
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=20

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0115, -0.0160, -0.0408, 0.0248]
    Predicted: [0.1282, -0.0163, -0.1340, -0.1289]
    Δ (Delta): [0.1397, -0.0004, -0.0932, -0.1537]
    MAPE: 515.94%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [-0.509,  0.251,  0.136, -1.312, -0.286,  0.34 , -0.842, -0.014]
    Predicted:
      [-0.55 , -0.036,  0.255,  0.562, -0.093,  0.049,  0.081,  0.443]
    Δ (Delta):
      [-0.041, -0.287,  0.118,  1.874,  0.193, -0.29 ,  0.923,  0.456]
    MAPE: 489.51%, RMSE: 0.7738

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 4.205340

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.010600
    Max norm: 0.013130
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.025097
    Max norm: 0.259576
    Zero gradient ratio: 15.56%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.55s
  Backward pass: 13.08s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.01207629, max_change=0.05238355
  earth4d: mean_change=0.08511346, max_change=0.09512629
  multimodal: mean_change=0.01813748, max_change=0.49811566
  perceiver: mean_change=0.03643893, max_change=0.14802116

================================================================================
BATCH 4/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 113 tokens (22.1%)
    BioCLIP: 129 tokens (25.2%)
    AlphaEarth: 270 tokens (52.7%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.091796875, 'data': 0.150390625, 'dataset': 0.02734375, 'modality': 0.0625, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.9298

📉 LOSS BREAKDOWN:
  Total loss: 0.929752

  Universal Space Losses (256D token space):
    spacetime_loss: 0.063533
    data_loss: 0.265332
    dataset_loss: 0.116656
    encoder_loss: 0.253962

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.048839
    alphaearth_modality_loss: 0.225501
    phenovision_modality_loss: 0.218451
    bioclip_modality_loss: 0.673931

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=47
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=77
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=14
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=32
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 11):
    Original:  [-0.0119, -0.0165, -0.0434, 0.0269]
    Predicted: [-0.4878, 0.4695, 0.2949, -0.0949]
    Δ (Delta): [-0.4759, 0.4860, 0.3383, -0.1218]
    MAPE: 2044.44%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.989,  0.688,  0.223, -0.032, -0.319,  0.179, -0.235, -0.028]
    Predicted:
      [ 0.096,  0.017,  0.149, -0.229,  0.061,  0.073, -0.704,  0.148]
    Δ (Delta):
      [-0.893, -0.671, -0.074, -0.197,  0.38 , -0.106, -0.469,  0.176]
    MAPE: 228.58%, RMSE: 0.4609

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 9.022598

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.007192
    Max norm: 0.008202
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.017919
    Max norm: 0.435903
    Zero gradient ratio: 15.56%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.54s
  Backward pass: 13.06s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00985341, max_change=0.04239912
  earth4d: mean_change=0.08140709, max_change=0.09110113
  multimodal: mean_change=0.01520076, max_change=0.44156972
  perceiver: mean_change=0.03134033, max_change=0.13026893

================================================================================
BATCH 5/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 123 tokens (24.0%)
    BioCLIP: 129 tokens (25.2%)
    Earth4D: 3 tokens (0.6%)
    AlphaEarth: 257 tokens (50.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09375, 'data': 0.15625, 'dataset': 0.029296875, 'modality': 0.05078125, 'encoder': 0.044921875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.8753

📉 LOSS BREAKDOWN:
  Total loss: 0.875320

  Universal Space Losses (256D token space):
    spacetime_loss: 0.046341
    data_loss: 0.203712
    dataset_loss: 0.019812
    encoder_loss: 0.178140

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.045699
    phenovision_modality_loss: 0.217598
    alphaearth_modality_loss: 0.211868
    bioclip_modality_loss: 0.772340

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=48
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=80
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=15
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=23

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 15):
    Original:  [-0.0042, -0.0329, -0.0273, -0.0017]
    Predicted: [-0.0341, 0.5630, 0.0395, 0.1819]
    Δ (Delta): [-0.0299, 0.5959, 0.0668, 0.1835]
    MAPE: 3453.73%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.663, -0.042,  0.917,  0.136, -0.363, -0.381, -0.858,  0.205]
    Predicted:
      [ 3.033e-02, -5.029e-02,  1.636e-01, -2.940e-02, -2.820e-04,  2.190e-01, -5.327e-01,  3.120e-01]
    Δ (Delta):
      [-0.632, -0.008, -0.753, -0.165,  0.363,  0.6  ,  0.325,  0.107]
    MAPE: 83.19%, RMSE: 0.4476
    🌸 Flowering Probability:
      Original:  0.663
      Predicted: 0.030
      Δ (Delta): -0.632

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 4.417517

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.005139
    Max norm: 0.007119
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.021720
    Max norm: 0.403075
    Zero gradient ratio: 15.56%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.51s
  Backward pass: 13.06s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00874815, max_change=0.03719682
  earth4d: mean_change=0.07823118, max_change=0.08747734
  multimodal: mean_change=0.01341325, max_change=0.41484222
  perceiver: mean_change=0.02766373, max_change=0.12051274

================================================================================
BATCH 6/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 260 tokens (50.8%)
    Encoder_0: 131 tokens (25.6%)
    BioCLIP: 119 tokens (23.2%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.123046875, 'data': 0.1640625, 'dataset': 0.0234375, 'modality': 0.05078125, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.5316

📉 LOSS BREAKDOWN:
  Total loss: 0.531616

  Universal Space Losses (256D token space):
    spacetime_loss: 0.031089
    data_loss: 0.147742
    dataset_loss: 0.011395
    encoder_loss: 0.026634

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.006662
    bioclip_modality_loss: 0.292137
    phenovision_modality_loss: 0.229558
    alphaearth_modality_loss: 0.174936

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=63
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=84
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0145, -0.0048, -0.0422, 0.0273]
    Predicted: [0.1327, 0.3467, -0.1334, 0.2410]
    Δ (Delta): [0.1472, 0.3515, -0.0913, 0.2137]
    MAPE: 2318.13%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [-0.042,  0.175,  0.211, -0.834, -0.133, -0.433, -0.184,  0.561]
    Predicted:
      [-0.024,  0.129,  0.415,  0.102, -0.086,  0.379, -0.219,  0.334]
    Δ (Delta):
      [ 0.019, -0.045,  0.204,  0.936,  0.047,  0.812, -0.036, -0.227]
    MAPE: 70.23%, RMSE: 0.4521

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 4.867266

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.007239
    Max norm: 0.009272
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.023712
    Max norm: 0.311454
    Zero gradient ratio: 15.19%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.53s
  Backward pass: 13.09s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00795707, max_change=0.03360861
  earth4d: mean_change=0.07697520, max_change=0.08653365
  multimodal: mean_change=0.01241091, max_change=0.37169105
  perceiver: mean_change=0.02478952, max_change=0.11419425

================================================================================
BATCH 7/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 133 tokens (26.0%)
    BioCLIP: 136 tokens (26.6%)
    AlphaEarth: 243 tokens (47.5%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.103515625, 'data': 0.15234375, 'dataset': 0.005859375, 'modality': 0.064453125, 'encoder': 0.041015625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.3924

📉 LOSS BREAKDOWN:
  Total loss: 0.392423

  Universal Space Losses (256D token space):
    spacetime_loss: 0.028671
    data_loss: 0.141056
    dataset_loss: 0.004223
    encoder_loss: 0.003495

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.005129
    bioclip_modality_loss: 0.100873
    phenovision_modality_loss: 0.177060
    alphaearth_modality_loss: 0.164890

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=53
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=78
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=3
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=33
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=21

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0100, -0.0166, -0.0379, 0.0201]
    Predicted: [0.0189, 0.1512, -0.0092, 0.3584]
    Δ (Delta): [0.0290, 0.1679, 0.0286, 0.3383]
    MAPE: 764.59%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 1, seq 7):
    Original (first 8 dims):
      [-0.128,  0.167,  0.458, -0.469, -0.041, -0.278,  0.056,  0.429]
    Predicted:
      [ 0.177,  0.132,  0.657, -0.182, -0.064,  0.245, -0.321,  0.239]
    Δ (Delta):
      [ 0.306, -0.035,  0.199,  0.287, -0.023,  0.523, -0.377, -0.19 ]
    MAPE: 165.41%, RMSE: 0.2892

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 1.835266

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.002813
    Max norm: 0.003585
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.020733
    Max norm: 0.283344
    Zero gradient ratio: 15.56%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.69s
  Backward pass: 13.07s
  Ratio (back/fwd): 4.9x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00691060, max_change=0.02874946
  earth4d: mean_change=0.07685284, max_change=0.08667255
  multimodal: mean_change=0.01142252, max_change=0.34167442
  perceiver: mean_change=0.02152709, max_change=0.11326796

================================================================================
BATCH 8/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 252 tokens (49.2%)
    Encoder_0: 122 tokens (23.8%)
    BioCLIP: 138 tokens (27.0%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.119140625, 'data': 0.15625, 'dataset': 0.021484375, 'modality': 0.044921875, 'encoder': 0.02734375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.3389

📉 LOSS BREAKDOWN:
  Total loss: 0.338894

  Universal Space Losses (256D token space):
    spacetime_loss: 0.013410
    data_loss: 0.083248
    dataset_loss: 0.000292
    encoder_loss: 0.012027

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.001017
    alphaearth_modality_loss: 0.132299
    bioclip_modality_loss: 0.135064
    phenovision_modality_loss: 0.214442

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=61
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=80
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=23
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=14

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 2):
    Original:  [-0.0148, -0.0047, -0.0448, 0.0285]
    Predicted: [0.0018, 0.1096, -0.0296, 0.2703]
    Δ (Delta): [0.0166, 0.1143, 0.0152, 0.2417]
    MAPE: 853.28%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [-0.032, -0.006,  0.231,  0.244,  0.06 ,  0.543, -0.737,  0.422]
    Predicted:
      [ 0.19 ,  0.035,  0.488, -0.261,  0.007,  0.118, -0.411,  0.323]
    Δ (Delta):
      [ 0.222,  0.042,  0.257, -0.505, -0.053, -0.425,  0.326, -0.1  ]
    MAPE: 239.97%, RMSE: 0.2898

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 1.636445

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.003582
    Max norm: 0.004304
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.022186
    Max norm: 0.400553
    Zero gradient ratio: 15.56%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.51s
  Backward pass: 13.08s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00625963, max_change=0.02574447
  earth4d: mean_change=0.07650976, max_change=0.08648175
  multimodal: mean_change=0.01029196, max_change=0.28129289
  perceiver: mean_change=0.01996721, max_change=0.11287552

================================================================================
BATCH 9/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 253 tokens (49.4%)
    Encoder_0: 122 tokens (23.8%)
    BioCLIP: 135 tokens (26.4%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.107421875, 'data': 0.158203125, 'dataset': 0.0234375, 'modality': 0.05078125, 'encoder': 0.0390625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2806

📉 LOSS BREAKDOWN:
  Total loss: 0.280559

  Universal Space Losses (256D token space):
    spacetime_loss: 0.007714
    data_loss: 0.069301
    dataset_loss: 0.006476
    encoder_loss: 0.004135

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.003454
    bioclip_modality_loss: 0.146849
    alphaearth_modality_loss: 0.133965
    phenovision_modality_loss: 0.123462

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=55
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=81
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=20

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0144, -0.0040, -0.0425, 0.0272]
    Predicted: [0.0315, 0.0675, -0.0575, 0.0734]
    Δ (Delta): [0.0458, 0.0715, -0.0150, 0.0462]
    MAPE: 575.42%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.011,  0.093,  0.373, -0.406, -0.001,  0.039, -0.307,  0.496]
    Predicted:
      [ 0.261, -0.01 ,  0.392, -0.247, -0.063, -0.011, -0.444,  0.387]
    Δ (Delta):
      [ 0.251, -0.103,  0.019,  0.159, -0.062, -0.05 , -0.137, -0.109]
    MAPE: 1147.25%, RMSE: 0.1305

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 2.383981

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.005301
    Max norm: 0.006253
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.018306
    Max norm: 0.397836
    Zero gradient ratio: 15.93%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.50s
  Backward pass: 13.07s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00569988, max_change=0.02330675
  earth4d: mean_change=0.07455930, max_change=0.08479468
  multimodal: mean_change=0.00919586, max_change=0.25081098
  perceiver: mean_change=0.01759474, max_change=0.10298623

================================================================================
BATCH 10/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 258 tokens (50.4%)
    Encoder_0: 146 tokens (28.5%)
    BioCLIP: 107 tokens (20.9%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.107421875, 'data': 0.12890625, 'dataset': 0.029296875, 'modality': 0.06640625, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.4028

📉 LOSS BREAKDOWN:
  Total loss: 0.402759

  Universal Space Losses (256D token space):
    spacetime_loss: 0.008266
    data_loss: 0.076687
    dataset_loss: 0.007012
    encoder_loss: 0.003266

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.003442
    bioclip_modality_loss: 0.190483
    alphaearth_modality_loss: 0.127566
    phenovision_modality_loss: 0.314820

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=55
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=66
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=15
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=34
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0099, -0.0155, -0.0383, 0.0205]
    Predicted: [0.0560, 0.0186, -0.0800, -0.0483]
    Δ (Delta): [0.0659, 0.0340, -0.0416, -0.0688]
    MAPE: 331.64%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 11):
    Original (first 8 dims):
      [ 0.638, -0.023,  0.52 , -0.157,  0.196,  0.574, -0.413,  0.524]
    Predicted:
      [ 0.099,  0.134,  0.384, -0.129, -0.085,  0.048, -0.302,  0.457]
    Δ (Delta):
      [-0.539,  0.156, -0.136,  0.028, -0.281, -0.526,  0.11 , -0.068]
    MAPE: 136.95%, RMSE: 0.2971

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 2.983012

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.003955
    Max norm: 0.004726
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.017729
    Max norm: 0.433342
    Zero gradient ratio: 16.30%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.45s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.3x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00514785, max_change=0.02140911
  earth4d: mean_change=0.07260454, max_change=0.08317168
  multimodal: mean_change=0.00810120, max_change=0.21079245
  perceiver: mean_change=0.01546001, max_change=0.09212653

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.008266 (trend: decreasing)
  data_loss: 0.076687 (trend: decreasing)
  dataset_loss: 0.007012 (trend: decreasing)
  modality_loss: 0.003442 (trend: decreasing)
  encoder_loss: 0.003266 (trend: decreasing)
  phenovision_modality_loss: 0.314820 (trend: increasing)
  bioclip_modality_loss: 0.190483 (trend: decreasing)
  alphaearth_modality_loss: 0.127566 (trend: decreasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.003955, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.017729, zero_ratio=16.30%

================================================================================
BATCH 11/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 264 tokens (51.6%)
    BioCLIP: 118 tokens (23.0%)
    Encoder_0: 128 tokens (25.0%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.091796875, 'data': 0.16796875, 'dataset': 0.02734375, 'modality': 0.05859375, 'encoder': 0.05859375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2817

📉 LOSS BREAKDOWN:
  Total loss: 0.281730

  Universal Space Losses (256D token space):
    spacetime_loss: 0.010053
    data_loss: 0.068761
    dataset_loss: 0.002570
    encoder_loss: 0.004463

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000685
    alphaearth_modality_loss: 0.122470
    bioclip_modality_loss: 0.093254
    phenovision_modality_loss: 0.188564

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=47
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=86
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=14
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=30
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=30

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 10):
    Original:  [-0.0145, -0.0042, -0.0449, 0.0287]
    Predicted: [0.0443, -0.0470, -0.0298, -0.0923]
    Δ (Delta): [0.0588, -0.0428, 0.0151, -0.1210]
    MAPE: 472.41%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.192, -0.051,  0.948, -0.501,  0.123,  0.361, -0.532,  0.639]
    Predicted:
      [ 0.109,  0.226,  0.421, -0.167, -0.053, -0.035, -0.403,  0.558]
    Δ (Delta):
      [-0.083,  0.277, -0.527,  0.334, -0.176, -0.396,  0.128, -0.081]
    MAPE: 124.39%, RMSE: 0.2924

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 1.074712

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.003387
    Max norm: 0.003849
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.022204
    Max norm: 0.302278
    Zero gradient ratio: 15.93%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.64s
  Backward pass: 13.10s
  Ratio (back/fwd): 5.0x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00466692, max_change=0.01936349
  earth4d: mean_change=0.07170395, max_change=0.08250213
  multimodal: mean_change=0.00801296, max_change=0.23929337
  perceiver: mean_change=0.01465756, max_change=0.08638828

================================================================================
BATCH 12/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 131 tokens (25.6%)
    AlphaEarth: 243 tokens (47.5%)
    Encoder_0: 136 tokens (26.6%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09375, 'data': 0.138671875, 'dataset': 0.0234375, 'modality': 0.048828125, 'encoder': 0.03515625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.3011

📉 LOSS BREAKDOWN:
  Total loss: 0.301056

  Universal Space Losses (256D token space):
    spacetime_loss: 0.007906
    data_loss: 0.063704
    dataset_loss: 0.009547
    encoder_loss: 0.001304

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.004782
    alphaearth_modality_loss: 0.119269
    bioclip_modality_loss: 0.109179
    phenovision_modality_loss: 0.227317

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=48
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=71
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=25
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=18

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0101, -0.0150, -0.0399, 0.0222]
    Predicted: [-0.0464, -0.1342, 0.0388, -0.0423]
    Δ (Delta): [-0.0363, -0.1191, 0.0788, -0.0645]
    MAPE: 409.67%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [-0.099,  0.088,  0.588, -0.258,  0.164,  0.192, -0.043,  0.933]
    Predicted:
      [ 0.218,  0.111,  0.422, -0.273, -0.018, -0.13 , -0.452,  0.498]
    Δ (Delta):
      [ 0.317,  0.023, -0.166, -0.015, -0.182, -0.322, -0.41 , -0.435]
    MAPE: 207.65%, RMSE: 0.2790

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 1.499253

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.003104
    Max norm: 0.003391
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.018087
    Max norm: 0.454632
    Zero gradient ratio: 16.30%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.46s
  Backward pass: 13.05s
  Ratio (back/fwd): 5.3x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00429459, max_change=0.01717530
  earth4d: mean_change=0.07102141, max_change=0.08165310
  multimodal: mean_change=0.00742172, max_change=0.21008149
  perceiver: mean_change=0.01301802, max_change=0.08049457

================================================================================
BATCH 13/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 243 tokens (47.5%)
    BioCLIP: 127 tokens (24.8%)
    Encoder_0: 140 tokens (27.3%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09765625, 'data': 0.12890625, 'dataset': 0.017578125, 'modality': 0.05859375, 'encoder': 0.048828125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.3113

📉 LOSS BREAKDOWN:
  Total loss: 0.311257

  Universal Space Losses (256D token space):
    spacetime_loss: 0.006699
    data_loss: 0.056805
    dataset_loss: 0.013216
    encoder_loss: 0.001340

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.003378
    phenovision_modality_loss: 0.252492
    bioclip_modality_loss: 0.095696
    alphaearth_modality_loss: 0.143729

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=50
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=66
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=30
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=25

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 13):
    Original:  [-0.0036, -0.0308, -0.0293, 0.0015]
    Predicted: [-0.1021, -0.1703, 0.0430, 0.0015]
    Δ (Delta): [-0.0985, -0.1395, 0.0723, 0.0001]
    MAPE: 868.23%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 6):
    Original (first 8 dims):
      [ 0.102,  0.135,  0.45 ,  0.168,  0.461,  0.681, -0.068,  0.531]
    Predicted:
      [ 0.29 ,  0.083,  0.456, -0.275, -0.015, -0.107, -0.42 ,  0.458]
    Δ (Delta):
      [ 0.187, -0.052,  0.005, -0.443, -0.476, -0.788, -0.352, -0.072]
    MAPE: 154.79%, RMSE: 0.3890
    🌸 Flowering Probability:
      Original:  0.102
      Predicted: 0.290
      Δ (Delta): 0.187

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.947115

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.002952
    Max norm: 0.004150
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.020018
    Max norm: 0.356606
    Zero gradient ratio: 15.93%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.41s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00408344, max_change=0.01562245
  earth4d: mean_change=0.07132452, max_change=0.08189169
  multimodal: mean_change=0.00696902, max_change=0.18393154
  perceiver: mean_change=0.01228695, max_change=0.07661191

================================================================================
BATCH 14/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 254 tokens (49.6%)
    BioCLIP: 137 tokens (26.8%)
    Encoder_0: 120 tokens (23.4%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.076171875, 'data': 0.1640625, 'dataset': 0.017578125, 'modality': 0.04296875, 'encoder': 0.05859375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2970

📉 LOSS BREAKDOWN:
  Total loss: 0.296952

  Universal Space Losses (256D token space):
    spacetime_loss: 0.005279
    data_loss: 0.049889
    dataset_loss: 0.000670
    encoder_loss: 0.000988

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.004310
    phenovision_modality_loss: 0.269543
    bioclip_modality_loss: 0.085963
    alphaearth_modality_loss: 0.126868

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=39
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=84
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=30

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 10):
    Original:  [-0.0109, -0.0138, -0.0427, 0.0251]
    Predicted: [-0.0574, -0.1666, -0.0371, 0.0206]
    Δ (Delta): [-0.0465, -0.1528, 0.0056, -0.0045]
    MAPE: 391.17%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.039,  0.141,  0.311,  0.08 , -0.123,  0.333, -0.732,  0.824]
    Predicted:
      [ 0.278,  0.044,  0.5  , -0.271, -0.049, -0.016, -0.421,  0.428]
    Δ (Delta):
      [ 0.239, -0.097,  0.189, -0.35 ,  0.074, -0.349,  0.311, -0.396]
    MAPE: 179.82%, RMSE: 0.2751
    🌸 Flowering Probability:
      Original:  0.039
      Predicted: 0.278
      Δ (Delta): 0.239

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 1.003359

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001833
    Max norm: 0.001993
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.020164
    Max norm: 0.397075
    Zero gradient ratio: 16.30%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.52s
  Backward pass: 13.09s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00371058, max_change=0.01424456
  earth4d: mean_change=0.07018762, max_change=0.08036573
  multimodal: mean_change=0.00673486, max_change=0.19728163
  perceiver: mean_change=0.01096228, max_change=0.07231572

================================================================================
BATCH 15/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 250 tokens (48.8%)
    Encoder_0: 145 tokens (28.3%)
    BioCLIP: 117 tokens (22.9%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.0546875, 'data': 0.1484375, 'dataset': 0.03125, 'modality': 0.037109375, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2899

📉 LOSS BREAKDOWN:
  Total loss: 0.289879

  Universal Space Losses (256D token space):
    spacetime_loss: 0.004714
    data_loss: 0.049051
    dataset_loss: 0.014569
    encoder_loss: 0.000375

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.007837
    alphaearth_modality_loss: 0.119787
    bioclip_modality_loss: 0.063314
    phenovision_modality_loss: 0.284571

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=28
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=76
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=16
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=19
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 7):
    Original:  [-0.0142, -0.0033, -0.0445, 0.0284]
    Predicted: [-0.0244, -0.1172, -0.0862, 0.0253]
    Δ (Delta): [-0.0103, -0.1139, -0.0418, -0.0030]
    MAPE: 918.53%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.438,  0.233,  0.225, -0.346,  0.104, -0.337, -0.705,  0.724]
    Predicted:
      [ 0.249,  0.073,  0.5  , -0.249, -0.073,  0.005, -0.46 ,  0.405]
    Δ (Delta):
      [-0.189, -0.159,  0.275,  0.097, -0.177,  0.343,  0.245, -0.32 ]
    MAPE: 76.53%, RMSE: 0.2391

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.756240

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.002557
    Max norm: 0.003182
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.013694
    Max norm: 0.393723
    Zero gradient ratio: 15.93%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.64s
  Backward pass: 13.07s
  Ratio (back/fwd): 4.9x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00339664, max_change=0.01325514
  earth4d: mean_change=0.06848576, max_change=0.07835454
  multimodal: mean_change=0.00610281, max_change=0.15887922
  perceiver: mean_change=0.01012349, max_change=0.06983121

================================================================================
BATCH 16/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 127 tokens (24.8%)
    Encoder_0: 128 tokens (25.0%)
    AlphaEarth: 257 tokens (50.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.103515625, 'data': 0.140625, 'dataset': 0.015625, 'modality': 0.0625, 'encoder': 0.068359375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2590

📉 LOSS BREAKDOWN:
  Total loss: 0.259001

  Universal Space Losses (256D token space):
    spacetime_loss: 0.003926
    data_loss: 0.043734
    dataset_loss: 0.006405
    encoder_loss: 0.001366

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000694
    bioclip_modality_loss: 0.080025
    alphaearth_modality_loss: 0.117553
    phenovision_modality_loss: 0.223410

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=53
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=72
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=32
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=35

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0091, -0.0158, -0.0379, 0.0196]
    Predicted: [0.0139, -0.0429, -0.0559, 0.0119]
    Δ (Delta): [0.0230, -0.0271, -0.0179, -0.0077]
    MAPE: 127.85%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.365, -0.009,  0.658, -0.129,  0.07 ,  0.083, -0.434,  0.559]
    Predicted:
      [ 0.236,  0.13 ,  0.525, -0.211, -0.068, -0.01 , -0.491,  0.417]
    Δ (Delta):
      [-0.129,  0.138, -0.133, -0.082, -0.138, -0.092, -0.057, -0.142]
    MAPE: 260.97%, RMSE: 0.1179

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.722482

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001180
    Max norm: 0.001515
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.015347
    Max norm: 0.297371
    Zero gradient ratio: 15.93%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.47s
  Backward pass: 13.06s
  Ratio (back/fwd): 5.3x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00312242, max_change=0.01188183
  earth4d: mean_change=0.06853587, max_change=0.07883964
  multimodal: mean_change=0.00567484, max_change=0.15074505
  perceiver: mean_change=0.00936936, max_change=0.06679240

================================================================================
BATCH 17/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 243 tokens (47.5%)
    Encoder_0: 124 tokens (24.2%)
    BioCLIP: 145 tokens (28.3%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.107421875, 'data': 0.12109375, 'dataset': 0.017578125, 'modality': 0.0546875, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2758

📉 LOSS BREAKDOWN:
  Total loss: 0.275772

  Universal Space Losses (256D token space):
    spacetime_loss: 0.003835
    data_loss: 0.047156
    dataset_loss: 0.000419
    encoder_loss: 0.001477

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.004037
    phenovision_modality_loss: 0.272973
    alphaearth_modality_loss: 0.112920
    bioclip_modality_loss: 0.062482

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=55
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=62
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=28
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0091, -0.0161, -0.0386, 0.0202]
    Predicted: [0.0155, 0.0140, -0.0733, 0.0078]
    Δ (Delta): [0.0246, 0.0302, -0.0347, -0.0124]
    MAPE: 151.94%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 8):
    Original (first 8 dims):
      [ 0.416,  0.187,  0.94 , -0.359, -0.203, -0.001, -0.66 ,  0.587]
    Predicted:
      [ 0.214,  0.1  ,  0.529, -0.172, -0.045,  0.015, -0.462,  0.457]
    Δ (Delta):
      [-0.202, -0.087, -0.412,  0.187,  0.157,  0.017,  0.198, -0.131]
    MAPE: 182.37%, RMSE: 0.2043
    🌸 Flowering Probability:
      Original:  0.416
      Predicted: 0.214
      Δ (Delta): -0.202

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.648134

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001590
    Max norm: 0.001960
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.012071
    Max norm: 0.325786
    Zero gradient ratio: 16.30%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.42s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00286044, max_change=0.01053434
  earth4d: mean_change=0.06908350, max_change=0.07995202
  multimodal: mean_change=0.00540608, max_change=0.15115753
  perceiver: mean_change=0.00856644, max_change=0.06244026

================================================================================
BATCH 18/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 104 tokens (20.3%)
    AlphaEarth: 272 tokens (53.1%)
    BioCLIP: 136 tokens (26.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.107421875, 'data': 0.1796875, 'dataset': 0.01953125, 'modality': 0.06640625, 'encoder': 0.029296875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2545

📉 LOSS BREAKDOWN:
  Total loss: 0.254476

  Universal Space Losses (256D token space):
    spacetime_loss: 0.003719
    data_loss: 0.043072
    dataset_loss: 0.003603
    encoder_loss: 0.001458

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.003445
    alphaearth_modality_loss: 0.125906
    bioclip_modality_loss: 0.060103
    phenovision_modality_loss: 0.227659

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=55
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=92
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=34
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=15

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 5):
    Original:  [-0.0096, -0.0152, -0.0395, 0.0212]
    Predicted: [0.0101, 0.0281, -0.0745, 0.0238]
    Δ (Delta): [0.0197, 0.0432, -0.0351, 0.0025]
    MAPE: 147.65%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [-0.114, -0.235,  0.655, -0.27 , -0.471, -0.534,  0.044, -0.004]
    Predicted:
      [ 0.214,  0.037,  0.522, -0.156, -0.025,  0.02 , -0.411,  0.492]
    Δ (Delta):
      [ 0.328,  0.272, -0.133,  0.114,  0.446,  0.553, -0.455,  0.496]
    MAPE: 1625.47%, RMSE: 0.3825

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.572892

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001666
    Max norm: 0.002244
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.011595
    Max norm: 0.226390
    Zero gradient ratio: 16.30%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.52s
  Backward pass: 13.12s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00270121, max_change=0.00982336
  earth4d: mean_change=0.06973439, max_change=0.08103675
  multimodal: mean_change=0.00499636, max_change=0.13348506
  perceiver: mean_change=0.00807048, max_change=0.05800718

================================================================================
BATCH 19/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 250 tokens (48.8%)
    BioCLIP: 119 tokens (23.2%)
    Encoder_0: 142 tokens (27.7%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.119140625, 'data': 0.1328125, 'dataset': 0.021484375, 'modality': 0.0390625, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2418

📉 LOSS BREAKDOWN:
  Total loss: 0.241793

  Universal Space Losses (256D token space):
    spacetime_loss: 0.004049
    data_loss: 0.039929
    dataset_loss: 0.003056
    encoder_loss: 0.001117

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000811
    phenovision_modality_loss: 0.210136
    alphaearth_modality_loss: 0.124917
    bioclip_modality_loss: 0.059580

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=61
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=68
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=20
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0142, -0.0036, -0.0453, 0.0287]
    Predicted: [0.0092, 0.0580, -0.0578, 0.0297]
    Δ (Delta): [0.0234, 0.0616, -0.0126, 0.0010]
    MAPE: 473.27%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 15):
    Original (first 8 dims):
      [-0.102, -0.006,  0.291,  0.137, -0.073, -0.485, -0.267,  0.217]
    Predicted:
      [ 0.225, -0.007,  0.522, -0.16 , -0.04 , -0.025, -0.383,  0.491]
    Δ (Delta):
      [ 0.327, -0.001,  0.231, -0.297,  0.033,  0.46 , -0.116,  0.275]
    MAPE: 117.61%, RMSE: 0.2622
    🌸 Flowering Probability:
      Original:  -0.102
      Predicted: 0.225
      Δ (Delta): 0.327

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.517424

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001477
    Max norm: 0.001956
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.010256
    Max norm: 0.213141
    Zero gradient ratio: 16.30%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.45s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.3x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00252719, max_change=0.00913155
  earth4d: mean_change=0.07059834, max_change=0.08205131
  multimodal: mean_change=0.00467889, max_change=0.12475863
  perceiver: mean_change=0.00732361, max_change=0.05289653

================================================================================
BATCH 20/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 259 tokens (50.6%)
    Encoder_0: 130 tokens (25.4%)
    BioCLIP: 123 tokens (24.0%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.10546875, 'data': 0.1796875, 'dataset': 0.017578125, 'modality': 0.052734375, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2861

📉 LOSS BREAKDOWN:
  Total loss: 0.286132

  Universal Space Losses (256D token space):
    spacetime_loss: 0.004172
    data_loss: 0.042106
    dataset_loss: 0.000597
    encoder_loss: 0.001288

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.002124
    bioclip_modality_loss: 0.058084
    alphaearth_modality_loss: 0.128558
    phenovision_modality_loss: 0.292262

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=54
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=92
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=27
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 7):
    Original:  [-0.0111, -0.0114, -0.0416, 0.0238]
    Predicted: [0.0158, 0.0965, -0.0484, 0.0310]
    Δ (Delta): [0.0268, 0.1078, -0.0068, 0.0072]
    MAPE: 309.76%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.433,  0.15 ,  0.446, -0.531, -0.151,  0.234, -0.516,  0.521]
    Predicted:
      [ 0.254,  0.012,  0.465, -0.146, -0.052, -0.03 , -0.402,  0.481]
    Δ (Delta):
      [-0.179, -0.138,  0.019,  0.385,  0.099, -0.264,  0.114, -0.041]
    MAPE: 52.30%, RMSE: 0.1916

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.577131

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.002056
    Max norm: 0.002794
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.008914
    Max norm: 0.323863
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.65s
  Backward pass: 13.13s
  Ratio (back/fwd): 5.0x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00234046, max_change=0.00829216
  earth4d: mean_change=0.07097759, max_change=0.08210219
  multimodal: mean_change=0.00433837, max_change=0.11447748
  perceiver: mean_change=0.00667478, max_change=0.04928249

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.004172 (trend: increasing)
  data_loss: 0.042106 (trend: decreasing)
  dataset_loss: 0.000597 (trend: decreasing)
  modality_loss: 0.002124 (trend: increasing)
  encoder_loss: 0.001288 (trend: decreasing)
  phenovision_modality_loss: 0.292262 (trend: increasing)
  bioclip_modality_loss: 0.058084 (trend: decreasing)
  alphaearth_modality_loss: 0.128558 (trend: increasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.002056, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.008914, zero_ratio=17.04%

================================================================================
BATCH 21/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 131 tokens (25.6%)
    BioCLIP: 129 tokens (25.2%)
    AlphaEarth: 252 tokens (49.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.130859375, 'data': 0.166015625, 'dataset': 0.015625, 'modality': 0.03515625, 'encoder': 0.0625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2466

📉 LOSS BREAKDOWN:
  Total loss: 0.246644

  Universal Space Losses (256D token space):
    spacetime_loss: 0.003853
    data_loss: 0.041484
    dataset_loss: 0.001642
    encoder_loss: 0.001834

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.002182
    bioclip_modality_loss: 0.053300
    phenovision_modality_loss: 0.232720
    alphaearth_modality_loss: 0.115462

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=67
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=85
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=18
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=32

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0126, -0.0088, -0.0451, 0.0272]
    Predicted: [-0.0063, 0.1157, -0.0077, 0.0312]
    Δ (Delta): [0.0063, 0.1245, 0.0373, 0.0040]
    MAPE: 390.37%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [-0.022,  0.043,  0.454, -0.221,  0.157,  0.172, -0.447,  0.308]
    Predicted:
      [ 0.266,  0.062,  0.508, -0.13 , -0.062, -0.057, -0.425,  0.458]
    Δ (Delta):
      [ 0.287,  0.019,  0.055,  0.091, -0.22 , -0.229,  0.022,  0.15 ]
    MAPE: 218.53%, RMSE: 0.1650

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.518448

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001488
    Max norm: 0.001856
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.008499
    Max norm: 0.282769
    Zero gradient ratio: 16.30%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.48s
  Backward pass: 13.09s
  Ratio (back/fwd): 5.3x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00226045, max_change=0.00758516
  earth4d: mean_change=0.07156368, max_change=0.08257578
  multimodal: mean_change=0.00399953, max_change=0.10817572
  perceiver: mean_change=0.00635656, max_change=0.04742405

================================================================================
BATCH 22/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 247 tokens (48.2%)
    Encoder_0: 121 tokens (23.6%)
    BioCLIP: 144 tokens (28.1%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.12109375, 'data': 0.15625, 'dataset': 0.021484375, 'modality': 0.03515625, 'encoder': 0.052734375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2388

📉 LOSS BREAKDOWN:
  Total loss: 0.238845

  Universal Space Losses (256D token space):
    spacetime_loss: 0.003239
    data_loss: 0.039303
    dataset_loss: 0.000256
    encoder_loss: 0.001977

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000160
    bioclip_modality_loss: 0.050573
    alphaearth_modality_loss: 0.125609
    phenovision_modality_loss: 0.215944

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=62
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=80
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=18
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=27

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 6):
    Original:  [-0.0107, -0.0134, -0.0429, 0.0247]
    Predicted: [-0.0249, 0.0502, 0.0297, 0.0457]
    Δ (Delta): [-0.0141, 0.0636, 0.0726, 0.0210]
    MAPE: 215.18%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 14):
    Original (first 8 dims):
      [ 0.481, -0.161,  0.716, -0.235, -0.134,  0.109, -0.135,  0.589]
    Predicted:
      [ 0.258,  0.104,  0.553, -0.114, -0.063, -0.082, -0.451,  0.453]
    Δ (Delta):
      [-0.223,  0.265, -0.163,  0.121,  0.071, -0.191, -0.316, -0.136]
    MAPE: 96.19%, RMSE: 0.2002

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.443610

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001246
    Max norm: 0.001463
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.008115
    Max norm: 0.203078
    Zero gradient ratio: 16.30%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.45s
  Backward pass: 13.08s
  Ratio (back/fwd): 5.3x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00214916, max_change=0.00690546
  earth4d: mean_change=0.07245558, max_change=0.08344976
  multimodal: mean_change=0.00377949, max_change=0.10431126
  perceiver: mean_change=0.00606326, max_change=0.04586186

================================================================================
BATCH 23/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 272 tokens (53.1%)
    Encoder_0: 118 tokens (23.0%)
    BioCLIP: 122 tokens (23.8%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.103515625, 'data': 0.162109375, 'dataset': 0.015625, 'modality': 0.0546875, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2476

📉 LOSS BREAKDOWN:
  Total loss: 0.247571

  Universal Space Losses (256D token space):
    spacetime_loss: 0.002782
    data_loss: 0.036142
    dataset_loss: 0.001620
    encoder_loss: 0.001683

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.002729
    bioclip_modality_loss: 0.049712
    phenovision_modality_loss: 0.242033
    alphaearth_modality_loss: 0.124342

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=53
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=83
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=28
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0139, -0.0037, -0.0450, 0.0282]
    Predicted: [-0.0365, 0.0008, 0.0015, 0.0014]
    Δ (Delta): [-0.0226, 0.0045, 0.0466, -0.0268]
    MAPE: 120.62%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [ 0.236, -0.024,  0.555, -0.252, -0.181, -0.115, -0.29 ,  0.597]
    Predicted:
      [ 0.261,  0.067,  0.568, -0.149, -0.026, -0.03 , -0.431,  0.477]
    Δ (Delta):
      [ 0.025,  0.091,  0.013,  0.103,  0.155,  0.085, -0.142, -0.12 ]
    MAPE: 83.18%, RMSE: 0.1031

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.433167

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001839
    Max norm: 0.002127
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.007075
    Max norm: 0.186942
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.48s
  Backward pass: 13.10s
  Ratio (back/fwd): 5.3x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00222812, max_change=0.00755107
  earth4d: mean_change=0.07271618, max_change=0.08396667
  multimodal: mean_change=0.00356649, max_change=0.09847164
  perceiver: mean_change=0.00579118, max_change=0.04284106

================================================================================
BATCH 24/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 126 tokens (24.6%)
    AlphaEarth: 258 tokens (50.4%)
    BioCLIP: 127 tokens (24.8%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.107421875, 'data': 0.1484375, 'dataset': 0.0234375, 'modality': 0.046875, 'encoder': 0.048828125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2358

📉 LOSS BREAKDOWN:
  Total loss: 0.235848

  Universal Space Losses (256D token space):
    spacetime_loss: 0.002812
    data_loss: 0.033255
    dataset_loss: 0.003062
    encoder_loss: 0.000575

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000923
    alphaearth_modality_loss: 0.121180
    bioclip_modality_loss: 0.051871
    phenovision_modality_loss: 0.225600

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=55
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=76
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=24
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=25

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 9):
    Original:  [-0.0140, -0.0031, -0.0447, 0.0282]
    Predicted: [-0.0362, -0.0527, 0.0141, 0.0080]
    Δ (Delta): [-0.0222, -0.0496, 0.0588, -0.0202]
    MAPE: 491.11%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.87 ,  0.214,  0.569,  0.104, -0.321, -0.283, -0.597,  0.533]
    Predicted:
      [ 0.234,  0.096,  0.563, -0.151, -0.019, -0.026, -0.405,  0.466]
    Δ (Delta):
      [-0.636, -0.118, -0.005, -0.254,  0.302,  0.256,  0.192, -0.067]
    MAPE: 75.49%, RMSE: 0.2919

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.411787

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001361
    Max norm: 0.001601
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.006829
    Max norm: 0.188728
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.63s
  Backward pass: 13.06s
  Ratio (back/fwd): 5.0x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00208737, max_change=0.00701067
  earth4d: mean_change=0.07281569, max_change=0.08413202
  multimodal: mean_change=0.00334306, max_change=0.08995322
  perceiver: mean_change=0.00535447, max_change=0.04098474

================================================================================
BATCH 25/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 123 tokens (24.0%)
    Encoder_0: 134 tokens (26.2%)
    AlphaEarth: 254 tokens (49.6%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.099609375, 'data': 0.130859375, 'dataset': 0.01953125, 'modality': 0.033203125, 'encoder': 0.048828125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1722

📉 LOSS BREAKDOWN:
  Total loss: 0.172195

  Universal Space Losses (256D token space):
    spacetime_loss: 0.002728
    data_loss: 0.032138
    dataset_loss: 0.000697
    encoder_loss: 0.001024

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000770
    bioclip_modality_loss: 0.047658
    alphaearth_modality_loss: 0.114403
    phenovision_modality_loss: 0.112098

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=51
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=67
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=17
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=25

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 6):
    Original:  [-0.0086, -0.0161, -0.0378, 0.0192]
    Predicted: [-0.0135, -0.0770, 0.0010, 0.0157]
    Δ (Delta): [-0.0048, -0.0609, 0.0388, -0.0035]
    MAPE: 139.04%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 6):
    Original (first 8 dims):
      [ 0.44 ,  0.191,  0.334, -0.136, -0.119, -0.043, -0.474,  0.505]
    Predicted:
      [ 0.216,  0.1  ,  0.53 , -0.147, -0.029, -0.023, -0.382,  0.461]
    Δ (Delta):
      [-0.225, -0.091,  0.196, -0.011,  0.09 ,  0.02 ,  0.093, -0.043]
    MAPE: 39.56%, RMSE: 0.1206

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.368570

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001193
    Max norm: 0.001428
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.006200
    Max norm: 0.163084
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.44s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.3x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00192436, max_change=0.00635426
  earth4d: mean_change=0.07235358, max_change=0.08375742
  multimodal: mean_change=0.00317407, max_change=0.08637842
  perceiver: mean_change=0.00524859, max_change=0.03980049

================================================================================
BATCH 26/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 245 tokens (47.9%)
    BioCLIP: 153 tokens (29.9%)
    Encoder_0: 114 tokens (22.3%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.125, 'data': 0.1484375, 'dataset': 0.013671875, 'modality': 0.046875, 'encoder': 0.0390625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2513

📉 LOSS BREAKDOWN:
  Total loss: 0.251272

  Universal Space Losses (256D token space):
    spacetime_loss: 0.002388
    data_loss: 0.040415
    dataset_loss: 0.000557
    encoder_loss: 0.000424

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000921
    phenovision_modality_loss: 0.253707
    alphaearth_modality_loss: 0.117508
    bioclip_modality_loss: 0.045343

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=64
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=76
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=7
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=24
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=20

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 7):
    Original:  [-0.0084, -0.0166, -0.0377, 0.0191]
    Predicted: [0.0076, -0.0475, -0.0216, 0.0165]
    Δ (Delta): [0.0161, -0.0308, 0.0161, -0.0027]
    MAPE: 108.04%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.346, -0.067,  0.443,  0.123, -0.166,  0.045, -0.222,  0.322]
    Predicted:
      [ 0.227,  0.121,  0.513, -0.157, -0.052, -0.026, -0.395,  0.482]
    Δ (Delta):
      [-0.119,  0.187,  0.07 , -0.279,  0.114, -0.071, -0.173,  0.16 ]
    MAPE: 113.98%, RMSE: 0.1604
    🌸 Flowering Probability:
      Original:  0.346
      Predicted: 0.227
      Δ (Delta): -0.119

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.495587

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001245
    Max norm: 0.001478
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.007810
    Max norm: 0.296058
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.45s
  Backward pass: 13.05s
  Ratio (back/fwd): 5.3x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00188574, max_change=0.00602409
  earth4d: mean_change=0.07317965, max_change=0.08453688
  multimodal: mean_change=0.00296788, max_change=0.08392853
  perceiver: mean_change=0.00480641, max_change=0.03725691

================================================================================
BATCH 27/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 260 tokens (50.8%)
    Encoder_0: 141 tokens (27.5%)
    BioCLIP: 108 tokens (21.1%)
    Earth4D: 3 tokens (0.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.103515625, 'data': 0.154296875, 'dataset': 0.025390625, 'modality': 0.0546875, 'encoder': 0.04296875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2371

📉 LOSS BREAKDOWN:
  Total loss: 0.237135

  Universal Space Losses (256D token space):
    spacetime_loss: 0.002447
    data_loss: 0.037523
    dataset_loss: 0.002206
    encoder_loss: 0.000573

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.001281
    bioclip_modality_loss: 0.043087
    phenovision_modality_loss: 0.242434
    alphaearth_modality_loss: 0.107996

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=53
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=79
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=13
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=28
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=22

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0097, -0.0140, -0.0399, 0.0220]
    Predicted: [0.0282, -0.0275, -0.0312, 0.0110]
    Δ (Delta): [0.0379, -0.0136, 0.0086, -0.0110]
    MAPE: 139.79%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 6):
    Original (first 8 dims):
      [ 0.335,  0.18 ,  0.738, -0.077,  0.096, -0.053, -0.423,  0.465]
    Predicted:
      [ 0.226,  0.129,  0.488, -0.163, -0.083, -0.052, -0.411,  0.478]
    Δ (Delta):
      [-0.109, -0.052, -0.25 , -0.086, -0.178,  0.001,  0.012,  0.012]
    MAPE: 50.18%, RMSE: 0.1207

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.424172

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001383
    Max norm: 0.001866
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.006985
    Max norm: 0.246077
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.46s
  Backward pass: 13.07s
  Ratio (back/fwd): 5.3x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00197284, max_change=0.00633388
  earth4d: mean_change=0.07337762, max_change=0.08486397
  multimodal: mean_change=0.00279540, max_change=0.08226308
  perceiver: mean_change=0.00468462, max_change=0.03434242

================================================================================
BATCH 28/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 113 tokens (22.1%)
    AlphaEarth: 242 tokens (47.3%)
    Encoder_0: 156 tokens (30.5%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.115234375, 'data': 0.15625, 'dataset': 0.017578125, 'modality': 0.060546875, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2116

📉 LOSS BREAKDOWN:
  Total loss: 0.211615

  Universal Space Losses (256D token space):
    spacetime_loss: 0.002206
    data_loss: 0.034514
    dataset_loss: 0.001572
    encoder_loss: 0.000705

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000180
    phenovision_modality_loss: 0.192521
    bioclip_modality_loss: 0.044229
    alphaearth_modality_loss: 0.112551

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=59
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=80
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=31
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 6):
    Original:  [-0.0140, -0.0032, -0.0450, 0.0288]
    Predicted: [0.0048, -0.0066, -0.0431, 0.0090]
    Δ (Delta): [0.0188, -0.0034, 0.0019, -0.0198]
    MAPE: 78.67%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.03 ,  0.187,  0.206, -0.267, -0.34 ,  0.114, -0.325,  0.706]
    Predicted:
      [ 0.227,  0.113,  0.487, -0.168, -0.095, -0.098, -0.412,  0.448]
    Δ (Delta):
      [ 0.197, -0.074,  0.281,  0.099,  0.245, -0.211, -0.087, -0.258]
    MAPE: 147.39%, RMSE: 0.1974
    🌸 Flowering Probability:
      Original:  0.030
      Predicted: 0.227
      Δ (Delta): 0.197

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.307686

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000901
    Max norm: 0.001307
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.005388
    Max norm: 0.149423
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.60s
  Backward pass: 13.08s
  Ratio (back/fwd): 5.0x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00181850, max_change=0.00564976
  earth4d: mean_change=0.07347075, max_change=0.08528314
  multimodal: mean_change=0.00268069, max_change=0.07722183
  perceiver: mean_change=0.00440150, max_change=0.03158364

================================================================================
BATCH 29/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 126 tokens (24.6%)
    Encoder_0: 111 tokens (21.7%)
    AlphaEarth: 274 tokens (53.5%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.087890625, 'data': 0.115234375, 'dataset': 0.033203125, 'modality': 0.052734375, 'encoder': 0.044921875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2148

📉 LOSS BREAKDOWN:
  Total loss: 0.214834

  Universal Space Losses (256D token space):
    spacetime_loss: 0.002225
    data_loss: 0.035535
    dataset_loss: 0.000390
    encoder_loss: 0.000744

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000780
    phenovision_modality_loss: 0.201090
    bioclip_modality_loss: 0.041892
    alphaearth_modality_loss: 0.110783

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=45
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=59
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=17
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=27
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=23

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 12):
    Original:  [-0.0083, -0.0166, -0.0372, 0.0189]
    Predicted: [-0.0105, -0.0147, -0.0745, 0.0072]
    Δ (Delta): [-0.0022, 0.0018, -0.0373, -0.0117]
    MAPE: 49.85%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 6):
    Original (first 8 dims):
      [ 0.355,  0.549,  0.447, -0.281, -0.3  ,  0.042, -0.065,  0.932]
    Predicted:
      [ 0.237,  0.084,  0.517, -0.174, -0.088, -0.118, -0.402,  0.432]
    Δ (Delta):
      [-0.118, -0.465,  0.07 ,  0.107,  0.212, -0.159, -0.336, -0.5  ]
    MAPE: 149.17%, RMSE: 0.2915
    🌸 Flowering Probability:
      Original:  0.355
      Predicted: 0.237
      Δ (Delta): -0.118

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.444889

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000988
    Max norm: 0.001112
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.006992
    Max norm: 0.229334
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00176408, max_change=0.00555339
  earth4d: mean_change=0.07315129, max_change=0.08494630
  multimodal: mean_change=0.00257232, max_change=0.07181613
  perceiver: mean_change=0.00430269, max_change=0.02931376

================================================================================
BATCH 30/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 130 tokens (25.4%)
    AlphaEarth: 271 tokens (52.9%)
    Encoder_0: 108 tokens (21.1%)
    Earth4D: 3 tokens (0.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1171875, 'data': 0.1796875, 'dataset': 0.015625, 'modality': 0.052734375, 'encoder': 0.033203125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2087

📉 LOSS BREAKDOWN:
  Total loss: 0.208671

  Universal Space Losses (256D token space):
    spacetime_loss: 0.002162
    data_loss: 0.035380
    dataset_loss: 0.000224
    encoder_loss: 0.000260

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.001110
    alphaearth_modality_loss: 0.111248
    bioclip_modality_loss: 0.041749
    phenovision_modality_loss: 0.188943

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=60
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=92
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=27
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=17

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 9):
    Original:  [-0.0108, -0.0127, -0.0428, 0.0254]
    Predicted: [-0.0177, -0.0396, -0.0797, 0.0067]
    Δ (Delta): [-0.0069, -0.0269, -0.0369, -0.0187]
    MAPE: 109.30%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 11):
    Original (first 8 dims):
      [ 0.169,  0.32 ,  0.475, -0.001,  0.054,  0.115, -0.518,  0.434]
    Predicted:
      [ 0.217,  0.023,  0.479, -0.165, -0.082, -0.1  , -0.335,  0.385]
    Δ (Delta):
      [ 0.048, -0.297,  0.004, -0.164, -0.136, -0.214,  0.182, -0.049]
    MAPE: 2926.66%, RMSE: 0.1649

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.317167

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001060
    Max norm: 0.001251
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.005420
    Max norm: 0.134980
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.53s
  Backward pass: 13.12s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00157655, max_change=0.00476479
  earth4d: mean_change=0.07398283, max_change=0.08583849
  multimodal: mean_change=0.00241861, max_change=0.06864559
  perceiver: mean_change=0.00391905, max_change=0.02804742

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.002162 (trend: decreasing)
  data_loss: 0.035380 (trend: decreasing)
  dataset_loss: 0.000224 (trend: decreasing)
  modality_loss: 0.001110 (trend: increasing)
  encoder_loss: 0.000260 (trend: decreasing)
  phenovision_modality_loss: 0.188943 (trend: decreasing)
  bioclip_modality_loss: 0.041749 (trend: decreasing)
  alphaearth_modality_loss: 0.111248 (trend: decreasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.001060, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.005420, zero_ratio=17.04%

================================================================================
BATCH 31/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 124 tokens (24.2%)
    AlphaEarth: 248 tokens (48.4%)
    BioCLIP: 140 tokens (27.3%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.107421875, 'data': 0.142578125, 'dataset': 0.021484375, 'modality': 0.0703125, 'encoder': 0.048828125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2002

📉 LOSS BREAKDOWN:
  Total loss: 0.200191

  Universal Space Losses (256D token space):
    spacetime_loss: 0.002016
    data_loss: 0.032398
    dataset_loss: 0.000490
    encoder_loss: 0.000640

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000155
    bioclip_modality_loss: 0.042730
    phenovision_modality_loss: 0.177894
    alphaearth_modality_loss: 0.110673

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=55
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=73
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=36
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=25

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 6):
    Original:  [-0.0028, -0.0304, -0.0282, -0.0014]
    Predicted: [-0.0282, -0.0478, -0.0875, 0.0066]
    Δ (Delta): [-0.0253, -0.0174, -0.0593, 0.0080]
    MAPE: 435.06%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 1):
    Original (first 8 dims):
      [ 0.332,  0.045,  0.56 , -0.055, -0.083, -0.001, -0.393,  0.436]
    Predicted:
      [ 0.229,  0.028,  0.508, -0.139, -0.061, -0.083, -0.327,  0.419]
    Δ (Delta):
      [-0.102, -0.017, -0.052, -0.085,  0.021, -0.082,  0.066, -0.016]
    MAPE: 1387.33%, RMSE: 0.0637

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.318018

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001160
    Max norm: 0.001470
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.005219
    Max norm: 0.170032
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.45s
  Backward pass: 13.06s
  Ratio (back/fwd): 5.3x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00139392, max_change=0.00392215
  earth4d: mean_change=0.07457758, max_change=0.08635665
  multimodal: mean_change=0.00231974, max_change=0.06947560
  perceiver: mean_change=0.00374145, max_change=0.02751825

================================================================================
BATCH 32/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 129 tokens (25.2%)
    AlphaEarth: 258 tokens (50.4%)
    Encoder_0: 124 tokens (24.2%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.130859375, 'data': 0.12109375, 'dataset': 0.015625, 'modality': 0.048828125, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1941

📉 LOSS BREAKDOWN:
  Total loss: 0.194096

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001884
    data_loss: 0.031213
    dataset_loss: 0.000625
    encoder_loss: 0.000561

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000768
    bioclip_modality_loss: 0.039512
    alphaearth_modality_loss: 0.125926
    phenovision_modality_loss: 0.156168

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=67
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=62
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=25
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 7):
    Original:  [-0.0095, -0.0137, -0.0390, 0.0214]
    Predicted: [-0.0215, -0.0579, -0.0654, 0.0223]
    Δ (Delta): [-0.0120, -0.0442, -0.0264, 0.0010]
    MAPE: 130.11%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.363,  0.019,  0.463, -0.22 , -0.13 , -0.176, -0.603,  0.27 ]
    Predicted:
      [ 0.251,  0.059,  0.527, -0.121, -0.031, -0.043, -0.33 ,  0.462]
    Δ (Delta):
      [-0.111,  0.04 ,  0.064,  0.1  ,  0.099,  0.132,  0.273,  0.192]
    MAPE: 71.25%, RMSE: 0.1443

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.322105

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001071
    Max norm: 0.001110
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.005234
    Max norm: 0.194123
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.40s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00140993, max_change=0.00400914
  earth4d: mean_change=0.07575417, max_change=0.08791088
  multimodal: mean_change=0.00222408, max_change=0.06708266
  perceiver: mean_change=0.00367862, max_change=0.02649485

================================================================================
BATCH 33/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 251 tokens (49.0%)
    Encoder_0: 130 tokens (25.4%)
    BioCLIP: 128 tokens (25.0%)
    Earth4D: 3 tokens (0.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1015625, 'data': 0.146484375, 'dataset': 0.02734375, 'modality': 0.05859375, 'encoder': 0.041015625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1731

📉 LOSS BREAKDOWN:
  Total loss: 0.173093

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001875
    data_loss: 0.031429
    dataset_loss: 0.000585
    encoder_loss: 0.000170

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000857
    alphaearth_modality_loss: 0.116576
    bioclip_modality_loss: 0.038010
    phenovision_modality_loss: 0.124670

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=52
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=75
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=14
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=30
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=21

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 7):
    Original:  [-0.0088, -0.0156, -0.0385, 0.0205]
    Predicted: [-0.0158, -0.0394, -0.0739, 0.0485]
    Δ (Delta): [-0.0069, -0.0237, -0.0354, 0.0280]
    MAPE: 114.72%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.04 ,  0.126,  0.538, -0.317, -0.194, -0.261, -0.291,  0.439]
    Predicted:
      [ 0.249,  0.092,  0.536, -0.116, -0.026, -0.009, -0.355,  0.478]
    Δ (Delta):
      [ 0.209, -0.034, -0.002,  0.201,  0.169,  0.252, -0.064,  0.039]
    MAPE: 103.56%, RMSE: 0.1513

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.325189

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000990
    Max norm: 0.001074
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.005489
    Max norm: 0.146955
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.61s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.0x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00143124, max_change=0.00419209
  earth4d: mean_change=0.07566586, max_change=0.08791378
  multimodal: mean_change=0.00213616, max_change=0.06275189
  perceiver: mean_change=0.00346032, max_change=0.02495078

================================================================================
BATCH 34/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 249 tokens (48.6%)
    BioCLIP: 138 tokens (27.0%)
    Encoder_0: 124 tokens (24.2%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09765625, 'data': 0.1484375, 'dataset': 0.01953125, 'modality': 0.07421875, 'encoder': 0.044921875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2647

📉 LOSS BREAKDOWN:
  Total loss: 0.264686

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001953
    data_loss: 0.033451
    dataset_loss: 0.000197
    encoder_loss: 0.000778

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000688
    bioclip_modality_loss: 0.038923
    phenovision_modality_loss: 0.309475
    alphaearth_modality_loss: 0.109833

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=50
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=76
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=38
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=23

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0094, -0.0148, -0.0395, 0.0217]
    Predicted: [-0.0236, -0.0275, -0.0550, 0.0630]
    Δ (Delta): [-0.0143, -0.0128, -0.0155, 0.0413]
    MAPE: 117.07%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.062,  0.221,  0.435, -0.292, -0.104, -0.292, -0.385,  0.328]
    Predicted:
      [ 0.237,  0.125,  0.503, -0.132, -0.031,  0.01 , -0.359,  0.454]
    Δ (Delta):
      [ 0.175, -0.096,  0.068,  0.16 ,  0.073,  0.303,  0.027,  0.126]
    MAPE: 76.75%, RMSE: 0.1515

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.663329

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.001070
    Max norm: 0.001233
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.007542
    Max norm: 0.460559
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.44s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00159586, max_change=0.00444251
  earth4d: mean_change=0.07533164, max_change=0.08752430
  multimodal: mean_change=0.00208593, max_change=0.06243319
  perceiver: mean_change=0.00358032, max_change=0.02519272

================================================================================
BATCH 35/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 244 tokens (47.7%)
    Encoder_0: 143 tokens (27.9%)
    BioCLIP: 124 tokens (24.2%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.08203125, 'data': 0.162109375, 'dataset': 0.029296875, 'modality': 0.048828125, 'encoder': 0.060546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2223

📉 LOSS BREAKDOWN:
  Total loss: 0.222260

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001975
    data_loss: 0.029021
    dataset_loss: 0.000106
    encoder_loss: 0.000125

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000464
    alphaearth_modality_loss: 0.104855
    bioclip_modality_loss: 0.036594
    phenovision_modality_loss: 0.240941

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=42
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=83
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=15
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=25
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=31

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 2, seq 6):
    Original:  [-0.0116, -0.0107, -0.0437, 0.0263]
    Predicted: [-0.0216, -0.0069, -0.0641, 0.0482]
    Δ (Delta): [-0.0100, 0.0039, -0.0204, 0.0219]
    MAPE: 63.05%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.369, -0.015,  0.651, -0.197,  0.208, -0.187, -0.302,  0.457]
    Predicted:
      [ 0.232,  0.141,  0.515, -0.152, -0.065, -0.008, -0.368,  0.411]
    Δ (Delta):
      [-0.138,  0.157, -0.137,  0.045, -0.273,  0.179, -0.066, -0.045]
    MAPE: 170.00%, RMSE: 0.1487

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.473087

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000957
    Max norm: 0.001243
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.005875
    Max norm: 0.344510
    Zero gradient ratio: 17.41%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.44s
  Backward pass: 13.07s
  Ratio (back/fwd): 5.3x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00152116, max_change=0.00423176
  earth4d: mean_change=0.07474886, max_change=0.08688174
  multimodal: mean_change=0.00199168, max_change=0.06337235
  perceiver: mean_change=0.00333843, max_change=0.02370243

================================================================================
BATCH 36/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 261 tokens (51.0%)
    Encoder_0: 127 tokens (24.8%)
    BioCLIP: 123 tokens (24.0%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.099609375, 'data': 0.16015625, 'dataset': 0.02734375, 'modality': 0.046875, 'encoder': 0.03515625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2337

📉 LOSS BREAKDOWN:
  Total loss: 0.233749

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001883
    data_loss: 0.028595
    dataset_loss: 0.000347
    encoder_loss: 0.000337

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.001560
    bioclip_modality_loss: 0.036578
    phenovision_modality_loss: 0.252110
    alphaearth_modality_loss: 0.117403

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=51
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=82
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=14
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=24
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=18

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0084, -0.0166, -0.0376, 0.0195]
    Predicted: [0.0036, -0.0166, -0.0406, 0.0256]
    Δ (Delta): [0.0120, 0.0000, -0.0030, 0.0061]
    MAPE: 45.53%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.414,  0.164,  0.392, -0.286, -0.12 ,  0.196, -0.369,  0.237]
    Predicted:
      [ 0.201,  0.112,  0.458, -0.174, -0.084, -0.02 , -0.33 ,  0.349]
    Δ (Delta):
      [-0.213, -0.053,  0.066,  0.112,  0.036, -0.216,  0.039,  0.112]
    MAPE: 42.17%, RMSE: 0.1261

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.444753

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000889
    Max norm: 0.001074
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.005747
    Max norm: 0.302369
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.44s
  Backward pass: 13.05s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00140681, max_change=0.00377902
  earth4d: mean_change=0.07542269, max_change=0.08729201
  multimodal: mean_change=0.00185405, max_change=0.05875224
  perceiver: mean_change=0.00310868, max_change=0.02233456

================================================================================
BATCH 37/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 128 tokens (25.0%)
    AlphaEarth: 261 tokens (51.0%)
    BioCLIP: 123 tokens (24.0%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.087890625, 'data': 0.15234375, 'dataset': 0.01171875, 'modality': 0.04296875, 'encoder': 0.072265625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2293

📉 LOSS BREAKDOWN:
  Total loss: 0.229254

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001734
    data_loss: 0.030139
    dataset_loss: 0.000111
    encoder_loss: 0.000243

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000941
    bioclip_modality_loss: 0.035770
    phenovision_modality_loss: 0.252924
    alphaearth_modality_loss: 0.105808

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=45
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=78
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=6
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=37

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0128, -0.0056, -0.0426, 0.0266]
    Predicted: [-0.0045, -0.0124, -0.0150, -0.0010]
    Δ (Delta): [0.0082, -0.0068, 0.0276, -0.0276]
    MAPE: 88.58%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 11):
    Original (first 8 dims):
      [ 0.086,  0.275,  0.531, -0.395, -0.346,  0.083, -0.308,  0.625]
    Predicted:
      [ 0.213,  0.119,  0.479, -0.17 , -0.092, -0.054, -0.315,  0.338]
    Δ (Delta):
      [ 0.126, -0.155, -0.052,  0.226,  0.254, -0.137, -0.008, -0.287]
    MAPE: 69.61%, RMSE: 0.1801

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.401165

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000781
    Max norm: 0.001018
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.005944
    Max norm: 0.240654
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.56s
  Backward pass: 13.03s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00129110, max_change=0.00321404
  earth4d: mean_change=0.07534051, max_change=0.08722945
  multimodal: mean_change=0.00180801, max_change=0.05558010
  perceiver: mean_change=0.00316734, max_change=0.02212496

================================================================================
BATCH 38/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 254 tokens (49.6%)
    Encoder_0: 133 tokens (26.0%)
    BioCLIP: 124 tokens (24.2%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.078125, 'data': 0.13671875, 'dataset': 0.015625, 'modality': 0.02734375, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2337

📉 LOSS BREAKDOWN:
  Total loss: 0.233688

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001636
    data_loss: 0.025730
    dataset_loss: 0.000131
    encoder_loss: 0.000344

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000503
    bioclip_modality_loss: 0.033755
    phenovision_modality_loss: 0.268151
    alphaearth_modality_loss: 0.110542

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=40
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=70
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=14
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0080, -0.0175, -0.0367, 0.0184]
    Predicted: [-0.0143, -0.0199, 0.0131, -0.0125]
    Δ (Delta): [-0.0063, -0.0025, 0.0498, -0.0309]
    MAPE: 99.05%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 9):
    Original (first 8 dims):
      [ 0.129,  0.09 ,  0.384, -0.154,  0.129,  0.053, -0.471,  0.373]
    Predicted:
      [ 0.227,  0.114,  0.472, -0.157, -0.067, -0.054, -0.289,  0.374]
    Δ (Delta):
      [ 0.099,  0.023,  0.088, -0.003, -0.197, -0.107,  0.182,  0.   ]
    MAPE: 65.24%, RMSE: 0.1125

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.453986

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000735
    Max norm: 0.000908
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.005904
    Max norm: 0.279785
    Zero gradient ratio: 17.41%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.42s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00120059, max_change=0.00290882
  earth4d: mean_change=0.07465122, max_change=0.08644358
  multimodal: mean_change=0.00178237, max_change=0.05408926
  perceiver: mean_change=0.00286428, max_change=0.02131987

================================================================================
BATCH 39/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 137 tokens (26.8%)
    AlphaEarth: 251 tokens (49.0%)
    Encoder_0: 123 tokens (24.0%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.095703125, 'data': 0.146484375, 'dataset': 0.013671875, 'modality': 0.05859375, 'encoder': 0.037109375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1972

📉 LOSS BREAKDOWN:
  Total loss: 0.197201

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001657
    data_loss: 0.028865
    dataset_loss: 0.000242
    encoder_loss: 0.000235

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000757
    phenovision_modality_loss: 0.196312
    bioclip_modality_loss: 0.035019
    alphaearth_modality_loss: 0.101779

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=49
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=75
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=7
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=30
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=19

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0084, -0.0169, -0.0377, 0.0197]
    Predicted: [-0.0181, -0.0163, 0.0212, -0.0251]
    Δ (Delta): [-0.0097, 0.0006, 0.0589, -0.0448]
    MAPE: 125.82%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 1):
    Original (first 8 dims):
      [ 0.479,  0.263,  0.516, -0.098, -0.128, -0.072, -0.326,  0.349]
    Predicted:
      [ 0.223,  0.108,  0.469, -0.13 , -0.061, -0.056, -0.285,  0.406]
    Δ (Delta):
      [-0.256, -0.155, -0.047, -0.032,  0.067,  0.016,  0.041,  0.057]
    MAPE: 32.13%, RMSE: 0.1130
    🌸 Flowering Probability:
      Original:  0.479
      Predicted: 0.223
      Δ (Delta): -0.256

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.278521

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000825
    Max norm: 0.000966
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.004282
    Max norm: 0.165405
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.41s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00122633, max_change=0.00258271
  earth4d: mean_change=0.07504450, max_change=0.08673439
  multimodal: mean_change=0.00173219, max_change=0.05315612
  perceiver: mean_change=0.00282423, max_change=0.02019049

================================================================================
BATCH 40/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 113 tokens (22.1%)
    AlphaEarth: 262 tokens (51.2%)
    Encoder_0: 135 tokens (26.4%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.107421875, 'data': 0.134765625, 'dataset': 0.0234375, 'modality': 0.037109375, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1972

📉 LOSS BREAKDOWN:
  Total loss: 0.197181

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001529
    data_loss: 0.029723
    dataset_loss: 0.000048
    encoder_loss: 0.000451

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000141
    bioclip_modality_loss: 0.033595
    alphaearth_modality_loss: 0.104241
    phenovision_modality_loss: 0.193893

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=55
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=69
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=19
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0109, -0.0124, -0.0426, 0.0253]
    Predicted: [-0.0001, -0.0168, -0.0133, 0.0015]
    Δ (Delta): [0.0108, -0.0044, 0.0292, -0.0238]
    MAPE: 74.19%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [ 0.209,  0.117,  0.58 , -0.189,  0.168, -0.115, -0.364,  0.591]
    Predicted:
      [ 0.209,  0.061,  0.463, -0.136, -0.052, -0.026, -0.279,  0.399]
    Δ (Delta):
      [-1.221e-04, -5.673e-02, -1.172e-01,  5.298e-02, -2.194e-01,  8.914e-02,  8.545e-02, -1.926e-01]
    MAPE: 45.10%, RMSE: 0.1226

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.281851

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000685
    Max norm: 0.000818
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.004541
    Max norm: 0.138365
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00117481, max_change=0.00245847
  earth4d: mean_change=0.07562948, max_change=0.08747894
  multimodal: mean_change=0.00173249, max_change=0.05427771
  perceiver: mean_change=0.00279943, max_change=0.01863282

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.001529 (trend: decreasing)
  data_loss: 0.029723 (trend: increasing)
  dataset_loss: 0.000048 (trend: decreasing)
  modality_loss: 0.000141 (trend: decreasing)
  encoder_loss: 0.000451 (trend: increasing)
  phenovision_modality_loss: 0.193893 (trend: decreasing)
  bioclip_modality_loss: 0.033595 (trend: decreasing)
  alphaearth_modality_loss: 0.104241 (trend: decreasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000685, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.004541, zero_ratio=17.78%

================================================================================
BATCH 41/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 125 tokens (24.4%)
    Encoder_0: 129 tokens (25.2%)
    AlphaEarth: 258 tokens (50.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.08984375, 'data': 0.14453125, 'dataset': 0.03125, 'modality': 0.05078125, 'encoder': 0.041015625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2066

📉 LOSS BREAKDOWN:
  Total loss: 0.206636

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001631
    data_loss: 0.028260
    dataset_loss: 0.000811
    encoder_loss: 0.001065

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000383
    phenovision_modality_loss: 0.215468
    alphaearth_modality_loss: 0.105682
    bioclip_modality_loss: 0.031887

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=46
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=74
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=16
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=21

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0029, -0.0308, -0.0284, -0.0010]
    Predicted: [0.0205, -0.0144, -0.0374, 0.0223]
    Δ (Delta): [0.0234, 0.0164, -0.0090, 0.0232]
    MAPE: 819.14%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.231,  0.265,  0.269, -0.225, -0.104,  0.058,  0.089,  0.429]
    Predicted:
      [ 0.188,  0.038,  0.393, -0.131, -0.052,  0.013, -0.24 ,  0.362]
    Δ (Delta):
      [-0.043, -0.227,  0.124,  0.094,  0.052, -0.045, -0.33 , -0.067]
    MAPE: 88.09%, RMSE: 0.1564
    🌸 Flowering Probability:
      Original:  0.231
      Predicted: 0.188
      Δ (Delta): -0.043

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.367143

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000655
    Max norm: 0.000713
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.005404
    Max norm: 0.221730
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.56s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00124601, max_change=0.00250845
  earth4d: mean_change=0.07638561, max_change=0.08825221
  multimodal: mean_change=0.00172264, max_change=0.05128550
  perceiver: mean_change=0.00263841, max_change=0.01708441

================================================================================
BATCH 42/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 142 tokens (27.7%)
    Encoder_0: 106 tokens (20.7%)
    AlphaEarth: 264 tokens (51.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.103515625, 'data': 0.166015625, 'dataset': 0.017578125, 'modality': 0.052734375, 'encoder': 0.0625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2288

📉 LOSS BREAKDOWN:
  Total loss: 0.228765

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001678
    data_loss: 0.028173
    dataset_loss: 0.000654
    encoder_loss: 0.000354

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000526
    bioclip_modality_loss: 0.033687
    phenovision_modality_loss: 0.242069
    alphaearth_modality_loss: 0.121764

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=53
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=85
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=27
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=32

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 13):
    Original:  [-0.0086, -0.0164, -0.0376, 0.0196]
    Predicted: [0.0274, 0.0049, -0.0489, 0.0304]
    Δ (Delta): [0.0360, 0.0214, -0.0113, 0.0108]
    MAPE: 158.93%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 6):
    Original (first 8 dims):
      [ 0.274,  0.34 ,  0.398, -0.346, -0.137,  0.12 , -0.264,  0.492]
    Predicted:
      [ 0.194,  0.068,  0.46 , -0.134, -0.065,  0.002, -0.273,  0.355]
    Δ (Delta):
      [-0.08 , -0.272,  0.062,  0.212,  0.072, -0.118, -0.009, -0.137]
    MAPE: 46.06%, RMSE: 0.1446

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.395239

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000667
    Max norm: 0.000741
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.005589
    Max norm: 0.249242
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.43s
  Backward pass: 13.06s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00119065, max_change=0.00229750
  earth4d: mean_change=0.07667615, max_change=0.08850396
  multimodal: mean_change=0.00172131, max_change=0.04882425
  perceiver: mean_change=0.00249125, max_change=0.01615328

================================================================================
BATCH 43/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 129 tokens (25.2%)
    AlphaEarth: 264 tokens (51.6%)
    Encoder_0: 119 tokens (23.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.119140625, 'data': 0.13671875, 'dataset': 0.017578125, 'modality': 0.03515625, 'encoder': 0.07421875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2061

📉 LOSS BREAKDOWN:
  Total loss: 0.206123

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001531
    data_loss: 0.027080
    dataset_loss: 0.000246
    encoder_loss: 0.000794

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000055
    alphaearth_modality_loss: 0.104751
    bioclip_modality_loss: 0.035008
    phenovision_modality_loss: 0.215049

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=61
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=70
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=18
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=38

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0108, -0.0131, -0.0428, 0.0256]
    Predicted: [0.0112, -0.0011, -0.0366, 0.0333]
    Δ (Delta): [0.0220, 0.0120, 0.0062, 0.0077]
    MAPE: 85.05%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 10):
    Original (first 8 dims):
      [ 0.18 ,  0.278,  0.232, -0.351, -0.165,  0.019, -0.318,  0.316]
    Predicted:
      [ 0.191,  0.088,  0.444, -0.124, -0.06 , -0.005, -0.274,  0.34 ]
    Δ (Delta):
      [ 0.011, -0.19 ,  0.211,  0.226,  0.105, -0.023,  0.045,  0.024]
    MAPE: 54.98%, RMSE: 0.1352

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.282523

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000736
    Max norm: 0.000942
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.004234
    Max norm: 0.147453
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.41s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00110673, max_change=0.00204172
  earth4d: mean_change=0.07738460, max_change=0.08965810
  multimodal: mean_change=0.00174726, max_change=0.04946471
  perceiver: mean_change=0.00255422, max_change=0.01645680

================================================================================
BATCH 44/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 281 tokens (54.9%)
    Encoder_0: 121 tokens (23.6%)
    BioCLIP: 110 tokens (21.5%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.091796875, 'data': 0.13671875, 'dataset': 0.029296875, 'modality': 0.048828125, 'encoder': 0.0546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2098

📉 LOSS BREAKDOWN:
  Total loss: 0.209763

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001424
    data_loss: 0.028654
    dataset_loss: 0.000178
    encoder_loss: 0.001021

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000735
    phenovision_modality_loss: 0.235875
    bioclip_modality_loss: 0.031131
    alphaearth_modality_loss: 0.091977

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=47
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=70
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=15
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=25
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=28

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 15):
    Original:  [-0.0088, -0.0162, -0.0382, 0.0204]
    Predicted: [-0.0311, 0.0015, -0.0288, 0.0236]
    Δ (Delta): [-0.0223, 0.0177, 0.0094, 0.0031]
    MAPE: 101.08%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 9):
    Original (first 8 dims):
      [ 0.157, -0.218,  0.453, -0.217, -0.142, -0.337, -0.063,  0.028]
    Predicted:
      [ 0.197,  0.088,  0.452, -0.139, -0.052, -0.005, -0.288,  0.35 ]
    Δ (Delta):
      [ 0.039,  0.307, -0.001,  0.078,  0.09 ,  0.333, -0.225,  0.322]
    MAPE: 233.23%, RMSE: 0.2165
    🌸 Flowering Probability:
      Original:  0.157
      Predicted: 0.197
      Δ (Delta): 0.039

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.414312

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000676
    Max norm: 0.000770
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.005431
    Max norm: 0.240754
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00101723, max_change=0.00183563
  earth4d: mean_change=0.07746774, max_change=0.08967446
  multimodal: mean_change=0.00170234, max_change=0.04980291
  perceiver: mean_change=0.00230930, max_change=0.01578734

================================================================================
BATCH 45/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 133 tokens (26.0%)
    AlphaEarth: 261 tokens (51.0%)
    Encoder_0: 118 tokens (23.0%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.087890625, 'data': 0.13671875, 'dataset': 0.0234375, 'modality': 0.03515625, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2053

📉 LOSS BREAKDOWN:
  Total loss: 0.205330

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001456
    data_loss: 0.025139
    dataset_loss: 0.001144
    encoder_loss: 0.000110

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000655
    phenovision_modality_loss: 0.214888
    bioclip_modality_loss: 0.032222
    alphaearth_modality_loss: 0.109977

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=45
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=70
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=18
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 14):
    Original:  [-0.0106, -0.0135, -0.0428, 0.0256]
    Predicted: [-0.0554, 0.0074, -0.0201, 0.0182]
    Δ (Delta): [-0.0447, 0.0209, 0.0227, -0.0074]
    MAPE: 164.26%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 1):
    Original (first 8 dims):
      [ 0.413,  0.134,  0.484,  0.017,  0.289, -0.011, -0.209,  0.242]
    Predicted:
      [ 0.194,  0.07 ,  0.449, -0.147, -0.055, -0.013, -0.284,  0.331]
    Δ (Delta):
      [-0.219, -0.065, -0.035, -0.164, -0.344, -0.002, -0.075,  0.088]
    MAPE: 158.18%, RMSE: 0.1627
    🌸 Flowering Probability:
      Original:  0.413
      Predicted: 0.194
      Δ (Delta): -0.219

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.335803

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000644
    Max norm: 0.000728
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.005056
    Max norm: 0.198218
    Zero gradient ratio: 17.41%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.39s
  Backward pass: 12.99s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00105550, max_change=0.00204449
  earth4d: mean_change=0.07706328, max_change=0.08930685
  multimodal: mean_change=0.00163477, max_change=0.04859290
  perceiver: mean_change=0.00223220, max_change=0.01549896

================================================================================
BATCH 46/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 117 tokens (22.9%)
    BioCLIP: 123 tokens (24.0%)
    AlphaEarth: 271 tokens (52.9%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.0859375, 'data': 0.16015625, 'dataset': 0.0234375, 'modality': 0.03515625, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2017

📉 LOSS BREAKDOWN:
  Total loss: 0.201709

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001527
    data_loss: 0.029921
    dataset_loss: 0.000927
    encoder_loss: 0.000387

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000086
    bioclip_modality_loss: 0.030236
    alphaearth_modality_loss: 0.103675
    phenovision_modality_loss: 0.206331

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=44
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=82
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=18
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 9):
    Original:  [-0.0084, -0.0164, -0.0373, 0.0193]
    Predicted: [-0.0622, 0.0185, -0.0421, 0.0160]
    Δ (Delta): [-0.0537, 0.0348, -0.0048, -0.0034]
    MAPE: 219.91%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [-0.004, -0.052,  0.342, -0.24 , -0.136, -0.344, -0.043,  0.525]
    Predicted:
      [ 0.209,  0.065,  0.447, -0.136, -0.047, -0.012, -0.277,  0.33 ]
    Δ (Delta):
      [ 0.213,  0.118,  0.105,  0.103,  0.088,  0.332, -0.234, -0.196]
    MAPE: 766.94%, RMSE: 0.1911

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.283998

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000711
    Max norm: 0.000841
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.004582
    Max norm: 0.154193
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.56s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00102127, max_change=0.00187926
  earth4d: mean_change=0.07666787, max_change=0.08914588
  multimodal: mean_change=0.00157488, max_change=0.04644877
  perceiver: mean_change=0.00229589, max_change=0.01489164

================================================================================
BATCH 47/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 236 tokens (46.1%)
    Encoder_0: 143 tokens (27.9%)
    BioCLIP: 133 tokens (26.0%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.11328125, 'data': 0.140625, 'dataset': 0.0234375, 'modality': 0.04296875, 'encoder': 0.060546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1804

📉 LOSS BREAKDOWN:
  Total loss: 0.180359

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001406
    data_loss: 0.029174
    dataset_loss: 0.000169
    encoder_loss: 0.000471

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000594
    bioclip_modality_loss: 0.029492
    alphaearth_modality_loss: 0.114948
    phenovision_modality_loss: 0.154872

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=58
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=72
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=31

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0088, -0.0161, -0.0383, 0.0204]
    Predicted: [-0.0287, 0.0104, -0.0498, 0.0162]
    Δ (Delta): [-0.0199, 0.0265, -0.0115, -0.0043]
    MAPE: 110.73%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 13):
    Original (first 8 dims):
      [ 0.193, -0.058,  0.515, -0.12 , -0.043, -0.018, -0.25 ,  0.345]
    Predicted:
      [ 0.204,  0.06 ,  0.439, -0.123, -0.056, -0.014, -0.257,  0.327]
    Δ (Delta):
      [ 0.01 ,  0.119, -0.076, -0.003, -0.012,  0.004, -0.007, -0.018]
    MAPE: 35.84%, RMSE: 0.0506

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.300660

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000553
    Max norm: 0.000666
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.004715
    Max norm: 0.166054
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.39s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00100820, max_change=0.00182148
  earth4d: mean_change=0.07706422, max_change=0.08935419
  multimodal: mean_change=0.00161423, max_change=0.04740038
  perceiver: mean_change=0.00231843, max_change=0.01426116

================================================================================
BATCH 48/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 233 tokens (45.5%)
    BioCLIP: 129 tokens (25.2%)
    Encoder_0: 147 tokens (28.7%)
    Earth4D: 3 tokens (0.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1015625, 'data': 0.1484375, 'dataset': 0.021484375, 'modality': 0.048828125, 'encoder': 0.052734375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1968

📉 LOSS BREAKDOWN:
  Total loss: 0.196824

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001439
    data_loss: 0.022185
    dataset_loss: 0.000321
    encoder_loss: 0.000213

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000909
    phenovision_modality_loss: 0.209300
    alphaearth_modality_loss: 0.105264
    bioclip_modality_loss: 0.031547

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=52
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=76
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=25
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=27

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0090, -0.0152, -0.0381, 0.0204]
    Predicted: [0.0044, -0.0059, -0.0576, 0.0158]
    Δ (Delta): [0.0134, 0.0093, -0.0194, -0.0046]
    MAPE: 70.74%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.102,  0.111,  0.355, -0.061, -0.128, -0.049, -0.257,  0.38 ]
    Predicted:
      [ 0.201,  0.076,  0.429, -0.121, -0.053,  0.006, -0.222,  0.333]
    Δ (Delta):
      [ 0.099, -0.035,  0.074, -0.06 ,  0.075,  0.054,  0.035, -0.048]
    MAPE: 55.42%, RMSE: 0.0633
    🌸 Flowering Probability:
      Original:  0.102
      Predicted: 0.201
      Δ (Delta): 0.099

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.230444

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000694
    Max norm: 0.000841
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003483
    Max norm: 0.133524
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.39s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00098723, max_change=0.00178515
  earth4d: mean_change=0.07751463, max_change=0.08990350
  multimodal: mean_change=0.00162445, max_change=0.04781315
  perceiver: mean_change=0.00221668, max_change=0.01421904

================================================================================
BATCH 49/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 265 tokens (51.8%)
    BioCLIP: 119 tokens (23.2%)
    Encoder_0: 127 tokens (24.8%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.103515625, 'data': 0.15625, 'dataset': 0.015625, 'modality': 0.05078125, 'encoder': 0.044921875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1817

📉 LOSS BREAKDOWN:
  Total loss: 0.181652

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001435
    data_loss: 0.024570
    dataset_loss: 0.001733
    encoder_loss: 0.000392

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000321
    bioclip_modality_loss: 0.030595
    alphaearth_modality_loss: 0.112063
    phenovision_modality_loss: 0.168148

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=53
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=80
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=23

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 10):
    Original:  [-0.0130, -0.0079, -0.0454, 0.0283]
    Predicted: [0.0269, -0.0230, -0.0682, 0.0179]
    Δ (Delta): [0.0399, -0.0151, -0.0228, -0.0105]
    MAPE: 146.63%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 11):
    Original (first 8 dims):
      [-0.049, -0.257,  0.323,  0.183,  0.05 , -0.529, -0.31 ,  0.648]
    Predicted:
      [ 0.19 ,  0.087,  0.437, -0.128, -0.055,  0.017, -0.216,  0.328]
    Δ (Delta):
      [ 0.239,  0.344,  0.114, -0.311, -0.104,  0.546,  0.094, -0.321]
    MAPE: 152.02%, RMSE: 0.2970

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.223981

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000644
    Max norm: 0.000723
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003338
    Max norm: 0.134699
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.41s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00109142, max_change=0.00207170
  earth4d: mean_change=0.07817600, max_change=0.09095937
  multimodal: mean_change=0.00159934, max_change=0.04639017
  perceiver: mean_change=0.00201156, max_change=0.01401085

================================================================================
BATCH 50/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 256 tokens (50.0%)
    Encoder_0: 112 tokens (21.9%)
    BioCLIP: 144 tokens (28.1%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.10546875, 'data': 0.154296875, 'dataset': 0.017578125, 'modality': 0.037109375, 'encoder': 0.037109375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1752

📉 LOSS BREAKDOWN:
  Total loss: 0.175221

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001398
    data_loss: 0.026364
    dataset_loss: 0.000875
    encoder_loss: 0.000465

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000070
    bioclip_modality_loss: 0.030965
    alphaearth_modality_loss: 0.100511
    phenovision_modality_loss: 0.163162

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=54
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=79
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=19
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=19

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 0):
    Original:  [-0.0092, -0.0149, -0.0384, 0.0208]
    Predicted: [0.0219, -0.0351, -0.0486, 0.0379]
    Δ (Delta): [0.0311, -0.0201, -0.0102, 0.0171]
    MAPE: 145.49%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.205,  0.063,  0.195, -0.337,  0.04 , -0.099, -0.214,  0.129]
    Predicted:
      [ 0.17 ,  0.091,  0.427, -0.126, -0.057,  0.022, -0.206,  0.309]
    Δ (Delta):
      [-0.035,  0.028,  0.231,  0.211, -0.098,  0.121,  0.008,  0.181]
    MAPE: 93.89%, RMSE: 0.1401

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.222210

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000723
    Max norm: 0.000880
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003598
    Max norm: 0.120604
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.60s
  Backward pass: 13.03s
  Ratio (back/fwd): 5.0x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00109513, max_change=0.00190253
  earth4d: mean_change=0.07889077, max_change=0.09187239
  multimodal: mean_change=0.00155471, max_change=0.04449996
  perceiver: mean_change=0.00226226, max_change=0.01440888

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.001398 (trend: decreasing)
  data_loss: 0.026364 (trend: decreasing)
  dataset_loss: 0.000875 (trend: decreasing)
  modality_loss: 0.000070 (trend: decreasing)
  encoder_loss: 0.000465 (trend: increasing)
  phenovision_modality_loss: 0.163162 (trend: decreasing)
  bioclip_modality_loss: 0.030965 (trend: increasing)
  alphaearth_modality_loss: 0.100511 (trend: decreasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000723, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.003598, zero_ratio=17.78%

================================================================================
BATCH 51/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 120 tokens (23.4%)
    AlphaEarth: 247 tokens (48.2%)
    BioCLIP: 143 tokens (27.9%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.08984375, 'data': 0.158203125, 'dataset': 0.009765625, 'modality': 0.056640625, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1867

📉 LOSS BREAKDOWN:
  Total loss: 0.186689

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001360
    data_loss: 0.028666
    dataset_loss: 0.000200
    encoder_loss: 0.000132

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000342
    bioclip_modality_loss: 0.029080
    phenovision_modality_loss: 0.179105
    alphaearth_modality_loss: 0.105005

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=46
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=81
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=5
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=29
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0029, -0.0279, -0.0253, -0.0031]
    Predicted: [0.0046, -0.0276, -0.0250, 0.0320]
    Δ (Delta): [0.0076, 0.0003, 0.0003, 0.0351]
    MAPE: 352.50%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.026,  0.094,  0.299, -0.141,  0.278,  0.058, -0.409,  0.196]
    Predicted:
      [ 0.147,  0.066,  0.439, -0.141, -0.06 ,  0.012, -0.201,  0.294]
    Δ (Delta):
      [ 0.121, -0.027,  0.139, -0.   , -0.338, -0.046,  0.208,  0.098]
    MAPE: 104.80%, RMSE: 0.1597

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.240776

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000598
    Max norm: 0.000733
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003695
    Max norm: 0.170713
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.40s
  Backward pass: 13.03s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00108104, max_change=0.00195597
  earth4d: mean_change=0.07909911, max_change=0.09193414
  multimodal: mean_change=0.00160068, max_change=0.04580649
  perceiver: mean_change=0.00213162, max_change=0.01389925

================================================================================
BATCH 52/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 224 tokens (43.8%)
    Encoder_0: 148 tokens (28.9%)
    BioCLIP: 140 tokens (27.3%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.087890625, 'data': 0.15625, 'dataset': 0.029296875, 'modality': 0.0390625, 'encoder': 0.064453125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1921

📉 LOSS BREAKDOWN:
  Total loss: 0.192110

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001361
    data_loss: 0.023902
    dataset_loss: 0.000714
    encoder_loss: 0.000448

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000731
    bioclip_modality_loss: 0.028718
    alphaearth_modality_loss: 0.100935
    phenovision_modality_loss: 0.203661

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=45
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=80
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=15
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=20
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=33

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0134, -0.0030, -0.0420, 0.0267]
    Predicted: [-0.0196, -0.0086, -0.0105, 0.0215]
    Δ (Delta): [-0.0062, -0.0056, 0.0315, -0.0053]
    MAPE: 81.56%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.257,  0.101,  0.475, -0.142, -0.072, -0.06 , -0.21 ,  0.273]
    Predicted:
      [ 0.155,  0.049,  0.426, -0.138, -0.054,  0.004, -0.211,  0.3  ]
    Δ (Delta):
      [-0.102, -0.052, -0.049,  0.005,  0.019,  0.065, -0.001,  0.026]
    MAPE: 30.93%, RMSE: 0.0509

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.225211

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000754
    Max norm: 0.000971
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003589
    Max norm: 0.126707
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.40s
  Backward pass: 13.03s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00115887, max_change=0.00243630
  earth4d: mean_change=0.07903327, max_change=0.09182452
  multimodal: mean_change=0.00157390, max_change=0.04555406
  perceiver: mean_change=0.00208789, max_change=0.01340859

================================================================================
BATCH 53/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 148 tokens (28.9%)
    AlphaEarth: 249 tokens (48.6%)
    BioCLIP: 113 tokens (22.1%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.087890625, 'data': 0.14453125, 'dataset': 0.0234375, 'modality': 0.0546875, 'encoder': 0.0546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1865

📉 LOSS BREAKDOWN:
  Total loss: 0.186463

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001375
    data_loss: 0.026254
    dataset_loss: 0.001636
    encoder_loss: 0.000110

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000132
    bioclip_modality_loss: 0.028000
    alphaearth_modality_loss: 0.105016
    phenovision_modality_loss: 0.184277

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=45
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=74
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=28
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=28

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 3):
    Original:  [-0.0090, -0.0152, -0.0382, 0.0204]
    Predicted: [-0.0410, 0.0043, -0.0193, 0.0371]
    Δ (Delta): [-0.0320, 0.0195, 0.0189, 0.0167]
    MAPE: 153.63%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 8):
    Original (first 8 dims):
      [ 0.175,  0.167,  0.334, -0.016, -0.173,  0.082, -0.161,  0.293]
    Predicted:
      [ 0.161,  0.049,  0.406, -0.13 , -0.072, -0.007, -0.216,  0.265]
    Δ (Delta):
      [-0.014, -0.117,  0.072, -0.114,  0.101, -0.089, -0.056, -0.028]
    MAPE: 127.10%, RMSE: 0.0821

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.280481

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000716
    Max norm: 0.000841
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003977
    Max norm: 0.136218
    Zero gradient ratio: 17.41%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 12.97s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00105658, max_change=0.00220233
  earth4d: mean_change=0.07919618, max_change=0.09207845
  multimodal: mean_change=0.00151863, max_change=0.04541071
  perceiver: mean_change=0.00215164, max_change=0.01311801

================================================================================
BATCH 54/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 268 tokens (52.3%)
    BioCLIP: 122 tokens (23.8%)
    Encoder_0: 122 tokens (23.8%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.099609375, 'data': 0.14453125, 'dataset': 0.033203125, 'modality': 0.056640625, 'encoder': 0.037109375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1912

📉 LOSS BREAKDOWN:
  Total loss: 0.191174

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001360
    data_loss: 0.022080
    dataset_loss: 0.000726
    encoder_loss: 0.000084

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000126
    bioclip_modality_loss: 0.026695
    alphaearth_modality_loss: 0.112282
    phenovision_modality_loss: 0.196305

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=51
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=74
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=17
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=29
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=19

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 9):
    Original:  [-0.0106, -0.0141, -0.0433, 0.0264]
    Predicted: [-0.0319, 0.0037, -0.0323, 0.0196]
    Δ (Delta): [-0.0212, 0.0177, 0.0111, -0.0068]
    MAPE: 94.28%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.185, -0.091,  0.239,  0.236, -0.27 , -0.304, -0.121,  0.272]
    Predicted:
      [ 0.175,  0.057,  0.406, -0.122, -0.072, -0.015, -0.238,  0.284]
    Δ (Delta):
      [-0.01 ,  0.148,  0.167, -0.359,  0.198,  0.289, -0.117,  0.012]
    MAPE: 82.40%, RMSE: 0.1985

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.232848

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000588
    Max norm: 0.000718
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003372
    Max norm: 0.157567
    Zero gradient ratio: 18.52%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 12.99s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00096279, max_change=0.00191584
  earth4d: mean_change=0.07936451, max_change=0.09244748
  multimodal: mean_change=0.00144055, max_change=0.04368201
  perceiver: mean_change=0.00208619, max_change=0.01259708

================================================================================
BATCH 55/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 147 tokens (28.7%)
    AlphaEarth: 238 tokens (46.5%)
    Encoder_0: 127 tokens (24.8%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.12109375, 'data': 0.15234375, 'dataset': 0.01953125, 'modality': 0.0625, 'encoder': 0.04296875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1625

📉 LOSS BREAKDOWN:
  Total loss: 0.162469

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001237
    data_loss: 0.019917
    dataset_loss: 0.000062
    encoder_loss: 0.000391

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000164
    bioclip_modality_loss: 0.025914
    alphaearth_modality_loss: 0.101245
    phenovision_modality_loss: 0.155346

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=62
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=78
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=32
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=22

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 2, seq 7):
    Original:  [-0.0029, -0.0280, -0.0254, -0.0029]
    Predicted: [-0.0185, -0.0036, -0.0483, 0.0349]
    Δ (Delta): [-0.0156, 0.0244, -0.0229, 0.0378]
    MAPE: 508.17%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 1):
    Original (first 8 dims):
      [ 0.184,  0.099,  0.494, -0.17 , -0.111,  0.078, -0.115,  0.215]
    Predicted:
      [ 0.169,  0.063,  0.365, -0.11 , -0.08 ,  0.009, -0.225,  0.267]
    Δ (Delta):
      [-0.015, -0.037, -0.128,  0.06 ,  0.031, -0.069, -0.11 ,  0.052]
    MAPE: 42.82%, RMSE: 0.0726

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.265797

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000584
    Max norm: 0.000674
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.004071
    Max norm: 0.136956
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.54s
  Backward pass: 13.03s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00082542, max_change=0.00146622
  earth4d: mean_change=0.08064063, max_change=0.09415040
  multimodal: mean_change=0.00138894, max_change=0.04419030
  perceiver: mean_change=0.00197380, max_change=0.01228076

================================================================================
BATCH 56/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 244 tokens (47.7%)
    BioCLIP: 133 tokens (26.0%)
    Encoder_0: 133 tokens (26.0%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.103515625, 'data': 0.16796875, 'dataset': 0.01171875, 'modality': 0.0625, 'encoder': 0.052734375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1989

📉 LOSS BREAKDOWN:
  Total loss: 0.198947

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001300
    data_loss: 0.021915
    dataset_loss: 0.000600
    encoder_loss: 0.000137

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000121
    bioclip_modality_loss: 0.024897
    phenovision_modality_loss: 0.220444
    alphaearth_modality_loss: 0.105952

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=53
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=86
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=6
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=32
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=27

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0138, -0.0035, -0.0442, 0.0283]
    Predicted: [0.0029, -0.0439, -0.0324, 0.0282]
    Δ (Delta): [0.0167, -0.0404, 0.0118, -0.0001]
    MAPE: 321.34%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.15 ,  0.09 ,  0.44 , -0.287,  0.051, -0.017, -0.15 ,  0.391]
    Predicted:
      [ 0.175,  0.075,  0.39 , -0.124, -0.062,  0.029, -0.228,  0.299]
    Δ (Delta):
      [ 0.026, -0.015, -0.05 ,  0.163, -0.113,  0.045, -0.078, -0.092]
    MAPE: 83.88%, RMSE: 0.0861

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.246036

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000571
    Max norm: 0.000699
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003840
    Max norm: 0.127869
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.40s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00079898, max_change=0.00142043
  earth4d: mean_change=0.08151980, max_change=0.09509762
  multimodal: mean_change=0.00138375, max_change=0.04246207
  perceiver: mean_change=0.00177642, max_change=0.01178917

================================================================================
BATCH 57/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 283 tokens (55.3%)
    Encoder_0: 119 tokens (23.2%)
    BioCLIP: 109 tokens (21.3%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.109375, 'data': 0.17578125, 'dataset': 0.0234375, 'modality': 0.052734375, 'encoder': 0.064453125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1982

📉 LOSS BREAKDOWN:
  Total loss: 0.198199

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001260
    data_loss: 0.019817
    dataset_loss: 0.000942
    encoder_loss: 0.000055

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000060
    bioclip_modality_loss: 0.027537
    phenovision_modality_loss: 0.223644
    alphaearth_modality_loss: 0.102852

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=56
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=90
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=27
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=33

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0138, -0.0034, -0.0440, 0.0283]
    Predicted: [0.0090, -0.0465, -0.0290, 0.0209]
    Δ (Delta): [0.0228, -0.0431, 0.0150, -0.0074]
    MAPE: 372.77%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [ 0.294, -0.01 ,  0.566, -0.258, -0.052, -0.067, -0.28 ,  0.129]
    Predicted:
      [ 0.162,  0.076,  0.384, -0.122, -0.059,  0.056, -0.207,  0.281]
    Δ (Delta):
      [-0.132,  0.086, -0.182,  0.136, -0.007,  0.123,  0.073,  0.152]
    MAPE: 166.78%, RMSE: 0.1225

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.202370

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000532
    Max norm: 0.000675
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003083
    Max norm: 0.131103
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.42s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00078838, max_change=0.00142815
  earth4d: mean_change=0.08263533, max_change=0.09619554
  multimodal: mean_change=0.00142272, max_change=0.04062822
  perceiver: mean_change=0.00176410, max_change=0.01162802

================================================================================
BATCH 58/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 122 tokens (23.8%)
    AlphaEarth: 270 tokens (52.7%)
    BioCLIP: 119 tokens (23.2%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.095703125, 'data': 0.189453125, 'dataset': 0.013671875, 'modality': 0.044921875, 'encoder': 0.0703125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1526

📉 LOSS BREAKDOWN:
  Total loss: 0.152559

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001251
    data_loss: 0.020160
    dataset_loss: 0.000253
    encoder_loss: 0.000044

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000130
    bioclip_modality_loss: 0.024446
    alphaearth_modality_loss: 0.089597
    phenovision_modality_loss: 0.148169

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=49
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=97
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=7
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=23
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=36

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0091, -0.0153, -0.0387, 0.0213]
    Predicted: [0.0011, -0.0487, -0.0332, 0.0250]
    Δ (Delta): [0.0103, -0.0334, 0.0055, 0.0037]
    MAPE: 90.65%

  🎨 EARTH4D EMBEDDING RECONSTRUCTION (sample 0, seq 8):
    Original (first 8 dims):
      [0., 0., 0., 0., 0., 0., 0., 0.]
    Predicted:
      [ 0.16 ,  0.072,  0.421, -0.133, -0.051,  0.059, -0.207,  0.275]
    Δ (Delta):
      [ 0.16 ,  0.072,  0.421, -0.133, -0.051,  0.059, -0.207,  0.275]
    MAPE: 1724357632.00%, RMSE: 0.2094

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.209894

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000589
    Max norm: 0.000719
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003078
    Max norm: 0.112104
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.45s
  Backward pass: 13.07s
  Ratio (back/fwd): 5.3x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00072370, max_change=0.00122478
  earth4d: mean_change=0.08288701, max_change=0.09626413
  multimodal: mean_change=0.00144861, max_change=0.04214932
  perceiver: mean_change=0.00188008, max_change=0.01269679

================================================================================
BATCH 59/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 103 tokens (20.1%)
    AlphaEarth: 281 tokens (54.9%)
    BioCLIP: 127 tokens (24.8%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1171875, 'data': 0.154296875, 'dataset': 0.017578125, 'modality': 0.04296875, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1780

📉 LOSS BREAKDOWN:
  Total loss: 0.177969

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001222
    data_loss: 0.015738
    dataset_loss: 0.000022
    encoder_loss: 0.000290

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000072
    alphaearth_modality_loss: 0.102737
    bioclip_modality_loss: 0.023740
    phenovision_modality_loss: 0.195464

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=60
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=79
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0084, -0.0165, -0.0373, 0.0196]
    Predicted: [-0.0091, -0.0336, -0.0418, 0.0084]
    Δ (Delta): [-0.0006, -0.0171, -0.0046, -0.0112]
    MAPE: 45.09%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 1):
    Original (first 8 dims):
      [ 0.158,  0.131,  0.276, -0.186,  0.099, -0.093, -0.23 ,  0.261]
    Predicted:
      [ 0.151,  0.052,  0.406, -0.139, -0.051,  0.06 , -0.182,  0.249]
    Δ (Delta):
      [-0.007, -0.079,  0.129,  0.047, -0.15 ,  0.153,  0.048, -0.011]
    MAPE: 59.71%, RMSE: 0.0959

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.215450

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000510
    Max norm: 0.000575
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003039
    Max norm: 0.155847
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.42s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00070097, max_change=0.00122710
  earth4d: mean_change=0.08342504, max_change=0.09720260
  multimodal: mean_change=0.00142033, max_change=0.03997517
  perceiver: mean_change=0.00173495, max_change=0.01168086

================================================================================
BATCH 60/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 248 tokens (48.4%)
    Encoder_0: 142 tokens (27.7%)
    BioCLIP: 121 tokens (23.6%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09375, 'data': 0.154296875, 'dataset': 0.013671875, 'modality': 0.048828125, 'encoder': 0.041015625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1923

📉 LOSS BREAKDOWN:
  Total loss: 0.192296

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001236
    data_loss: 0.017562
    dataset_loss: 0.000340
    encoder_loss: 0.000261

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000031
    bioclip_modality_loss: 0.024879
    alphaearth_modality_loss: 0.107401
    phenovision_modality_loss: 0.214587

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=48
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=79
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=7
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=25
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=21

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 12):
    Original:  [-0.0139, -0.0031, -0.0439, 0.0283]
    Predicted: [-0.0180, -0.0300, -0.0403, 0.0071]
    Δ (Delta): [-0.0042, -0.0268, 0.0035, -0.0212]
    MAPE: 242.34%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.189, -0.09 ,  0.503,  0.025, -0.062,  0.211, -0.112,  0.266]
    Predicted:
      [ 0.156,  0.067,  0.427, -0.136, -0.052,  0.037, -0.178,  0.246]
    Δ (Delta):
      [-0.033,  0.157, -0.076, -0.16 ,  0.01 , -0.174, -0.066, -0.02 ]
    MAPE: 127.99%, RMSE: 0.1074

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.306886

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000516
    Max norm: 0.000611
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003885
    Max norm: 0.211478
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.54s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00067318, max_change=0.00122327
  earth4d: mean_change=0.08342700, max_change=0.09698379
  multimodal: mean_change=0.00142335, max_change=0.03990155
  perceiver: mean_change=0.00169227, max_change=0.01090669

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.001236 (trend: decreasing)
  data_loss: 0.017562 (trend: decreasing)
  dataset_loss: 0.000340 (trend: decreasing)
  modality_loss: 0.000031 (trend: decreasing)
  encoder_loss: 0.000261 (trend: increasing)
  phenovision_modality_loss: 0.214587 (trend: decreasing)
  bioclip_modality_loss: 0.024879 (trend: decreasing)
  alphaearth_modality_loss: 0.107401 (trend: increasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000516, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.003885, zero_ratio=17.78%

================================================================================
BATCH 61/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 138 tokens (27.0%)
    Encoder_0: 124 tokens (24.2%)
    AlphaEarth: 249 tokens (48.6%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.091796875, 'data': 0.19140625, 'dataset': 0.017578125, 'modality': 0.056640625, 'encoder': 0.03125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1697

📉 LOSS BREAKDOWN:
  Total loss: 0.169701

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001224
    data_loss: 0.018551
    dataset_loss: 0.000365
    encoder_loss: 0.000075

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000089
    alphaearth_modality_loss: 0.095398
    phenovision_modality_loss: 0.180119
    bioclip_modality_loss: 0.024228

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=47
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=98
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=29
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=16

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 6):
    Original:  [-0.0095, -0.0141, -0.0388, 0.0216]
    Predicted: [-0.0163, -0.0256, -0.0453, -0.0037]
    Δ (Delta): [-0.0068, -0.0115, -0.0066, -0.0253]
    MAPE: 71.86%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.138,  0.101,  0.472, -0.081, -0.134, -0.003, -0.272,  0.316]
    Predicted:
      [ 0.154,  0.07 ,  0.431, -0.139, -0.052,  0.021, -0.167,  0.247]
    Δ (Delta):
      [ 0.015, -0.031, -0.041, -0.058,  0.082,  0.024,  0.105, -0.069]
    MAPE: 119.99%, RMSE: 0.0605

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.172937

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000564
    Max norm: 0.000654
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002563
    Max norm: 0.116716
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.43s
  Backward pass: 13.06s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00082054, max_change=0.00156618
  earth4d: mean_change=0.08303181, max_change=0.09640584
  multimodal: mean_change=0.00142772, max_change=0.03799576
  perceiver: mean_change=0.00167342, max_change=0.01051297

================================================================================
BATCH 62/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 118 tokens (23.0%)
    Encoder_0: 133 tokens (26.0%)
    AlphaEarth: 261 tokens (51.0%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.0859375, 'data': 0.158203125, 'dataset': 0.013671875, 'modality': 0.05078125, 'encoder': 0.041015625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1706

📉 LOSS BREAKDOWN:
  Total loss: 0.170576

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001241
    data_loss: 0.017242
    dataset_loss: 0.000163
    encoder_loss: 0.000069

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000313
    alphaearth_modality_loss: 0.106462
    phenovision_modality_loss: 0.175983
    bioclip_modality_loss: 0.021631

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=44
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=81
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=7
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=21

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 1):
    Original:  [-0.0096, -0.0142, -0.0391, 0.0220]
    Predicted: [-0.0138, -0.0126, -0.0507, -0.0028]
    Δ (Delta): [-0.0042, 0.0016, -0.0116, -0.0248]
    MAPE: 49.34%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [ 0.138, -0.073,  0.388, -0.086, -0.187,  0.115, -0.101,  0.249]
    Predicted:
      [ 0.139,  0.065,  0.387, -0.141, -0.06 ,  0.023, -0.137,  0.234]
    Δ (Delta):
      [ 0.001,  0.139, -0.002, -0.056,  0.128, -0.093, -0.037, -0.015]
    MAPE: 55.75%, RMSE: 0.0781

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.187276

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000615
    Max norm: 0.000778
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002741
    Max norm: 0.113181
    Zero gradient ratio: 18.15%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.39s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00077957, max_change=0.00142129
  earth4d: mean_change=0.08329603, max_change=0.09650800
  multimodal: mean_change=0.00146195, max_change=0.03661823
  perceiver: mean_change=0.00171046, max_change=0.01136760

================================================================================
BATCH 63/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 246 tokens (48.0%)
    Encoder_0: 131 tokens (25.6%)
    BioCLIP: 134 tokens (26.2%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1171875, 'data': 0.146484375, 'dataset': 0.025390625, 'modality': 0.041015625, 'encoder': 0.05859375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1837

📉 LOSS BREAKDOWN:
  Total loss: 0.183668

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001188
    data_loss: 0.018515
    dataset_loss: 0.000055
    encoder_loss: 0.000491

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000049
    phenovision_modality_loss: 0.210967
    bioclip_modality_loss: 0.022016
    alphaearth_modality_loss: 0.094828

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=60
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=75
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=13
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=21
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=30

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0105, -0.0136, -0.0418, 0.0248]
    Predicted: [-0.0107, -0.0053, -0.0405, 0.0192]
    Δ (Delta): [-0.0002, 0.0084, 0.0013, -0.0056]
    MAPE: 22.38%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.208,  0.264,  0.448, -0.113, -0.238,  0.153, -0.302,  0.083]
    Predicted:
      [ 0.149,  0.079,  0.393, -0.122, -0.069,  0.025, -0.156,  0.241]
    Δ (Delta):
      [-0.06 , -0.184, -0.055, -0.009,  0.169, -0.128,  0.146,  0.158]
    MAPE: 64.18%, RMSE: 0.1285
    🌸 Flowering Probability:
      Original:  0.208
      Predicted: 0.149
      Δ (Delta): -0.060

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.234051

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000553
    Max norm: 0.000738
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003423
    Max norm: 0.142487
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00085219, max_change=0.00193525
  earth4d: mean_change=0.08391890, max_change=0.09735890
  multimodal: mean_change=0.00150036, max_change=0.03679938
  perceiver: mean_change=0.00173316, max_change=0.01157671

================================================================================
BATCH 64/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 251 tokens (49.0%)
    Encoder_0: 131 tokens (25.6%)
    BioCLIP: 130 tokens (25.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09765625, 'data': 0.16015625, 'dataset': 0.01953125, 'modality': 0.044921875, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1851

📉 LOSS BREAKDOWN:
  Total loss: 0.185101

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001249
    data_loss: 0.018685
    dataset_loss: 0.000434
    encoder_loss: 0.000035

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000501
    bioclip_modality_loss: 0.022764
    alphaearth_modality_loss: 0.098702
    phenovision_modality_loss: 0.208673

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=50
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=82
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=23
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 6):
    Original:  [-0.0092, -0.0152, -0.0388, 0.0213]
    Predicted: [0.0026, -0.0051, -0.0350, 0.0208]
    Δ (Delta): [0.0119, 0.0101, 0.0038, -0.0006]
    MAPE: 51.83%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 1):
    Original (first 8 dims):
      [ 0.263,  0.127,  0.3  , -0.251, -0.088,  0.017, -0.167,  0.185]
    Predicted:
      [ 0.161,  0.063,  0.398, -0.124, -0.06 ,  0.04 , -0.173,  0.267]
    Δ (Delta):
      [-0.102, -0.063,  0.099,  0.127,  0.028,  0.023, -0.006,  0.083]
    MAPE: 48.50%, RMSE: 0.0778

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.313779

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000684
    Max norm: 0.000773
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003772
    Max norm: 0.204520
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.55s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00111752, max_change=0.00239834
  earth4d: mean_change=0.08482868, max_change=0.09856931
  multimodal: mean_change=0.00152005, max_change=0.03544290
  perceiver: mean_change=0.00194872, max_change=0.01246027

================================================================================
BATCH 65/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 135 tokens (26.4%)
    AlphaEarth: 248 tokens (48.4%)
    Encoder_0: 128 tokens (25.0%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.10546875, 'data': 0.146484375, 'dataset': 0.025390625, 'modality': 0.0390625, 'encoder': 0.04296875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1657

📉 LOSS BREAKDOWN:
  Total loss: 0.165683

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001200
    data_loss: 0.017609
    dataset_loss: 0.000389
    encoder_loss: 0.000256

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000034
    bioclip_modality_loss: 0.020605
    alphaearth_modality_loss: 0.100445
    phenovision_modality_loss: 0.172561

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=54
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=75
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=13
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=20
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=22

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 9):
    Original:  [-0.0088, -0.0161, -0.0379, 0.0203]
    Predicted: [-0.0030, 0.0068, -0.0261, 0.0334]
    Δ (Delta): [0.0057, 0.0229, 0.0119, 0.0132]
    MAPE: 75.87%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 12):
    Original (first 8 dims):
      [ 0.281,  0.037,  0.511, -0.175,  0.008,  0.214, -0.052,  0.292]
    Predicted:
      [ 0.151,  0.043,  0.374, -0.13 , -0.069,  0.047, -0.15 ,  0.222]
    Δ (Delta):
      [-0.13 ,  0.007, -0.137,  0.045, -0.077, -0.167, -0.098, -0.071]
    MAPE: 174.88%, RMSE: 0.1038

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.155175

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000560
    Max norm: 0.000672
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002208
    Max norm: 0.103905
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00111730, max_change=0.00262403
  earth4d: mean_change=0.08477724, max_change=0.09872583
  multimodal: mean_change=0.00148079, max_change=0.03439358
  perceiver: mean_change=0.00191342, max_change=0.01261787

================================================================================
BATCH 66/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 122 tokens (23.8%)
    AlphaEarth: 242 tokens (47.3%)
    BioCLIP: 148 tokens (28.9%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.083984375, 'data': 0.142578125, 'dataset': 0.015625, 'modality': 0.0390625, 'encoder': 0.05859375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1719

📉 LOSS BREAKDOWN:
  Total loss: 0.171916

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001195
    data_loss: 0.016658
    dataset_loss: 0.000039
    encoder_loss: 0.000164

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000361
    phenovision_modality_loss: 0.188859
    bioclip_modality_loss: 0.021277
    alphaearth_modality_loss: 0.097877

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=43
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=73
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=20
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=30

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 1):
    Original:  [-0.0090, -0.0157, -0.0384, 0.0208]
    Predicted: [-0.0130, 0.0113, -0.0228, 0.0413]
    Δ (Delta): [-0.0039, 0.0270, 0.0156, 0.0205]
    MAPE: 88.69%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 1, seq 0):
    Original (first 8 dims):
      [ 0.273, -0.082,  0.58 , -0.067,  0.072,  0.202, -0.463,  0.354]
    Predicted:
      [ 0.151,  0.046,  0.352, -0.135, -0.078,  0.048, -0.132,  0.184]
    Δ (Delta):
      [-0.122,  0.128, -0.228, -0.067, -0.15 , -0.154,  0.331, -0.171]
    MAPE: 93.12%, RMSE: 0.1847
    🌸 Flowering Probability:
      Original:  0.273
      Predicted: 0.151
      Δ (Delta): -0.122

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.206604

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000574
    Max norm: 0.000672
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003085
    Max norm: 0.131174
    Zero gradient ratio: 18.89%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00114264, max_change=0.00245681
  earth4d: mean_change=0.08459686, max_change=0.09868930
  multimodal: mean_change=0.00150971, max_change=0.03781216
  perceiver: mean_change=0.00179040, max_change=0.01153453

================================================================================
BATCH 67/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 136 tokens (26.6%)
    AlphaEarth: 239 tokens (46.7%)
    Encoder_0: 136 tokens (26.6%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.11328125, 'data': 0.158203125, 'dataset': 0.0234375, 'modality': 0.052734375, 'encoder': 0.0703125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1760

📉 LOSS BREAKDOWN:
  Total loss: 0.175981

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001134
    data_loss: 0.016539
    dataset_loss: 0.000083
    encoder_loss: 0.000030

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000131
    phenovision_modality_loss: 0.203793
    alphaearth_modality_loss: 0.089968
    bioclip_modality_loss: 0.022806

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=58
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=81
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=27
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=36

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 9):
    Original:  [-0.0088, -0.0161, -0.0381, 0.0204]
    Predicted: [-0.0201, -0.0024, -0.0306, 0.0318]
    Δ (Delta): [-0.0112, 0.0137, 0.0074, 0.0113]
    MAPE: 71.82%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [-0.057,  0.203,  0.18 , -0.409, -0.167,  0.103, -0.178,  0.257]
    Predicted:
      [ 0.157,  0.054,  0.39 , -0.14 , -0.06 ,  0.057, -0.153,  0.205]
    Δ (Delta):
      [ 0.214, -0.149,  0.21 ,  0.268,  0.106, -0.047,  0.025, -0.051]
    MAPE: 96.69%, RMSE: 0.1585
    🌸 Flowering Probability:
      Original:  -0.057
      Predicted: 0.157
      Δ (Delta): 0.214

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.166650

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000527
    Max norm: 0.000664
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002511
    Max norm: 0.109897
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00105651, max_change=0.00226259
  earth4d: mean_change=0.08532117, max_change=0.09986516
  multimodal: mean_change=0.00145878, max_change=0.03608613
  perceiver: mean_change=0.00172362, max_change=0.01096332

================================================================================
BATCH 68/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 130 tokens (25.4%)
    BioCLIP: 121 tokens (23.6%)
    AlphaEarth: 260 tokens (50.8%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1171875, 'data': 0.138671875, 'dataset': 0.029296875, 'modality': 0.064453125, 'encoder': 0.044921875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1493

📉 LOSS BREAKDOWN:
  Total loss: 0.149327

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001171
    data_loss: 0.018254
    dataset_loss: 0.000298
    encoder_loss: 0.000621

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000189
    bioclip_modality_loss: 0.019021
    alphaearth_modality_loss: 0.096218
    phenovision_modality_loss: 0.144342

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=60
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=71
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=15
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=33
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=23

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 11):
    Original:  [0.0005, -0.0064, 0.0110, -0.0067]
    Predicted: [-0.0172, -0.0244, -0.0216, 0.0320]
    Δ (Delta): [-0.0178, -0.0181, -0.0327, 0.0387]
    MAPE: 1120.88%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 1, seq 13):
    Original (first 8 dims):
      [ 0.228,  0.04 ,  0.317, -0.16 ,  0.007,  0.087, -0.167,  0.131]
    Predicted:
      [ 0.13 ,  0.059,  0.342, -0.13 , -0.055,  0.059, -0.138,  0.171]
    Δ (Delta):
      [-0.098,  0.019,  0.025,  0.03 , -0.062, -0.027,  0.029,  0.04 ]
    MAPE: 138.38%, RMSE: 0.0481

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.293617

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000578
    Max norm: 0.000698
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003601
    Max norm: 0.191961
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.51s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00095446, max_change=0.00196558
  earth4d: mean_change=0.08665793, max_change=0.10141813
  multimodal: mean_change=0.00139840, max_change=0.03753120
  perceiver: mean_change=0.00173177, max_change=0.01113452

================================================================================
BATCH 69/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 125 tokens (24.4%)
    AlphaEarth: 261 tokens (51.0%)
    BioCLIP: 125 tokens (24.4%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.103515625, 'data': 0.158203125, 'dataset': 0.017578125, 'modality': 0.05078125, 'encoder': 0.0546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2050

📉 LOSS BREAKDOWN:
  Total loss: 0.204992

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001185
    data_loss: 0.014706
    dataset_loss: 0.000074
    encoder_loss: 0.000194

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000033
    bioclip_modality_loss: 0.020479
    phenovision_modality_loss: 0.257547
    alphaearth_modality_loss: 0.100116

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=53
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=81
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=28

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 6):
    Original:  [-0.0110, -0.0101, -0.0399, 0.0234]
    Predicted: [-0.0233, -0.0334, -0.0241, 0.0086]
    Δ (Delta): [-0.0123, -0.0233, 0.0158, -0.0148]
    MAPE: 111.32%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 7):
    Original (first 8 dims):
      [ 0.131,  0.011,  0.337, -0.218, -0.164,  0.054, -0.114,  0.15 ]
    Predicted:
      [ 0.151,  0.08 ,  0.38 , -0.139, -0.049,  0.052, -0.146,  0.207]
    Δ (Delta):
      [ 0.02 ,  0.069,  0.043,  0.079,  0.115, -0.002, -0.031,  0.058]
    MAPE: 100.38%, RMSE: 0.0620

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.403064

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000494
    Max norm: 0.000564
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.004214
    Max norm: 0.297801
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00095871, max_change=0.00218496
  earth4d: mean_change=0.08770038, max_change=0.10262220
  multimodal: mean_change=0.00137916, max_change=0.03617920
  perceiver: mean_change=0.00190944, max_change=0.01136981

================================================================================
BATCH 70/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 137 tokens (26.8%)
    AlphaEarth: 239 tokens (46.7%)
    Encoder_0: 136 tokens (26.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.103515625, 'data': 0.15625, 'dataset': 0.0234375, 'modality': 0.0625, 'encoder': 0.060546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1713

📉 LOSS BREAKDOWN:
  Total loss: 0.171327

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001150
    data_loss: 0.016951
    dataset_loss: 0.000082
    encoder_loss: 0.000102

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000212
    phenovision_modality_loss: 0.183201
    bioclip_modality_loss: 0.020024
    alphaearth_modality_loss: 0.103148

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=53
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=80
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=32
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=31

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 12):
    Original:  [-0.0085, -0.0163, -0.0369, 0.0193]
    Predicted: [-0.0076, -0.0236, -0.0317, 0.0031]
    Δ (Delta): [0.0009, -0.0073, 0.0052, -0.0162]
    MAPE: 38.35%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.432, -0.165,  0.251, -0.298, -0.123, -0.112, -0.191,  0.064]
    Predicted:
      [ 0.137,  0.076,  0.331, -0.132, -0.069,  0.052, -0.123,  0.186]
    Δ (Delta):
      [-0.295,  0.241,  0.08 ,  0.166,  0.055,  0.164,  0.068,  0.122]
    MAPE: 89.81%, RMSE: 0.1690
    🌸 Flowering Probability:
      Original:  0.432
      Predicted: 0.137
      Δ (Delta): -0.295

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.190789

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000519
    Max norm: 0.000617
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002698
    Max norm: 0.138037
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00091459, max_change=0.00216669
  earth4d: mean_change=0.08758393, max_change=0.10245369
  multimodal: mean_change=0.00138887, max_change=0.03757909
  perceiver: mean_change=0.00177308, max_change=0.01061687

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.001150 (trend: decreasing)
  data_loss: 0.016951 (trend: increasing)
  dataset_loss: 0.000082 (trend: increasing)
  modality_loss: 0.000212 (trend: decreasing)
  encoder_loss: 0.000102 (trend: decreasing)
  phenovision_modality_loss: 0.183201 (trend: decreasing)
  bioclip_modality_loss: 0.020024 (trend: decreasing)
  alphaearth_modality_loss: 0.103148 (trend: increasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000519, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.002698, zero_ratio=19.26%

================================================================================
BATCH 71/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 259 tokens (50.6%)
    BioCLIP: 113 tokens (22.1%)
    Encoder_0: 137 tokens (26.8%)
    Earth4D: 3 tokens (0.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.123046875, 'data': 0.150390625, 'dataset': 0.021484375, 'modality': 0.048828125, 'encoder': 0.0390625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1665

📉 LOSS BREAKDOWN:
  Total loss: 0.166481

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001126
    data_loss: 0.013514
    dataset_loss: 0.000125
    encoder_loss: 0.000984

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000134
    bioclip_modality_loss: 0.019547
    alphaearth_modality_loss: 0.092395
    phenovision_modality_loss: 0.191491

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=63
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=77
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=25
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=20

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0089, -0.0160, -0.0379, 0.0205]
    Predicted: [-0.0017, -0.0233, -0.0334, -0.0024]
    Δ (Delta): [0.0072, -0.0073, 0.0045, -0.0228]
    MAPE: 62.36%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 1, seq 2):
    Original (first 8 dims):
      [ 0.278,  0.006,  0.517, -0.075, -0.097,  0.211, -0.064,  0.251]
    Predicted:
      [ 0.136,  0.065,  0.332, -0.143, -0.068,  0.055, -0.107,  0.181]
    Δ (Delta):
      [-0.142,  0.059, -0.185, -0.068,  0.029, -0.156, -0.043, -0.07 ]
    MAPE: 173.10%, RMSE: 0.1086

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.190968

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000555
    Max norm: 0.000730
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002689
    Max norm: 0.126201
    Zero gradient ratio: 18.52%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00095977, max_change=0.00247166
  earth4d: mean_change=0.08880417, max_change=0.10401422
  multimodal: mean_change=0.00133442, max_change=0.03593694
  perceiver: mean_change=0.00182550, max_change=0.01105912

================================================================================
BATCH 72/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 257 tokens (50.2%)
    BioCLIP: 138 tokens (27.0%)
    Encoder_0: 116 tokens (22.7%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.111328125, 'data': 0.15234375, 'dataset': 0.01953125, 'modality': 0.064453125, 'encoder': 0.03515625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1693

📉 LOSS BREAKDOWN:
  Total loss: 0.169289

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001139
    data_loss: 0.014131
    dataset_loss: 0.000005
    encoder_loss: 0.000069

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000096
    phenovision_modality_loss: 0.191596
    bioclip_modality_loss: 0.020645
    alphaearth_modality_loss: 0.095761

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=57
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=78
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=33
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=18

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0130, -0.0080, -0.0450, 0.0281]
    Predicted: [-0.0038, -0.0091, -0.0476, 0.0064]
    Δ (Delta): [0.0092, -0.0010, -0.0026, -0.0218]
    MAPE: 41.82%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 1):
    Original (first 8 dims):
      [ 0.151,  0.078,  0.348, -0.18 , -0.051,  0.023, -0.031,  0.114]
    Predicted:
      [ 0.145,  0.076,  0.356, -0.136, -0.07 ,  0.065, -0.118,  0.185]
    Δ (Delta):
      [-0.006, -0.002,  0.008,  0.044, -0.019,  0.043, -0.086,  0.072]
    MAPE: 74.56%, RMSE: 0.0459
    🌸 Flowering Probability:
      Original:  0.151
      Predicted: 0.145
      Δ (Delta): -0.006

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.223022

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000560
    Max norm: 0.000714
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002932
    Max norm: 0.136065
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00088311, max_change=0.00224569
  earth4d: mean_change=0.08893512, max_change=0.10387074
  multimodal: mean_change=0.00131456, max_change=0.03604871
  perceiver: mean_change=0.00165922, max_change=0.01079189

================================================================================
BATCH 73/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 253 tokens (49.4%)
    BioCLIP: 120 tokens (23.4%)
    Encoder_0: 138 tokens (27.0%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.115234375, 'data': 0.16015625, 'dataset': 0.0234375, 'modality': 0.04296875, 'encoder': 0.052734375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1883

📉 LOSS BREAKDOWN:
  Total loss: 0.188274

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001175
    data_loss: 0.012953
    dataset_loss: 0.000163
    encoder_loss: 0.000072

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000085
    alphaearth_modality_loss: 0.097820
    phenovision_modality_loss: 0.230859
    bioclip_modality_loss: 0.019551

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=59
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=82
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=27

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 1):
    Original:  [-0.0089, -0.0161, -0.0378, 0.0203]
    Predicted: [-0.0114, -0.0043, -0.0505, 0.0106]
    Δ (Delta): [-0.0025, 0.0118, -0.0127, -0.0098]
    MAPE: 45.68%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [-0.081, -0.046,  0.038,  0.069, -0.067,  0.09 , -0.143, -0.13 ]
    Predicted:
      [ 0.146,  0.066,  0.363, -0.144, -0.062,  0.069, -0.113,  0.183]
    Δ (Delta):
      [ 0.227,  0.113,  0.325, -0.213,  0.005, -0.02 ,  0.03 ,  0.313]
    MAPE: 246.44%, RMSE: 0.1982

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.200227

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000543
    Max norm: 0.000706
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002583
    Max norm: 0.136915
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.55s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00084493, max_change=0.00220080
  earth4d: mean_change=0.09036376, max_change=0.10552009
  multimodal: mean_change=0.00132302, max_change=0.03567881
  perceiver: mean_change=0.00163179, max_change=0.01062102

================================================================================
BATCH 74/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 236 tokens (46.1%)
    Encoder_0: 137 tokens (26.8%)
    BioCLIP: 138 tokens (27.0%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1015625, 'data': 0.111328125, 'dataset': 0.013671875, 'modality': 0.052734375, 'encoder': 0.033203125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1753

📉 LOSS BREAKDOWN:
  Total loss: 0.175335

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001169
    data_loss: 0.016871
    dataset_loss: 0.000258
    encoder_loss: 0.000327

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000198
    phenovision_modality_loss: 0.189882
    bioclip_modality_loss: 0.018287
    alphaearth_modality_loss: 0.106265

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=52
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=57
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=7
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=27
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=17

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0130, -0.0098, -0.0465, 0.0294]
    Predicted: [-0.0293, 0.0033, -0.0381, 0.0238]
    Δ (Delta): [-0.0163, 0.0131, 0.0084, -0.0056]
    MAPE: 73.89%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 1, seq 3):
    Original (first 8 dims):
      [ 0.015,  0.071,  0.415, -0.087, -0.023,  0.029, -0.111,  0.351]
    Predicted:
      [ 0.13 ,  0.044,  0.315, -0.158, -0.049,  0.077, -0.07 ,  0.149]
    Δ (Delta):
      [ 0.115, -0.027, -0.1  , -0.072, -0.025,  0.048,  0.042, -0.202]
    MAPE: 157.91%, RMSE: 0.0966
    🌸 Flowering Probability:
      Original:  0.015
      Predicted: 0.130
      Δ (Delta): 0.115

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.196159

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000468
    Max norm: 0.000573
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002905
    Max norm: 0.131410
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.32s
  Backward pass: 12.92s
  Ratio (back/fwd): 5.6x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00076992, max_change=0.00200684
  earth4d: mean_change=0.09101684, max_change=0.10605957
  multimodal: mean_change=0.00141746, max_change=0.03699461
  perceiver: mean_change=0.00188666, max_change=0.01245993

================================================================================
BATCH 75/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 131 tokens (25.6%)
    AlphaEarth: 247 tokens (48.2%)
    Encoder_0: 133 tokens (26.0%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09765625, 'data': 0.150390625, 'dataset': 0.013671875, 'modality': 0.046875, 'encoder': 0.0703125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1523

📉 LOSS BREAKDOWN:
  Total loss: 0.152277

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001129
    data_loss: 0.012035
    dataset_loss: 0.000030
    encoder_loss: 0.000168

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000099
    bioclip_modality_loss: 0.017608
    alphaearth_modality_loss: 0.095790
    phenovision_modality_loss: 0.164769

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=50
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=77
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=7
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=24
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=36

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0089, -0.0161, -0.0378, 0.0203]
    Predicted: [-0.0464, -0.0060, -0.0294, 0.0246]
    Δ (Delta): [-0.0375, 0.0101, 0.0085, 0.0043]
    MAPE: 131.37%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 7):
    Original (first 8 dims):
      [ 0.053,  0.112,  0.34 , -0.302, -0.112,  0.183, -0.065,  0.093]
    Predicted:
      [ 0.15 ,  0.055,  0.328, -0.158, -0.042,  0.065, -0.065,  0.168]
    Δ (Delta):
      [ 0.096, -0.057, -0.012,  0.144,  0.071, -0.118,  0.   ,  0.075]
    MAPE: 61.45%, RMSE: 0.0852

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.166882

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000525
    Max norm: 0.000626
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002341
    Max norm: 0.107837
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00077596, max_change=0.00198226
  earth4d: mean_change=0.09080666, max_change=0.10591585
  multimodal: mean_change=0.00139398, max_change=0.03564924
  perceiver: mean_change=0.00165697, max_change=0.01026279

================================================================================
BATCH 76/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 146 tokens (28.5%)
    Encoder_0: 125 tokens (24.4%)
    AlphaEarth: 240 tokens (46.9%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.087890625, 'data': 0.15234375, 'dataset': 0.005859375, 'modality': 0.048828125, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1694

📉 LOSS BREAKDOWN:
  Total loss: 0.169439

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001055
    data_loss: 0.013048
    dataset_loss: 0.000108
    encoder_loss: 0.000026

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000018
    phenovision_modality_loss: 0.206977
    alphaearth_modality_loss: 0.086938
    bioclip_modality_loss: 0.016727

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=45
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=78
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=3
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=25
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 4):
    Original:  [-0.0087, -0.0163, -0.0369, 0.0193]
    Predicted: [-0.0400, -0.0136, -0.0219, 0.0152]
    Δ (Delta): [-0.0313, 0.0026, 0.0151, -0.0041]
    MAPE: 109.38%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.288,  0.044,  0.406, -0.158,  0.088,  0.142, -0.107,  0.237]
    Predicted:
      [ 0.162,  0.055,  0.33 , -0.162, -0.037,  0.06 , -0.064,  0.172]
    Δ (Delta):
      [-0.126,  0.011, -0.076, -0.004, -0.125, -0.083,  0.043, -0.066]
    MAPE: 44.71%, RMSE: 0.0793
    🌸 Flowering Probability:
      Original:  0.288
      Predicted: 0.162
      Δ (Delta): -0.126

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.187083

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000565
    Max norm: 0.000632
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002514
    Max norm: 0.131668
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00085939, max_change=0.00252407
  earth4d: mean_change=0.09031757, max_change=0.10543714
  multimodal: mean_change=0.00138796, max_change=0.03476395
  perceiver: mean_change=0.00167521, max_change=0.01184183

================================================================================
BATCH 77/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 251 tokens (49.0%)
    BioCLIP: 126 tokens (24.6%)
    Encoder_0: 135 tokens (26.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.11328125, 'data': 0.15234375, 'dataset': 0.017578125, 'modality': 0.0390625, 'encoder': 0.064453125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1850

📉 LOSS BREAKDOWN:
  Total loss: 0.184955

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001129
    data_loss: 0.010256
    dataset_loss: 0.000222
    encoder_loss: 0.000126

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000113
    alphaearth_modality_loss: 0.091204
    bioclip_modality_loss: 0.017577
    phenovision_modality_loss: 0.238267

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=58
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=78
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=20
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=33

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0135, -0.0033, -0.0414, 0.0264]
    Predicted: [-0.0289, -0.0266, -0.0235, 0.0250]
    Δ (Delta): [-0.0154, -0.0233, 0.0179, -0.0014]
    MAPE: 216.84%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.04 ,  0.04 ,  0.305, -0.245, -0.032,  0.12 , -0.027,  0.108]
    Predicted:
      [ 0.155,  0.063,  0.317, -0.152, -0.054,  0.057, -0.077,  0.167]
    Δ (Delta):
      [ 0.115,  0.023,  0.012,  0.094, -0.022, -0.063, -0.05 ,  0.059]
    MAPE: 92.28%, RMSE: 0.0641

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.416025

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000669
    Max norm: 0.000924
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.004193
    Max norm: 0.327182
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.54s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00090148, max_change=0.00267349
  earth4d: mean_change=0.09160183, max_change=0.10718317
  multimodal: mean_change=0.00139924, max_change=0.03554551
  perceiver: mean_change=0.00190145, max_change=0.01260144

================================================================================
BATCH 78/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 124 tokens (24.2%)
    BioCLIP: 125 tokens (24.4%)
    AlphaEarth: 261 tokens (51.0%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09765625, 'data': 0.16015625, 'dataset': 0.015625, 'modality': 0.0625, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1819

📉 LOSS BREAKDOWN:
  Total loss: 0.181856

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001112
    data_loss: 0.013842
    dataset_loss: 0.000018
    encoder_loss: 0.000142

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000058
    alphaearth_modality_loss: 0.094721
    phenovision_modality_loss: 0.223101
    bioclip_modality_loss: 0.015938

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=50
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=82
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=32
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 14):
    Original:  [-0.0106, -0.0122, -0.0394, 0.0224]
    Predicted: [-0.0106, -0.0062, -0.0306, 0.0301]
    Δ (Delta): [0.0000, 0.0059, 0.0087, 0.0077]
    MAPE: 26.40%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.143,  0.084,  0.226, -0.116, -0.07 ,  0.115, -0.067,  0.265]
    Predicted:
      [ 0.145,  0.065,  0.289, -0.136, -0.078,  0.065, -0.085,  0.137]
    Δ (Delta):
      [ 0.001, -0.019,  0.063, -0.021, -0.009, -0.05 , -0.018, -0.128]
    MAPE: 25.11%, RMSE: 0.0549

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.217789

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000559
    Max norm: 0.000599
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002905
    Max norm: 0.135196
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00089500, max_change=0.00247939
  earth4d: mean_change=0.09174397, max_change=0.10776147
  multimodal: mean_change=0.00146967, max_change=0.03759125
  perceiver: mean_change=0.00175525, max_change=0.01187253

================================================================================
BATCH 79/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 118 tokens (23.0%)
    BioCLIP: 140 tokens (27.3%)
    AlphaEarth: 254 tokens (49.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.091796875, 'data': 0.16015625, 'dataset': 0.013671875, 'modality': 0.0390625, 'encoder': 0.052734375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1720

📉 LOSS BREAKDOWN:
  Total loss: 0.172042

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001190
    data_loss: 0.009653
    dataset_loss: 0.000078
    encoder_loss: 0.000120

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000043
    bioclip_modality_loss: 0.017362
    phenovision_modality_loss: 0.204401
    alphaearth_modality_loss: 0.100587

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=47
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=82
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=7
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=20
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=27

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0139, -0.0031, -0.0419, 0.0268]
    Predicted: [0.0131, -0.0150, -0.0382, 0.0293]
    Δ (Delta): [0.0270, -0.0120, 0.0037, 0.0025]
    MAPE: 151.05%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.188,  0.049,  0.346, -0.174, -0.032,  0.152, -0.121,  0.065]
    Predicted:
      [ 0.146,  0.074,  0.323, -0.14 , -0.083,  0.062, -0.1  ,  0.13 ]
    Δ (Delta):
      [-0.043,  0.025, -0.024,  0.034, -0.051, -0.09 ,  0.021,  0.065]
    MAPE: 54.68%, RMSE: 0.0495

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.197269

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000548
    Max norm: 0.000630
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002696
    Max norm: 0.122904
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.41s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00075478, max_change=0.00195120
  earth4d: mean_change=0.09227063, max_change=0.10822935
  multimodal: mean_change=0.00144104, max_change=0.03939858
  perceiver: mean_change=0.00188617, max_change=0.01096413

================================================================================
BATCH 80/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 254 tokens (49.6%)
    Encoder_0: 126 tokens (24.6%)
    BioCLIP: 131 tokens (25.6%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.083984375, 'data': 0.162109375, 'dataset': 0.017578125, 'modality': 0.060546875, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1655

📉 LOSS BREAKDOWN:
  Total loss: 0.165534

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001118
    data_loss: 0.011149
    dataset_loss: 0.000116
    encoder_loss: 0.000040

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000221
    bioclip_modality_loss: 0.015899
    phenovision_modality_loss: 0.194200
    alphaearth_modality_loss: 0.096357

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=43
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=83
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=31
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0108, -0.0139, -0.0414, 0.0244]
    Predicted: [0.0197, -0.0109, -0.0530, 0.0341]
    Δ (Delta): [0.0305, 0.0030, -0.0116, 0.0097]
    MAPE: 92.99%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.165,  0.039,  0.276, -0.173, -0.099,  0.113, -0.035,  0.115]
    Predicted:
      [ 0.132,  0.035,  0.323, -0.152, -0.071,  0.082, -0.085,  0.11 ]
    Δ (Delta):
      [-0.033, -0.004,  0.047,  0.021,  0.027, -0.031, -0.05 , -0.005]
    MAPE: 32.39%, RMSE: 0.0316

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.210846

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000582
    Max norm: 0.000643
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002725
    Max norm: 0.129973
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00067162, max_change=0.00156011
  earth4d: mean_change=0.09133978, max_change=0.10721638
  multimodal: mean_change=0.00143039, max_change=0.03939327
  perceiver: mean_change=0.00189264, max_change=0.01155035

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.001118 (trend: increasing)
  data_loss: 0.011149 (trend: decreasing)
  dataset_loss: 0.000116 (trend: increasing)
  modality_loss: 0.000221 (trend: increasing)
  encoder_loss: 0.000040 (trend: increasing)
  phenovision_modality_loss: 0.194200 (trend: decreasing)
  bioclip_modality_loss: 0.015899 (trend: decreasing)
  alphaearth_modality_loss: 0.096357 (trend: increasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000582, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.002725, zero_ratio=17.78%

================================================================================
BATCH 81/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 125 tokens (24.4%)
    BioCLIP: 139 tokens (27.1%)
    AlphaEarth: 245 tokens (47.9%)
    Earth4D: 3 tokens (0.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.076171875, 'data': 0.15625, 'dataset': 0.021484375, 'modality': 0.04296875, 'encoder': 0.060546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1401

📉 LOSS BREAKDOWN:
  Total loss: 0.140057

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001137
    data_loss: 0.009843
    dataset_loss: 0.000061
    encoder_loss: 0.000065

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000110
    bioclip_modality_loss: 0.015698
    alphaearth_modality_loss: 0.084325
    phenovision_modality_loss: 0.158084

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=39
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=80
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=31

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 9):
    Original:  [-0.0096, -0.0154, -0.0382, 0.0209]
    Predicted: [0.0244, -0.0175, -0.0502, 0.0179]
    Δ (Delta): [0.0340, -0.0021, -0.0120, -0.0030]
    MAPE: 103.63%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.097,  0.094,  0.239, -0.143,  0.001,  0.184, -0.1  ,  0.046]
    Predicted:
      [ 0.134,  0.025,  0.318, -0.153, -0.055,  0.093, -0.064,  0.118]
    Δ (Delta):
      [ 0.037, -0.07 ,  0.079, -0.01 , -0.055, -0.091,  0.036,  0.072]
    MAPE: 1095.65%, RMSE: 0.0616

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.325509

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000582
    Max norm: 0.000712
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003406
    Max norm: 0.228658
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.55s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00067237, max_change=0.00159360
  earth4d: mean_change=0.09098351, max_change=0.10678584
  multimodal: mean_change=0.00133515, max_change=0.03736317
  perceiver: mean_change=0.00188669, max_change=0.01184472

================================================================================
BATCH 82/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 123 tokens (24.0%)
    AlphaEarth: 263 tokens (51.4%)
    BioCLIP: 124 tokens (24.2%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.109375, 'data': 0.12890625, 'dataset': 0.015625, 'modality': 0.05078125, 'encoder': 0.064453125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.2205

📉 LOSS BREAKDOWN:
  Total loss: 0.220473

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001066
    data_loss: 0.008738
    dataset_loss: 0.000149
    encoder_loss: 0.000034

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000179
    bioclip_modality_loss: 0.015375
    phenovision_modality_loss: 0.317563
    alphaearth_modality_loss: 0.088326

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=56
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=66
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=33

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 8):
    Original:  [-0.0112, -0.0116, -0.0403, 0.0235]
    Predicted: [0.0177, -0.0027, -0.0443, 0.0081]
    Δ (Delta): [0.0289, 0.0089, -0.0040, -0.0154]
    MAPE: 102.74%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.244, -0.043,  0.369, -0.236, -0.022,  0.099, -0.032,  0.237]
    Predicted:
      [ 0.143,  0.047,  0.29 , -0.123, -0.061,  0.081, -0.055,  0.114]
    Δ (Delta):
      [-0.1  ,  0.09 , -0.079,  0.113, -0.039, -0.019, -0.023, -0.123]
    MAPE: 80.08%, RMSE: 0.0827

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.668196

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000577
    Max norm: 0.000649
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.005549
    Max norm: 0.519967
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.34s
  Backward pass: 12.95s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00064065, max_change=0.00157224
  earth4d: mean_change=0.09164565, max_change=0.10757470
  multimodal: mean_change=0.00125987, max_change=0.03571806
  perceiver: mean_change=0.00189613, max_change=0.01203143

================================================================================
BATCH 83/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 114 tokens (22.3%)
    AlphaEarth: 272 tokens (53.1%)
    BioCLIP: 125 tokens (24.4%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.095703125, 'data': 0.18359375, 'dataset': 0.02734375, 'modality': 0.033203125, 'encoder': 0.05859375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1699

📉 LOSS BREAKDOWN:
  Total loss: 0.169916

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001156
    data_loss: 0.009081
    dataset_loss: 0.000227
    encoder_loss: 0.000285

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000398
    bioclip_modality_loss: 0.014926
    phenovision_modality_loss: 0.216429
    alphaearth_modality_loss: 0.087821

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=49
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=94
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=14
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=17
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=30

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 0):
    Original:  [-0.0142, -0.0034, -0.0433, 0.0277]
    Predicted: [0.0100, 0.0039, -0.0327, -0.0001]
    Δ (Delta): [0.0242, 0.0073, 0.0106, -0.0278]
    MAPE: 127.67%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 6):
    Original (first 8 dims):
      [ 0.189,  0.059,  0.306, -0.177, -0.065,  0.131,  0.005,  0.08 ]
    Predicted:
      [ 0.149,  0.07 ,  0.277, -0.138, -0.054,  0.087, -0.032,  0.133]
    Δ (Delta):
      [-0.04 ,  0.011, -0.029,  0.039,  0.011, -0.045, -0.037,  0.053]
    MAPE: 108.80%, RMSE: 0.0361

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.220912

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000708
    Max norm: 0.000786
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002747
    Max norm: 0.131796
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.42s
  Backward pass: 13.07s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00071498, max_change=0.00187933
  earth4d: mean_change=0.09295204, max_change=0.10922937
  multimodal: mean_change=0.00124690, max_change=0.03477094
  perceiver: mean_change=0.00200421, max_change=0.01337962

================================================================================
BATCH 84/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 245 tokens (47.9%)
    Encoder_0: 135 tokens (26.4%)
    BioCLIP: 131 tokens (25.6%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.111328125, 'data': 0.1484375, 'dataset': 0.021484375, 'modality': 0.056640625, 'encoder': 0.048828125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1547

📉 LOSS BREAKDOWN:
  Total loss: 0.154736

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001103
    data_loss: 0.007489
    dataset_loss: 0.000101
    encoder_loss: 0.000101

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000061
    bioclip_modality_loss: 0.014774
    phenovision_modality_loss: 0.185549
    alphaearth_modality_loss: 0.091914

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=57
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=76
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=29
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=25

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0094, -0.0155, -0.0381, 0.0208]
    Predicted: [-0.0148, -0.0172, -0.0220, 0.0218]
    Δ (Delta): [-0.0054, -0.0016, 0.0160, 0.0010]
    MAPE: 28.60%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.096,  0.08 ,  0.292, -0.219, -0.094,  0.113, -0.115,  0.105]
    Predicted:
      [ 0.145,  0.054,  0.232, -0.144, -0.053,  0.088, -0.021,  0.127]
    Δ (Delta):
      [ 0.049, -0.026, -0.06 ,  0.075,  0.041, -0.024,  0.095,  0.022]
    MAPE: 38.22%, RMSE: 0.0548

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.252993

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000515
    Max norm: 0.000601
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002976
    Max norm: 0.175979
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00079316, max_change=0.00199943
  earth4d: mean_change=0.09353957, max_change=0.10964727
  multimodal: mean_change=0.00122717, max_change=0.03557335
  perceiver: mean_change=0.00223433, max_change=0.01440601

================================================================================
BATCH 85/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 265 tokens (51.8%)
    BioCLIP: 126 tokens (24.6%)
    Encoder_0: 119 tokens (23.2%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.095703125, 'data': 0.15234375, 'dataset': 0.021484375, 'modality': 0.056640625, 'encoder': 0.044921875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1727

📉 LOSS BREAKDOWN:
  Total loss: 0.172665

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001179
    data_loss: 0.006753
    dataset_loss: 0.000453
    encoder_loss: 0.000492

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000099
    bioclip_modality_loss: 0.014607
    phenovision_modality_loss: 0.231751
    alphaearth_modality_loss: 0.082901

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=49
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=78
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=29
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=23

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 7):
    Original:  [-0.0102, -0.0135, -0.0389, 0.0219]
    Predicted: [-0.0374, -0.0141, -0.0540, 0.0298]
    Δ (Delta): [-0.0272, -0.0006, -0.0152, 0.0079]
    MAPE: 86.60%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.177, -0.004,  0.224, -0.135, -0.038,  0.089, -0.051,  0.167]
    Predicted:
      [ 0.148,  0.049,  0.235, -0.15 , -0.059,  0.093, -0.04 ,  0.121]
    Δ (Delta):
      [-0.029,  0.053,  0.011, -0.015, -0.021,  0.005,  0.01 , -0.047]
    MAPE: 186.71%, RMSE: 0.0290

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.317816

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000751
    Max norm: 0.000879
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003490
    Max norm: 0.202098
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.52s
  Backward pass: 12.99s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00067983, max_change=0.00163364
  earth4d: mean_change=0.09477657, max_change=0.11092124
  multimodal: mean_change=0.00122843, max_change=0.03715599
  perceiver: mean_change=0.00160129, max_change=0.01198409

================================================================================
BATCH 86/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 268 tokens (52.3%)
    Encoder_0: 110 tokens (21.5%)
    BioCLIP: 130 tokens (25.4%)
    Earth4D: 4 tokens (0.8%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.095703125, 'data': 0.12890625, 'dataset': 0.025390625, 'modality': 0.04296875, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1635

📉 LOSS BREAKDOWN:
  Total loss: 0.163496

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001169
    data_loss: 0.007570
    dataset_loss: 0.000402
    encoder_loss: 0.000377

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000215
    bioclip_modality_loss: 0.013922
    alphaearth_modality_loss: 0.079317
    phenovision_modality_loss: 0.216076

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=49
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=66
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=13
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0092, -0.0160, -0.0376, 0.0202]
    Predicted: [-0.0422, -0.0081, -0.0554, 0.0323]
    Δ (Delta): [-0.0330, 0.0079, -0.0178, 0.0121]
    MAPE: 129.23%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 1, seq 2):
    Original (first 8 dims):
      [ 0.121,  0.051,  0.276, -0.183, -0.068,  0.145, -0.097,  0.138]
    Predicted:
      [ 0.141,  0.041,  0.218, -0.139, -0.068,  0.094, -0.044,  0.111]
    Δ (Delta):
      [ 0.02 , -0.01 , -0.058,  0.045,  0.001, -0.051,  0.052, -0.027]
    MAPE: 23.99%, RMSE: 0.0386

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.338295

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000673
    Max norm: 0.000776
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003441
    Max norm: 0.263896
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.34s
  Backward pass: 12.96s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00081731, max_change=0.00169889
  earth4d: mean_change=0.09517130, max_change=0.11166369
  multimodal: mean_change=0.00123607, max_change=0.03786308
  perceiver: mean_change=0.00222159, max_change=0.01330736

================================================================================
BATCH 87/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 128 tokens (25.0%)
    AlphaEarth: 248 tokens (48.4%)
    BioCLIP: 134 tokens (26.2%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.10546875, 'data': 0.15234375, 'dataset': 0.009765625, 'modality': 0.060546875, 'encoder': 0.048828125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1701

📉 LOSS BREAKDOWN:
  Total loss: 0.170149

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001057
    data_loss: 0.008983
    dataset_loss: 0.000664
    encoder_loss: 0.000121

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000117
    alphaearth_modality_loss: 0.094896
    bioclip_modality_loss: 0.013959
    phenovision_modality_loss: 0.211181

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=54
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=78
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=5
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=31
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=25

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0111, -0.0130, -0.0418, 0.0249]
    Predicted: [-0.0341, -0.0154, -0.0403, 0.0138]
    Δ (Delta): [-0.0230, -0.0024, 0.0015, -0.0111]
    MAPE: 68.49%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.189,  0.122,  0.234, -0.222, -0.171,  0.124, -0.08 ,  0.076]
    Predicted:
      [ 0.145,  0.035,  0.244, -0.144, -0.066,  0.098, -0.045,  0.109]
    Δ (Delta):
      [-0.043, -0.088,  0.01 ,  0.078,  0.105, -0.027,  0.035,  0.033]
    MAPE: 37.91%, RMSE: 0.0610

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.293292

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000585
    Max norm: 0.000669
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003256
    Max norm: 0.201359
    Zero gradient ratio: 17.04%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00102911, max_change=0.00264077
  earth4d: mean_change=0.09547392, max_change=0.11237072
  multimodal: mean_change=0.00129697, max_change=0.03835843
  perceiver: mean_change=0.00238338, max_change=0.01436159

================================================================================
BATCH 88/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 251 tokens (49.0%)
    BioCLIP: 121 tokens (23.6%)
    Encoder_0: 139 tokens (27.1%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.107421875, 'data': 0.13671875, 'dataset': 0.037109375, 'modality': 0.044921875, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1729

📉 LOSS BREAKDOWN:
  Total loss: 0.172887

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001161
    data_loss: 0.006361
    dataset_loss: 0.000470
    encoder_loss: 0.000551

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000026
    bioclip_modality_loss: 0.012756
    phenovision_modality_loss: 0.234739
    alphaearth_modality_loss: 0.083024

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=55
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=70
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=19
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=23
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 13):
    Original:  [-0.0137, -0.0032, -0.0411, 0.0262]
    Predicted: [-0.0003, -0.0277, -0.0138, 0.0018]
    Δ (Delta): [0.0134, -0.0245, 0.0273, -0.0244]
    MAPE: 258.41%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 6):
    Original (first 8 dims):
      [ 0.156,  0.046,  0.266, -0.165, -0.052,  0.152,  0.005,  0.073]
    Predicted:
      [ 0.139,  0.033,  0.225, -0.142, -0.063,  0.086, -0.027,  0.086]
    Δ (Delta):
      [-0.017, -0.013, -0.041,  0.022, -0.012, -0.066, -0.032,  0.013]
    MAPE: 96.13%, RMSE: 0.0322

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.364419

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000946
    Max norm: 0.001314
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003552
    Max norm: 0.280817
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.34s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00096000, max_change=0.00242596
  earth4d: mean_change=0.09661517, max_change=0.11363367
  multimodal: mean_change=0.00132199, max_change=0.03718240
  perceiver: mean_change=0.00198429, max_change=0.01423746

================================================================================
BATCH 89/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 134 tokens (26.2%)
    AlphaEarth: 235 tokens (45.9%)
    BioCLIP: 141 tokens (27.5%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.087890625, 'data': 0.162109375, 'dataset': 0.029296875, 'modality': 0.064453125, 'encoder': 0.0390625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1498

📉 LOSS BREAKDOWN:
  Total loss: 0.149765

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001080
    data_loss: 0.007472
    dataset_loss: 0.000557
    encoder_loss: 0.000321

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000033
    alphaearth_modality_loss: 0.086184
    bioclip_modality_loss: 0.011999
    phenovision_modality_loss: 0.184061

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=45
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=83
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=15
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=33
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=20

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 8):
    Original:  [-0.0143, -0.0034, -0.0438, 0.0280]
    Predicted: [0.0015, -0.0343, -0.0172, -0.0031]
    Δ (Delta): [0.0158, -0.0309, 0.0266, -0.0311]
    MAPE: 299.26%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 7):
    Original (first 8 dims):
      [ 0.152,  0.045,  0.212, -0.162, -0.054,  0.036,  0.042,  0.088]
    Predicted:
      [ 0.148,  0.025,  0.242, -0.152, -0.043,  0.09 , -0.025,  0.089]
    Δ (Delta):
      [-0.004, -0.02 ,  0.03 ,  0.01 ,  0.011,  0.054, -0.067,  0.001]
    MAPE: 49.92%, RMSE: 0.0335

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.136807

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000753
    Max norm: 0.000875
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001691
    Max norm: 0.086182
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00092629, max_change=0.00234210
  earth4d: mean_change=0.09612245, max_change=0.11307701
  multimodal: mean_change=0.00130166, max_change=0.03722162
  perceiver: mean_change=0.00239328, max_change=0.01503923

================================================================================
BATCH 90/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 135 tokens (26.4%)
    Encoder_0: 119 tokens (23.2%)
    AlphaEarth: 257 tokens (50.2%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1015625, 'data': 0.1875, 'dataset': 0.02734375, 'modality': 0.046875, 'encoder': 0.04296875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1604

📉 LOSS BREAKDOWN:
  Total loss: 0.160378

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001047
    data_loss: 0.006734
    dataset_loss: 0.000314
    encoder_loss: 0.000255

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000063
    phenovision_modality_loss: 0.205898
    bioclip_modality_loss: 0.011882
    alphaearth_modality_loss: 0.087288

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=52
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=96
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=14
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=24
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=22

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0092, -0.0160, -0.0376, 0.0201]
    Predicted: [0.0029, -0.0191, -0.0366, -0.0077]
    Δ (Delta): [0.0121, -0.0031, 0.0010, -0.0278]
    MAPE: 72.90%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.219,  0.107,  0.09 ,  0.024, -0.16 ,  0.119,  0.27 , -0.057]
    Predicted:
      [ 0.154,  0.028,  0.261, -0.149, -0.042,  0.087, -0.055,  0.098]
    Δ (Delta):
      [-0.065, -0.079,  0.171, -0.174,  0.118, -0.032, -0.325,  0.155]
    MAPE: 187.47%, RMSE: 0.1638
    🌸 Flowering Probability:
      Original:  0.219
      Predicted: 0.154
      Δ (Delta): -0.065

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.150361

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000715
    Max norm: 0.000970
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001880
    Max norm: 0.093956
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.58s
  Backward pass: 13.08s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00106158, max_change=0.00317147
  earth4d: mean_change=0.09638551, max_change=0.11329345
  multimodal: mean_change=0.00124825, max_change=0.03708554
  perceiver: mean_change=0.00206624, max_change=0.01673766

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.001047 (trend: decreasing)
  data_loss: 0.006734 (trend: decreasing)
  dataset_loss: 0.000314 (trend: decreasing)
  modality_loss: 0.000063 (trend: decreasing)
  encoder_loss: 0.000255 (trend: decreasing)
  phenovision_modality_loss: 0.205898 (trend: decreasing)
  bioclip_modality_loss: 0.011882 (trend: decreasing)
  alphaearth_modality_loss: 0.087288 (trend: increasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000715, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.001880, zero_ratio=20.00%

================================================================================
BATCH 91/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 263 tokens (51.4%)
    Encoder_0: 120 tokens (23.4%)
    BioCLIP: 127 tokens (24.8%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.119140625, 'data': 0.140625, 'dataset': 0.0234375, 'modality': 0.0625, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1701

📉 LOSS BREAKDOWN:
  Total loss: 0.170113

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001102
    data_loss: 0.004747
    dataset_loss: 0.000106
    encoder_loss: 0.000184

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000063
    alphaearth_modality_loss: 0.073687
    phenovision_modality_loss: 0.242827
    bioclip_modality_loss: 0.011943

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=61
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=72
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=32
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 0):
    Original:  [-0.0130, -0.0089, -0.0440, 0.0270]
    Predicted: [-0.0043, -0.0265, -0.0285, 0.0218]
    Δ (Delta): [0.0087, -0.0176, 0.0155, -0.0052]
    MAPE: 79.94%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.152,  0.121,  0.123, -0.202,  0.05 ,  0.172, -0.116,  0.102]
    Predicted:
      [ 0.147,  0.034,  0.22 , -0.147, -0.045,  0.091, -0.034,  0.095]
    Δ (Delta):
      [-0.005, -0.087,  0.097,  0.054, -0.094, -0.081,  0.082, -0.007]
    MAPE: 61.88%, RMSE: 0.0726

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.261800

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000681
    Max norm: 0.000915
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002856
    Max norm: 0.172072
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00103713, max_change=0.00306928
  earth4d: mean_change=0.09753243, max_change=0.11483026
  multimodal: mean_change=0.00117048, max_change=0.03677601
  perceiver: mean_change=0.00218666, max_change=0.01678430

================================================================================
BATCH 92/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 257 tokens (50.2%)
    BioCLIP: 128 tokens (25.0%)
    Encoder_0: 127 tokens (24.8%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1171875, 'data': 0.154296875, 'dataset': 0.021484375, 'modality': 0.048828125, 'encoder': 0.064453125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1636

📉 LOSS BREAKDOWN:
  Total loss: 0.163560

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001100
    data_loss: 0.006416
    dataset_loss: 0.000146
    encoder_loss: 0.000113

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000053
    phenovision_modality_loss: 0.202351
    bioclip_modality_loss: 0.011231
    alphaearth_modality_loss: 0.098443

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=60
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=79
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=25
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=33

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0142, -0.0038, -0.0434, 0.0274]
    Predicted: [-0.0164, -0.0016, -0.0343, 0.0433]
    Δ (Delta): [-0.0021, 0.0021, 0.0091, 0.0159]
    MAPE: 37.57%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 11):
    Original (first 8 dims):
      [ 0.139,  0.07 ,  0.183, -0.172, -0.012,  0.148, -0.038,  0.032]
    Predicted:
      [ 0.137,  0.041,  0.178, -0.129, -0.064,  0.084, -0.018,  0.068]
    Δ (Delta):
      [-0.002, -0.03 , -0.005,  0.042, -0.052, -0.064,  0.02 ,  0.036]
    MAPE: 89.10%, RMSE: 0.0374
    🌸 Flowering Probability:
      Original:  0.139
      Predicted: 0.137
      Δ (Delta): -0.002

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.254208

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000531
    Max norm: 0.000576
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003047
    Max norm: 0.156370
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00096033, max_change=0.00260349
  earth4d: mean_change=0.09735297, max_change=0.11478642
  multimodal: mean_change=0.00117023, max_change=0.03644809
  perceiver: mean_change=0.00195346, max_change=0.01502778

================================================================================
BATCH 93/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 257 tokens (50.2%)
    Encoder_0: 133 tokens (26.0%)
    BioCLIP: 122 tokens (23.8%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.109375, 'data': 0.115234375, 'dataset': 0.017578125, 'modality': 0.05859375, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1632

📉 LOSS BREAKDOWN:
  Total loss: 0.163170

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001059
    data_loss: 0.006199
    dataset_loss: 0.000120
    encoder_loss: 0.000063

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000017
    bioclip_modality_loss: 0.012055
    phenovision_modality_loss: 0.212948
    alphaearth_modality_loss: 0.086781

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=56
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=59
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=30
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0088, -0.0169, -0.0366, 0.0188]
    Predicted: [-0.0093, -0.0081, -0.0261, 0.0403]
    Δ (Delta): [-0.0004, 0.0088, 0.0105, 0.0214]
    MAPE: 49.85%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 15):
    Original (first 8 dims):
      [ 0.149,  0.025,  0.214, -0.116, -0.045,  0.073, -0.053,  0.091]
    Predicted:
      [ 0.148,  0.039,  0.165, -0.127, -0.06 ,  0.099, -0.015,  0.077]
    Δ (Delta):
      [-0.001,  0.013, -0.049, -0.011, -0.015,  0.025,  0.038, -0.014]
    MAPE: 30.37%, RMSE: 0.0255

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.194827

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000517
    Max norm: 0.000585
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002272
    Max norm: 0.163827
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.32s
  Backward pass: 12.93s
  Ratio (back/fwd): 5.6x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00087129, max_change=0.00188182
  earth4d: mean_change=0.09754962, max_change=0.11492520
  multimodal: mean_change=0.00121310, max_change=0.03712614
  perceiver: mean_change=0.00178977, max_change=0.01310326

================================================================================
BATCH 94/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 117 tokens (22.9%)
    Encoder_0: 123 tokens (24.0%)
    AlphaEarth: 270 tokens (52.7%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.12109375, 'data': 0.130859375, 'dataset': 0.025390625, 'modality': 0.046875, 'encoder': 0.052734375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1605

📉 LOSS BREAKDOWN:
  Total loss: 0.160490

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001060
    data_loss: 0.004977
    dataset_loss: 0.000096
    encoder_loss: 0.000129

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000090
    alphaearth_modality_loss: 0.079916
    bioclip_modality_loss: 0.010324
    phenovision_modality_loss: 0.218602

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=62
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=67
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=13
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=24
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=27

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0094, -0.0159, -0.0377, 0.0203]
    Predicted: [-0.0050, -0.0107, -0.0266, 0.0264]
    Δ (Delta): [0.0044, 0.0052, 0.0111, 0.0061]
    MAPE: 34.71%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.212,  0.082,  0.22 , -0.146, -0.014,  0.066, -0.024,  0.066]
    Predicted:
      [ 0.129,  0.028,  0.166, -0.115, -0.059,  0.099, -0.008,  0.078]
    Δ (Delta):
      [-0.083, -0.053, -0.053,  0.03 , -0.045,  0.033,  0.017,  0.012]
    MAPE: 75.61%, RMSE: 0.0462

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.164776

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000487
    Max norm: 0.000596
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002050
    Max norm: 0.109770
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.40s
  Backward pass: 12.95s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00077389, max_change=0.00165016
  earth4d: mean_change=0.09748489, max_change=0.11496436
  multimodal: mean_change=0.00116454, max_change=0.03737080
  perceiver: mean_change=0.00192251, max_change=0.01254007

================================================================================
BATCH 95/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 137 tokens (26.8%)
    AlphaEarth: 250 tokens (48.8%)
    BioCLIP: 124 tokens (24.2%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.119140625, 'data': 0.150390625, 'dataset': 0.015625, 'modality': 0.0625, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1412

📉 LOSS BREAKDOWN:
  Total loss: 0.141163

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001030
    data_loss: 0.006995
    dataset_loss: 0.000008
    encoder_loss: 0.000063

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000157
    alphaearth_modality_loss: 0.079631
    bioclip_modality_loss: 0.009703
    phenovision_modality_loss: 0.176895

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=61
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=77
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=32
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0125, -0.0096, -0.0426, 0.0258]
    Predicted: [-0.0075, 0.0024, -0.0412, 0.0081]
    Δ (Delta): [0.0049, 0.0121, 0.0014, -0.0177]
    MAPE: 59.21%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.146, -0.042,  0.207, -0.086,  0.002,  0.05 , -0.169,  0.098]
    Predicted:
      [ 0.135,  0.023,  0.218, -0.119, -0.053,  0.086, -0.031,  0.091]
    Δ (Delta):
      [-0.01 ,  0.065,  0.011, -0.033, -0.055,  0.036,  0.137, -0.007]
    MAPE: 491.42%, RMSE: 0.0600

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.161604

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000504
    Max norm: 0.000551
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002101
    Max norm: 0.107793
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.53s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00076438, max_change=0.00168556
  earth4d: mean_change=0.09808866, max_change=0.11547039
  multimodal: mean_change=0.00116795, max_change=0.03725404
  perceiver: mean_change=0.00180162, max_change=0.01265100

================================================================================
BATCH 96/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 135 tokens (26.4%)
    AlphaEarth: 251 tokens (49.0%)
    BioCLIP: 124 tokens (24.2%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.091796875, 'data': 0.146484375, 'dataset': 0.02734375, 'modality': 0.052734375, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1451

📉 LOSS BREAKDOWN:
  Total loss: 0.145143

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001087
    data_loss: 0.005589
    dataset_loss: 0.000018
    encoder_loss: 0.000041

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000044
    alphaearth_modality_loss: 0.085147
    bioclip_modality_loss: 0.010412
    phenovision_modality_loss: 0.181355

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=47
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=75
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=14
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=27
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0138, -0.0033, -0.0412, 0.0261]
    Predicted: [-0.0187, -0.0081, -0.0465, 0.0103]
    Δ (Delta): [-0.0049, -0.0049, -0.0053, -0.0158]
    MAPE: 64.65%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 12):
    Original (first 8 dims):
      [ 0.176, -0.012,  0.198, -0.17 , -0.073,  0.126, -0.043,  0.132]
    Predicted:
      [ 0.146,  0.002,  0.221, -0.125, -0.034,  0.094, -0.036,  0.109]
    Δ (Delta):
      [-0.03 ,  0.014,  0.022,  0.045,  0.039, -0.032,  0.007, -0.023]
    MAPE: 35.73%, RMSE: 0.0290

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.174303

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000653
    Max norm: 0.000754
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002172
    Max norm: 0.103299
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00077170, max_change=0.00166009
  earth4d: mean_change=0.09811386, max_change=0.11558425
  multimodal: mean_change=0.00119127, max_change=0.03784540
  perceiver: mean_change=0.00169727, max_change=0.01171315

================================================================================
BATCH 97/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 112 tokens (21.9%)
    BioCLIP: 134 tokens (26.2%)
    AlphaEarth: 265 tokens (51.8%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.138671875, 'data': 0.166015625, 'dataset': 0.0234375, 'modality': 0.03125, 'encoder': 0.06640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1418

📉 LOSS BREAKDOWN:
  Total loss: 0.141844

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001091
    data_loss: 0.005446
    dataset_loss: 0.000031
    encoder_loss: 0.000092

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000096
    phenovision_modality_loss: 0.179427
    alphaearth_modality_loss: 0.082186
    bioclip_modality_loss: 0.008957

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=71
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=85
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=16
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=34

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0095, -0.0156, -0.0377, 0.0203]
    Predicted: [-0.0323, -0.0090, -0.0504, 0.0093]
    Δ (Delta): [-0.0228, 0.0065, -0.0128, -0.0110]
    MAPE: 92.72%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.254, -0.11 ,  0.145, -0.098, -0.069, -0.012, -0.008,  0.052]
    Predicted:
      [ 0.15 , -0.02 ,  0.194, -0.129, -0.022,  0.106, -0.017,  0.112]
    Δ (Delta):
      [-0.104,  0.09 ,  0.049, -0.031,  0.047,  0.118, -0.009,  0.06 ]
    MAPE: 180.16%, RMSE: 0.0726
    🌸 Flowering Probability:
      Original:  0.254
      Predicted: 0.150
      Δ (Delta): -0.104

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.122115

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000743
    Max norm: 0.000890
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001516
    Max norm: 0.079939
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.39s
  Backward pass: 13.03s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00113322, max_change=0.00365214
  earth4d: mean_change=0.10018993, max_change=0.11771937
  multimodal: mean_change=0.00119003, max_change=0.03774286
  perceiver: mean_change=0.00235953, max_change=0.01797087

================================================================================
BATCH 98/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 254 tokens (49.6%)
    Encoder_0: 135 tokens (26.4%)
    BioCLIP: 122 tokens (23.8%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.095703125, 'data': 0.16015625, 'dataset': 0.03125, 'modality': 0.056640625, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1600

📉 LOSS BREAKDOWN:
  Total loss: 0.159959

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001049
    data_loss: 0.004193
    dataset_loss: 0.000097
    encoder_loss: 0.000047

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000269
    bioclip_modality_loss: 0.009240
    alphaearth_modality_loss: 0.085374
    phenovision_modality_loss: 0.214738

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=49
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=82
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=16
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=29
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 7):
    Original:  [-0.0096, -0.0153, -0.0378, 0.0205]
    Predicted: [-0.0179, -0.0061, -0.0372, 0.0197]
    Δ (Delta): [-0.0082, 0.0092, 0.0006, -0.0008]
    MAPE: 37.76%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 12):
    Original (first 8 dims):
      [ 0.13 ,  0.038,  0.176, -0.13 , -0.046,  0.04 , -0.044,  0.079]
    Predicted:
      [ 0.14 ,  0.027,  0.179, -0.102, -0.032,  0.073,  0.002,  0.081]
    Δ (Delta):
      [ 0.01 , -0.011,  0.003,  0.028,  0.014,  0.032,  0.046,  0.002]
    MAPE: 34.81%, RMSE: 0.0234

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.137019

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000567
    Max norm: 0.000598
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001627
    Max norm: 0.102150
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.39s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00110355, max_change=0.00334647
  earth4d: mean_change=0.10004662, max_change=0.11757578
  multimodal: mean_change=0.00113558, max_change=0.03732843
  perceiver: mean_change=0.00199823, max_change=0.01689046

================================================================================
BATCH 99/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 127 tokens (24.8%)
    Earth4D: 1 tokens (0.2%)
    AlphaEarth: 252 tokens (49.2%)
    Encoder_0: 132 tokens (25.8%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.12109375, 'data': 0.1328125, 'dataset': 0.015625, 'modality': 0.044921875, 'encoder': 0.03515625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1438

📉 LOSS BREAKDOWN:
  Total loss: 0.143753

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001072
    data_loss: 0.005773
    dataset_loss: 0.000023
    encoder_loss: 0.000024

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000098
    phenovision_modality_loss: 0.170099
    bioclip_modality_loss: 0.009486
    alphaearth_modality_loss: 0.094203

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=62
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=68
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=23
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=18

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0120, -0.0110, -0.0423, 0.0253]
    Predicted: [-0.0056, -0.0158, -0.0211, 0.0184]
    Δ (Delta): [0.0065, -0.0048, 0.0212, -0.0069]
    MAPE: 43.65%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [-0.026,  0.084,  0.059, -0.296, -0.225,  0.16 ,  0.085,  0.186]
    Predicted:
      [ 0.136,  0.042,  0.173, -0.098, -0.032,  0.055,  0.011,  0.066]
    Δ (Delta):
      [ 0.162, -0.043,  0.114,  0.198,  0.193, -0.105, -0.074, -0.12 ]
    MAPE: 155.68%, RMSE: 0.1361
    🌸 Flowering Probability:
      Original:  -0.026
      Predicted: 0.136
      Δ (Delta): 0.162

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.235149

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000623
    Max norm: 0.000728
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002776
    Max norm: 0.142962
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.50s
  Backward pass: 12.96s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00095143, max_change=0.00260732
  earth4d: mean_change=0.10087401, max_change=0.11834397
  multimodal: mean_change=0.00117966, max_change=0.03757726
  perceiver: mean_change=0.00189980, max_change=0.01504987

================================================================================
BATCH 100/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 258 tokens (50.4%)
    Encoder_0: 124 tokens (24.2%)
    BioCLIP: 130 tokens (25.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.095703125, 'data': 0.171875, 'dataset': 0.01953125, 'modality': 0.060546875, 'encoder': 0.0390625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1537

📉 LOSS BREAKDOWN:
  Total loss: 0.153724

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001053
    data_loss: 0.004972
    dataset_loss: 0.000007
    encoder_loss: 0.000141

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000207
    bioclip_modality_loss: 0.008499
    phenovision_modality_loss: 0.207233
    alphaearth_modality_loss: 0.079593

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=49
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=88
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=31
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=20

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 14):
    Original:  [-0.0092, -0.0161, -0.0368, 0.0193]
    Predicted: [0.0004, -0.0148, -0.0188, 0.0071]
    Δ (Delta): [0.0096, 0.0013, 0.0180, -0.0121]
    MAPE: 56.11%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.133,  0.034,  0.179, -0.143, -0.055,  0.083, -0.003,  0.073]
    Predicted:
      [ 0.129,  0.042,  0.189, -0.101, -0.033,  0.041,  0.008,  0.062]
    Δ (Delta):
      [-0.004,  0.008,  0.01 ,  0.042,  0.022, -0.041,  0.011, -0.01 ]
    MAPE: 63.38%, RMSE: 0.0233

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.219461

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000605
    Max norm: 0.000701
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002430
    Max norm: 0.132370
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.39s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00080738, max_change=0.00206708
  earth4d: mean_change=0.10012621, max_change=0.11721304
  multimodal: mean_change=0.00114433, max_change=0.03638919
  perceiver: mean_change=0.00169587, max_change=0.01327857

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.001053 (trend: decreasing)
  data_loss: 0.004972 (trend: decreasing)
  dataset_loss: 0.000007 (trend: decreasing)
  modality_loss: 0.000207 (trend: increasing)
  encoder_loss: 0.000141 (trend: increasing)
  phenovision_modality_loss: 0.207233 (trend: increasing)
  bioclip_modality_loss: 0.008499 (trend: decreasing)
  alphaearth_modality_loss: 0.079593 (trend: decreasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000605, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.002430, zero_ratio=17.78%

================================================================================
BATCH 101/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 253 tokens (49.4%)
    Encoder_0: 123 tokens (24.0%)
    BioCLIP: 136 tokens (26.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1015625, 'data': 0.13671875, 'dataset': 0.017578125, 'modality': 0.05078125, 'encoder': 0.0390625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1554

📉 LOSS BREAKDOWN:
  Total loss: 0.155385

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000999
    data_loss: 0.005450
    dataset_loss: 0.000016
    encoder_loss: 0.000027

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000182
    phenovision_modality_loss: 0.204899
    bioclip_modality_loss: 0.008422
    alphaearth_modality_loss: 0.084505

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=52
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=70
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=20

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0094, -0.0157, -0.0372, 0.0197]
    Predicted: [-0.0019, -0.0164, -0.0284, 0.0089]
    Δ (Delta): [0.0075, -0.0006, 0.0088, -0.0108]
    MAPE: 40.47%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.019,  0.105,  0.164, -0.002,  0.128, -0.007,  0.039,  0.099]
    Predicted:
      [ 0.114,  0.017,  0.161, -0.088, -0.042,  0.042,  0.003,  0.054]
    Δ (Delta):
      [ 0.095, -0.088, -0.003, -0.086, -0.17 ,  0.049, -0.036, -0.046]
    MAPE: 689.42%, RMSE: 0.0858
    🌸 Flowering Probability:
      Original:  0.019
      Predicted: 0.114
      Δ (Delta): 0.095

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.177332

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000550
    Max norm: 0.000618
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002192
    Max norm: 0.100085
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.35s
  Backward pass: 12.96s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00070216, max_change=0.00163489
  earth4d: mean_change=0.09957767, max_change=0.11627362
  multimodal: mean_change=0.00113241, max_change=0.03660080
  perceiver: mean_change=0.00173564, max_change=0.01265027

================================================================================
BATCH 102/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 134 tokens (26.2%)
    BioCLIP: 128 tokens (25.0%)
    AlphaEarth: 247 tokens (48.2%)
    Earth4D: 3 tokens (0.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.138671875, 'data': 0.115234375, 'dataset': 0.0234375, 'modality': 0.044921875, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1331

📉 LOSS BREAKDOWN:
  Total loss: 0.133079

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000996
    data_loss: 0.003926
    dataset_loss: 0.000053
    encoder_loss: 0.000132

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000033
    phenovision_modality_loss: 0.165565
    alphaearth_modality_loss: 0.083146
    bioclip_modality_loss: 0.007561

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=71
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=59
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=23
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0118, -0.0103, -0.0405, 0.0236]
    Predicted: [0.0048, -0.0184, -0.0471, 0.0122]
    Δ (Delta): [0.0167, -0.0081, -0.0065, -0.0114]
    MAPE: 70.89%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 14):
    Original (first 8 dims):
      [ 0.136,  0.089,  0.26 , -0.079, -0.069,  0.092,  0.011,  0.163]
    Predicted:
      [ 0.128,  0.018,  0.203, -0.088, -0.054,  0.031, -0.026,  0.062]
    Δ (Delta):
      [-0.008, -0.071, -0.057, -0.009,  0.014, -0.062, -0.037, -0.101]
    MAPE: 75.52%, RMSE: 0.0549
    🌸 Flowering Probability:
      Original:  0.136
      Predicted: 0.128
      Δ (Delta): -0.008

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.467979

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000470
    Max norm: 0.000611
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003771
    Max norm: 0.366130
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.33s
  Backward pass: 12.93s
  Ratio (back/fwd): 5.6x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00093294, max_change=0.00258022
  earth4d: mean_change=0.10077968, max_change=0.11768581
  multimodal: mean_change=0.00108344, max_change=0.03679977
  perceiver: mean_change=0.00184205, max_change=0.01241335

================================================================================
BATCH 103/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 277 tokens (54.1%)
    BioCLIP: 115 tokens (22.5%)
    Encoder_0: 120 tokens (23.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.126953125, 'data': 0.14453125, 'dataset': 0.021484375, 'modality': 0.0390625, 'encoder': 0.044921875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1501

📉 LOSS BREAKDOWN:
  Total loss: 0.150054

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000938
    data_loss: 0.003986
    dataset_loss: 0.000038
    encoder_loss: 0.000012

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000220
    phenovision_modality_loss: 0.193893
    bioclip_modality_loss: 0.008208
    alphaearth_modality_loss: 0.088102

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=65
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=74
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=20
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=23

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0093, -0.0161, -0.0372, 0.0195]
    Predicted: [0.0008, -0.0241, -0.0448, 0.0249]
    Δ (Delta): [0.0101, -0.0080, -0.0077, 0.0055]
    MAPE: 51.79%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.117,  0.038,  0.202, -0.047,  0.004,  0.027,  0.035,  0.109]
    Predicted:
      [ 0.122, -0.009,  0.161, -0.105, -0.058,  0.054, -0.014,  0.078]
    Δ (Delta):
      [ 0.004, -0.047, -0.041, -0.058, -0.062,  0.027, -0.049, -0.031]
    MAPE: 255.67%, RMSE: 0.0438
    🌸 Flowering Probability:
      Original:  0.117
      Predicted: 0.122
      Δ (Delta): 0.004

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.240915

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000499
    Max norm: 0.000601
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002665
    Max norm: 0.149330
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.52s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00091430, max_change=0.00234209
  earth4d: mean_change=0.10131252, max_change=0.11807314
  multimodal: mean_change=0.00109359, max_change=0.03725414
  perceiver: mean_change=0.00176008, max_change=0.01229045

================================================================================
BATCH 104/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 247 tokens (48.2%)
    BioCLIP: 137 tokens (26.8%)
    Encoder_0: 127 tokens (24.8%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.091796875, 'data': 0.15625, 'dataset': 0.025390625, 'modality': 0.0546875, 'encoder': 0.037109375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1423

📉 LOSS BREAKDOWN:
  Total loss: 0.142309

  Universal Space Losses (256D token space):
    spacetime_loss: 0.001025
    data_loss: 0.005173
    dataset_loss: 0.000029
    encoder_loss: 0.000094

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000498
    phenovision_modality_loss: 0.184849
    alphaearth_modality_loss: 0.080350
    bioclip_modality_loss: 0.006899

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=47
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=80
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=13
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=28
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=19

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 9):
    Original:  [-0.0145, -0.0035, -0.0434, 0.0272]
    Predicted: [-0.0002, -0.0100, -0.0466, 0.0303]
    Δ (Delta): [0.0143, -0.0065, -0.0032, 0.0032]
    MAPE: 75.48%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [-0.023,  0.141,  0.108,  0.014,  0.099,  0.067,  0.052,  0.097]
    Predicted:
      [ 0.115, -0.02 ,  0.127, -0.097, -0.063,  0.059, -0.004,  0.063]
    Δ (Delta):
      [ 0.138, -0.161,  0.019, -0.111, -0.162, -0.008, -0.056, -0.034]
    MAPE: 228.07%, RMSE: 0.1050
    🌸 Flowering Probability:
      Original:  -0.023
      Predicted: 0.115
      Δ (Delta): 0.138

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.190938

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000698
    Max norm: 0.000806
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002262
    Max norm: 0.110965
    Zero gradient ratio: 20.37%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00089460, max_change=0.00214684
  earth4d: mean_change=0.10114524, max_change=0.11792678
  multimodal: mean_change=0.00108097, max_change=0.03656117
  perceiver: mean_change=0.00167245, max_change=0.01370076

================================================================================
BATCH 105/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 138 tokens (27.0%)
    AlphaEarth: 240 tokens (46.9%)
    Encoder_0: 131 tokens (25.6%)
    Earth4D: 3 tokens (0.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09375, 'data': 0.162109375, 'dataset': 0.02734375, 'modality': 0.04296875, 'encoder': 0.0390625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1475

📉 LOSS BREAKDOWN:
  Total loss: 0.147488

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000983
    data_loss: 0.004219
    dataset_loss: 0.000023
    encoder_loss: 0.000013

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000015
    alphaearth_modality_loss: 0.082097
    phenovision_modality_loss: 0.195024
    bioclip_modality_loss: 0.007440

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=48
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=83
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=14
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=20

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0094, -0.0155, -0.0366, 0.0190]
    Predicted: [-0.0064, -0.0101, -0.0339, 0.0197]
    Δ (Delta): [0.0030, 0.0054, 0.0027, 0.0007]
    MAPE: 19.59%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.12 ,  0.041,  0.271, -0.101,  0.035, -0.014,  0.039,  0.003]
    Predicted:
      [ 0.118, -0.005,  0.146, -0.081, -0.043,  0.061, -0.012,  0.081]
    Δ (Delta):
      [-0.002, -0.046, -0.125,  0.02 , -0.078,  0.075, -0.051,  0.078]
    MAPE: 495.04%, RMSE: 0.0696

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.144707

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000595
    Max norm: 0.000769
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001711
    Max norm: 0.113404
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.39s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00088333, max_change=0.00197080
  earth4d: mean_change=0.10087970, max_change=0.11759192
  multimodal: mean_change=0.00110646, max_change=0.03675631
  perceiver: mean_change=0.00187828, max_change=0.01392323

================================================================================
BATCH 106/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 271 tokens (52.9%)
    BioCLIP: 123 tokens (24.0%)
    Encoder_0: 117 tokens (22.9%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.107421875, 'data': 0.1640625, 'dataset': 0.021484375, 'modality': 0.05859375, 'encoder': 0.05859375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1316

📉 LOSS BREAKDOWN:
  Total loss: 0.131577

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000960
    data_loss: 0.003754
    dataset_loss: 0.000173
    encoder_loss: 0.000186

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000433
    alphaearth_modality_loss: 0.078183
    phenovision_modality_loss: 0.168445
    bioclip_modality_loss: 0.006941

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=55
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=84
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=30
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=30

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 6):
    Original:  [-0.0093, -0.0161, -0.0371, 0.0195]
    Predicted: [-0.0309, -0.0102, -0.0263, 0.0278]
    Δ (Delta): [-0.0216, 0.0059, 0.0108, 0.0084]
    MAPE: 85.20%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.085,  0.046,  0.19 , -0.082,  0.051,  0.058, -0.003,  0.052]
    Predicted:
      [ 0.109,  0.017,  0.158, -0.06 , -0.036,  0.053, -0.016,  0.083]
    Δ (Delta):
      [ 0.024, -0.029, -0.031,  0.022, -0.087, -0.005, -0.013,  0.031]
    MAPE: 97.23%, RMSE: 0.0381

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.230950

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000637
    Max norm: 0.000791
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002372
    Max norm: 0.156824
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.41s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00091016, max_change=0.00251425
  earth4d: mean_change=0.10134562, max_change=0.11841045
  multimodal: mean_change=0.00106323, max_change=0.03654165
  perceiver: mean_change=0.00160828, max_change=0.01255290

================================================================================
BATCH 107/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 117 tokens (22.9%)
    AlphaEarth: 259 tokens (50.6%)
    BioCLIP: 136 tokens (26.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.12890625, 'data': 0.173828125, 'dataset': 0.017578125, 'modality': 0.05078125, 'encoder': 0.05859375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1406

📉 LOSS BREAKDOWN:
  Total loss: 0.140619

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000986
    data_loss: 0.003199
    dataset_loss: 0.000043
    encoder_loss: 0.000181

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000498
    alphaearth_modality_loss: 0.080284
    phenovision_modality_loss: 0.185066
    bioclip_modality_loss: 0.007375

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=66
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=89
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=30

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 12):
    Original:  [-0.0093, -0.0161, -0.0371, 0.0195]
    Predicted: [-0.0296, -0.0084, -0.0197, 0.0251]
    Δ (Delta): [-0.0203, 0.0077, 0.0174, 0.0057]
    MAPE: 85.39%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 8):
    Original (first 8 dims):
      [ 0.135,  0.053,  0.142, -0.096,  0.012,  0.02 , -0.029,  0.019]
    Predicted:
      [ 0.103,  0.019,  0.172, -0.063, -0.028,  0.051, -0.021,  0.09 ]
    Δ (Delta):
      [-0.032, -0.033,  0.03 ,  0.034, -0.04 ,  0.03 ,  0.007,  0.071]
    MAPE: 127.71%, RMSE: 0.0384

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.115674

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000648
    Max norm: 0.000726
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001356
    Max norm: 0.079116
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.54s
  Backward pass: 13.05s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00092017, max_change=0.00238826
  earth4d: mean_change=0.10296110, max_change=0.12027537
  multimodal: mean_change=0.00103931, max_change=0.03639645
  perceiver: mean_change=0.00173570, max_change=0.01312479

================================================================================
BATCH 108/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 235 tokens (45.9%)
    BioCLIP: 134 tokens (26.2%)
    Encoder_0: 142 tokens (27.7%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.095703125, 'data': 0.134765625, 'dataset': 0.015625, 'modality': 0.046875, 'encoder': 0.044921875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1247

📉 LOSS BREAKDOWN:
  Total loss: 0.124671

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000963
    data_loss: 0.003012
    dataset_loss: 0.000056
    encoder_loss: 0.000242

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000055
    alphaearth_modality_loss: 0.082355
    bioclip_modality_loss: 0.006700
    phenovision_modality_loss: 0.152267

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=49
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=69
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=24
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=23

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 9):
    Original:  [-0.0139, -0.0073, -0.0451, 0.0278]
    Predicted: [-0.0168, -0.0043, -0.0264, 0.0142]
    Δ (Delta): [-0.0029, 0.0030, 0.0187, -0.0136]
    MAPE: 38.12%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [ 0.26 ,  0.107,  0.227, -0.084,  0.007,  0.013,  0.14 ,  0.315]
    Predicted:
      [ 0.097,  0.004,  0.167, -0.072, -0.032,  0.044, -0.019,  0.071]
    Δ (Delta):
      [-0.163, -0.103, -0.059,  0.011, -0.039,  0.031, -0.158, -0.244]
    MAPE: 148.44%, RMSE: 0.1265

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.325228

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000530
    Max norm: 0.000584
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003009
    Max norm: 0.238877
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.34s
  Backward pass: 12.96s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00082674, max_change=0.00214071
  earth4d: mean_change=0.10222673, max_change=0.11986531
  multimodal: mean_change=0.00100544, max_change=0.03566859
  perceiver: mean_change=0.00170080, max_change=0.01244337

================================================================================
BATCH 109/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 267 tokens (52.1%)
    BioCLIP: 137 tokens (26.8%)
    Encoder_0: 108 tokens (21.1%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.072265625, 'data': 0.14453125, 'dataset': 0.015625, 'modality': 0.044921875, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1242

📉 LOSS BREAKDOWN:
  Total loss: 0.124212

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000990
    data_loss: 0.002493
    dataset_loss: 0.000093
    encoder_loss: 0.000764

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000268
    phenovision_modality_loss: 0.158323
    bioclip_modality_loss: 0.006474
    alphaearth_modality_loss: 0.076435

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=37
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=74
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=23
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0144, -0.0037, -0.0432, 0.0270]
    Predicted: [-0.0088, 0.0030, -0.0539, 0.0105]
    Δ (Delta): [0.0056, 0.0067, -0.0107, -0.0165]
    MAPE: 76.36%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.141, -0.091,  0.201, -0.09 ,  0.004,  0.086, -0.167,  0.102]
    Predicted:
      [ 0.102, -0.008,  0.159, -0.073, -0.022,  0.037, -0.005,  0.065]
    Δ (Delta):
      [-0.039,  0.083, -0.042,  0.017, -0.025, -0.049,  0.162, -0.037]
    MAPE: 131.34%, RMSE: 0.0718
    🌸 Flowering Probability:
      Original:  0.141
      Predicted: 0.102
      Δ (Delta): -0.039

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.140412

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000654
    Max norm: 0.000737
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001637
    Max norm: 0.093627
    Zero gradient ratio: 18.89%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 12.99s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00077368, max_change=0.00192651
  earth4d: mean_change=0.10155478, max_change=0.11884332
  multimodal: mean_change=0.00100949, max_change=0.03575474
  perceiver: mean_change=0.00176466, max_change=0.01183985

================================================================================
BATCH 110/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 123 tokens (24.0%)
    BioCLIP: 119 tokens (23.2%)
    AlphaEarth: 270 tokens (52.7%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1015625, 'data': 0.158203125, 'dataset': 0.015625, 'modality': 0.05078125, 'encoder': 0.04296875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1459

📉 LOSS BREAKDOWN:
  Total loss: 0.145907

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000944
    data_loss: 0.003189
    dataset_loss: 0.000021
    encoder_loss: 0.000126

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000563
    alphaearth_modality_loss: 0.076393
    phenovision_modality_loss: 0.200663
    bioclip_modality_loss: 0.006351

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=52
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=81
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=22

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 5):
    Original:  [-0.0117, -0.0087, -0.0385, 0.0222]
    Predicted: [-0.0062, -0.0102, -0.0419, 0.0155]
    Δ (Delta): [0.0056, -0.0015, -0.0034, -0.0067]
    MAPE: 25.89%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 1):
    Original (first 8 dims):
      [ 0.145,  0.012,  0.204, -0.027,  0.015,  0.064,  0.028,  0.112]
    Predicted:
      [ 0.103,  0.005,  0.139, -0.05 , -0.037,  0.024,  0.002,  0.066]
    Δ (Delta):
      [-0.042, -0.007, -0.065, -0.023, -0.052, -0.04 , -0.026, -0.046]
    MAPE: 92.63%, RMSE: 0.0413

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.179588

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000575
    Max norm: 0.000710
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002057
    Max norm: 0.104815
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00079523, max_change=0.00228300
  earth4d: mean_change=0.10238527, max_change=0.12001225
  multimodal: mean_change=0.00101539, max_change=0.03630956
  perceiver: mean_change=0.00150396, max_change=0.01131418

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.000944 (trend: decreasing)
  data_loss: 0.003189 (trend: decreasing)
  dataset_loss: 0.000021 (trend: decreasing)
  modality_loss: 0.000563 (trend: increasing)
  encoder_loss: 0.000126 (trend: decreasing)
  phenovision_modality_loss: 0.200663 (trend: increasing)
  bioclip_modality_loss: 0.006351 (trend: decreasing)
  alphaearth_modality_loss: 0.076393 (trend: decreasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000575, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.002057, zero_ratio=17.78%

================================================================================
BATCH 111/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 254 tokens (49.6%)
    Encoder_0: 117 tokens (22.9%)
    BioCLIP: 139 tokens (27.1%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.12109375, 'data': 0.12890625, 'dataset': 0.01171875, 'modality': 0.05078125, 'encoder': 0.0390625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1386

📉 LOSS BREAKDOWN:
  Total loss: 0.138579

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000940
    data_loss: 0.002583
    dataset_loss: 0.000012
    encoder_loss: 0.000233

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000087
    phenovision_modality_loss: 0.172801
    bioclip_modality_loss: 0.006514
    alphaearth_modality_loss: 0.090729

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=62
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=66
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=6
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=20

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0093, -0.0160, -0.0370, 0.0193]
    Predicted: [-0.0105, -0.0157, -0.0349, 0.0267]
    Δ (Delta): [-0.0012, 0.0003, 0.0021, 0.0074]
    MAPE: 14.70%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 6):
    Original (first 8 dims):
      [ 0.129, -0.058,  0.082, -0.029, -0.045,  0.035, -0.013,  0.028]
    Predicted:
      [ 0.098,  0.005,  0.138, -0.055, -0.034,  0.024,  0.011,  0.067]
    Δ (Delta):
      [-0.031,  0.063,  0.056, -0.026,  0.011, -0.011,  0.024,  0.039]
    MAPE: 83.83%, RMSE: 0.0373
    🌸 Flowering Probability:
      Original:  0.129
      Predicted: 0.098
      Δ (Delta): -0.031

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.271726

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000520
    Max norm: 0.000688
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002948
    Max norm: 0.171170
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 12.97s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00073334, max_change=0.00214090
  earth4d: mean_change=0.10292195, max_change=0.12078622
  multimodal: mean_change=0.00101768, max_change=0.03689875
  perceiver: mean_change=0.00159881, max_change=0.01105675

================================================================================
BATCH 112/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 126 tokens (24.6%)
    AlphaEarth: 264 tokens (51.6%)
    BioCLIP: 122 tokens (23.8%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.0859375, 'data': 0.1484375, 'dataset': 0.015625, 'modality': 0.046875, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1640

📉 LOSS BREAKDOWN:
  Total loss: 0.164029

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000955
    data_loss: 0.002489
    dataset_loss: 0.000078
    encoder_loss: 0.000024

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000305
    alphaearth_modality_loss: 0.071142
    phenovision_modality_loss: 0.243760
    bioclip_modality_loss: 0.006186

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=44
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=76
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=24
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 1):
    Original:  [-0.0031, -0.0300, -0.0250, -0.0049]
    Predicted: [-0.0084, -0.0104, -0.0526, 0.0166]
    Δ (Delta): [-0.0053, 0.0196, -0.0276, 0.0215]
    MAPE: 196.24%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [ 0.127, -0.05 ,  0.181, -0.004, -0.033, -0.049, -0.038, -0.001]
    Predicted:
      [ 0.111, -0.011,  0.127, -0.057, -0.036,  0.036,  0.005,  0.078]
    Δ (Delta):
      [-0.016,  0.039, -0.053, -0.053, -0.003,  0.085,  0.042,  0.079]
    MAPE: 941.56%, RMSE: 0.0533

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.230586

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000507
    Max norm: 0.000547
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002434
    Max norm: 0.137939
    Zero gradient ratio: 17.78%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.54s
  Backward pass: 12.99s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00071562, max_change=0.00193186
  earth4d: mean_change=0.10281278, max_change=0.12014929
  multimodal: mean_change=0.00102639, max_change=0.03668582
  perceiver: mean_change=0.00163167, max_change=0.01117169

================================================================================
BATCH 113/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 135 tokens (26.4%)
    AlphaEarth: 260 tokens (50.8%)
    BioCLIP: 116 tokens (22.7%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1015625, 'data': 0.16015625, 'dataset': 0.021484375, 'modality': 0.052734375, 'encoder': 0.048828125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1664

📉 LOSS BREAKDOWN:
  Total loss: 0.166354

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000980
    data_loss: 0.003104
    dataset_loss: 0.000004
    encoder_loss: 0.000339

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000132
    bioclip_modality_loss: 0.005375
    phenovision_modality_loss: 0.231681
    alphaearth_modality_loss: 0.087388

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=52
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=82
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=27
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=25

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0126, -0.0047, -0.0375, 0.0226]
    Predicted: [-0.0053, -0.0143, -0.0492, 0.0150]
    Δ (Delta): [0.0073, -0.0096, -0.0117, -0.0076]
    MAPE: 81.06%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [ 0.114, -0.008,  0.138, -0.053, -0.035,  0.046, -0.009,  0.059]
    Predicted:
      [ 0.109, -0.028,  0.141, -0.081, -0.043,  0.033,  0.   ,  0.074]
    Δ (Delta):
      [-0.004, -0.019,  0.003, -0.029, -0.008, -0.012,  0.009,  0.015]
    MAPE: 59.01%, RMSE: 0.0148

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.155814

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000539
    Max norm: 0.000624
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001956
    Max norm: 0.090504
    Zero gradient ratio: 18.52%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.41s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00076499, max_change=0.00224646
  earth4d: mean_change=0.10310045, max_change=0.12091481
  multimodal: mean_change=0.00101762, max_change=0.03573611
  perceiver: mean_change=0.00137000, max_change=0.01101695

================================================================================
BATCH 114/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 126 tokens (24.6%)
    Encoder_0: 130 tokens (25.4%)
    AlphaEarth: 255 tokens (49.8%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09375, 'data': 0.140625, 'dataset': 0.015625, 'modality': 0.05078125, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1445

📉 LOSS BREAKDOWN:
  Total loss: 0.144472

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000967
    data_loss: 0.003279
    dataset_loss: 0.000007
    encoder_loss: 0.000151

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000214
    phenovision_modality_loss: 0.185169
    bioclip_modality_loss: 0.006043
    alphaearth_modality_loss: 0.089167

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=48
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=72
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0035, -0.0300, -0.0275, -0.0004]
    Predicted: [-0.0136, -0.0151, -0.0451, 0.0154]
    Δ (Delta): [-0.0101, 0.0149, -0.0176, 0.0158]
    MAPE: 1028.30%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.098,  0.055,  0.128,  0.008, -0.023, -0.023,  0.082,  0.107]
    Predicted:
      [ 0.117, -0.028,  0.143, -0.07 , -0.046,  0.038, -0.02 ,  0.078]
    Δ (Delta):
      [ 0.019, -0.083,  0.016, -0.077, -0.023,  0.061, -0.102, -0.029]
    MAPE: 213.47%, RMSE: 0.0601
    🌸 Flowering Probability:
      Original:  0.098
      Predicted: 0.117
      Δ (Delta): 0.019

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.475192

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000654
    Max norm: 0.000769
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003992
    Max norm: 0.367581
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.34s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.6x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00087228, max_change=0.00204852
  earth4d: mean_change=0.10319903, max_change=0.12066685
  multimodal: mean_change=0.00104544, max_change=0.03625669
  perceiver: mean_change=0.00195212, max_change=0.01318585

================================================================================
BATCH 115/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 265 tokens (51.8%)
    BioCLIP: 127 tokens (24.8%)
    Encoder_0: 119 tokens (23.2%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.11328125, 'data': 0.162109375, 'dataset': 0.029296875, 'modality': 0.04296875, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1355

📉 LOSS BREAKDOWN:
  Total loss: 0.135463

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000939
    data_loss: 0.003237
    dataset_loss: 0.000045
    encoder_loss: 0.000101

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000411
    alphaearth_modality_loss: 0.079262
    bioclip_modality_loss: 0.005412
    phenovision_modality_loss: 0.177788

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=58
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=83
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=15
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0097, -0.0151, -0.0378, 0.0201]
    Predicted: [-0.0062, -0.0194, -0.0322, 0.0143]
    Δ (Delta): [0.0035, -0.0042, 0.0055, -0.0058]
    MAPE: 26.99%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 11):
    Original (first 8 dims):
      [ 0.024, -0.038, -0.062,  0.035, -0.049,  0.094, -0.07 , -0.078]
    Predicted:
      [ 0.101, -0.004,  0.118, -0.05 , -0.059,  0.023, -0.011,  0.064]
    Δ (Delta):
      [ 0.077,  0.034,  0.18 , -0.086, -0.009, -0.071,  0.059,  0.142]
    MAPE: 163.32%, RMSE: 0.0972

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.341142

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000479
    Max norm: 0.000584
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002945
    Max norm: 0.268913
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.42s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00082846, max_change=0.00194785
  earth4d: mean_change=0.10389260, max_change=0.12130064
  multimodal: mean_change=0.00103290, max_change=0.03549051
  perceiver: mean_change=0.00178403, max_change=0.01260342

================================================================================
BATCH 116/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 133 tokens (26.0%)
    BioCLIP: 133 tokens (26.0%)
    AlphaEarth: 243 tokens (47.5%)
    Earth4D: 3 tokens (0.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09765625, 'data': 0.15234375, 'dataset': 0.01953125, 'modality': 0.06640625, 'encoder': 0.037109375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1526

📉 LOSS BREAKDOWN:
  Total loss: 0.152627

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000970
    data_loss: 0.002700
    dataset_loss: 0.000050
    encoder_loss: 0.000210

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000315
    bioclip_modality_loss: 0.005252
    alphaearth_modality_loss: 0.090029
    phenovision_modality_loss: 0.202518

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=50
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=78
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=34
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=19

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0115, -0.0073, -0.0363, 0.0208]
    Predicted: [-0.0016, -0.0187, -0.0169, 0.0188]
    Δ (Delta): [0.0099, -0.0114, 0.0194, -0.0020]
    MAPE: 76.18%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 11):
    Original (first 8 dims):
      [ 0.079, -0.06 ,  0.115,  0.012, -0.026, -0.026,  0.031,  0.058]
    Predicted:
      [ 9.210e-02,  1.583e-03,  9.241e-02, -4.370e-02, -5.740e-02,  1.851e-02, -2.187e-05,  5.801e-02]
    Δ (Delta):
      [ 0.013,  0.062, -0.023, -0.055, -0.031,  0.045, -0.031, -0.   ]
    MAPE: 125.76%, RMSE: 0.0379

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.231221

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000585
    Max norm: 0.000693
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002506
    Max norm: 0.153602
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.55s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00069068, max_change=0.00168514
  earth4d: mean_change=0.10436992, max_change=0.12203829
  multimodal: mean_change=0.00103313, max_change=0.03446881
  perceiver: mean_change=0.00161240, max_change=0.01178010

================================================================================
BATCH 117/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 142 tokens (27.7%)
    BioCLIP: 110 tokens (21.5%)
    AlphaEarth: 258 tokens (50.4%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.091796875, 'data': 0.16015625, 'dataset': 0.01953125, 'modality': 0.044921875, 'encoder': 0.041015625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1254

📉 LOSS BREAKDOWN:
  Total loss: 0.125411

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000955
    data_loss: 0.002339
    dataset_loss: 0.000057
    encoder_loss: 0.000171

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000096
    alphaearth_modality_loss: 0.074993
    phenovision_modality_loss: 0.163968
    bioclip_modality_loss: 0.005208

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=47
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=82
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=23
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=21

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0095, -0.0157, -0.0375, 0.0198]
    Predicted: [-0.0057, -0.0261, -0.0105, 0.0272]
    Δ (Delta): [0.0038, -0.0104, 0.0270, 0.0075]
    MAPE: 54.02%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 15):
    Original (first 8 dims):
      [ 0.058, -0.076,  0.08 , -0.119, -0.047,  0.01 , -0.014,  0.102]
    Predicted:
      [ 0.074,  0.004,  0.101, -0.033, -0.034,  0.01 , -0.001,  0.055]
    Δ (Delta):
      [ 0.016,  0.08 ,  0.021,  0.086,  0.012,  0.   ,  0.013, -0.047]
    MAPE: 49.37%, RMSE: 0.0461

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.532608

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000648
    Max norm: 0.000803
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.004144
    Max norm: 0.405443
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.40s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00065355, max_change=0.00152922
  earth4d: mean_change=0.10454989, max_change=0.12228031
  multimodal: mean_change=0.00101046, max_change=0.03456138
  perceiver: mean_change=0.00185192, max_change=0.01229744

================================================================================
BATCH 118/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 241 tokens (47.1%)
    BioCLIP: 127 tokens (24.8%)
    Encoder_0: 143 tokens (27.9%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09375, 'data': 0.16015625, 'dataset': 0.0234375, 'modality': 0.04296875, 'encoder': 0.04296875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1436

📉 LOSS BREAKDOWN:
  Total loss: 0.143622

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000972
    data_loss: 0.001808
    dataset_loss: 0.000011
    encoder_loss: 0.000050

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000158
    bioclip_modality_loss: 0.004989
    phenovision_modality_loss: 0.193148
    alphaearth_modality_loss: 0.083505

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=48
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=82
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=22

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0139, -0.0032, -0.0410, 0.0254]
    Predicted: [-0.0184, -0.0101, -0.0271, 0.0276]
    Δ (Delta): [-0.0045, -0.0069, 0.0140, 0.0022]
    MAPE: 72.13%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 1.016e-01, -2.354e-03,  1.271e-01, -3.311e-02, -3.369e-02,  3.336e-02, -4.911e-05,  6.940e-02]
    Predicted:
      [ 0.082, -0.013,  0.137, -0.037, -0.027,  0.018, -0.019,  0.065]
    Δ (Delta):
      [-0.02 , -0.011,  0.01 , -0.003,  0.007, -0.015, -0.019, -0.005]
    MAPE: 5005.19%, RMSE: 0.0127

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.136063

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000557
    Max norm: 0.000719
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001470
    Max norm: 0.113280
    Zero gradient ratio: 21.48%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.40s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00056962, max_change=0.00149805
  earth4d: mean_change=0.10429418, max_change=0.12183901
  multimodal: mean_change=0.00098850, max_change=0.03547189
  perceiver: mean_change=0.00163870, max_change=0.01150065

================================================================================
BATCH 119/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 127 tokens (24.8%)
    BioCLIP: 141 tokens (27.5%)
    AlphaEarth: 244 tokens (47.7%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.076171875, 'data': 0.16796875, 'dataset': 0.017578125, 'modality': 0.068359375, 'encoder': 0.041015625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1414

📉 LOSS BREAKDOWN:
  Total loss: 0.141436

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000962
    data_loss: 0.001890
    dataset_loss: 0.000098
    encoder_loss: 0.000010

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000186
    alphaearth_modality_loss: 0.079695
    bioclip_modality_loss: 0.004202
    phenovision_modality_loss: 0.193214

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=39
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=86
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=35
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=21

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0088, -0.0169, -0.0360, 0.0179]
    Predicted: [-0.0091, -0.0102, -0.0296, 0.0128]
    Δ (Delta): [-0.0003, 0.0067, 0.0064, -0.0052]
    MAPE: 22.33%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.127,  0.021,  0.13 , -0.048, -0.017,  0.006,  0.011,  0.085]
    Predicted:
      [ 0.078, -0.034,  0.141, -0.047, -0.039,  0.03 , -0.023,  0.065]
    Δ (Delta):
      [-0.049, -0.054,  0.01 ,  0.002, -0.021,  0.024, -0.034, -0.02 ]
    MAPE: 148.38%, RMSE: 0.0316

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.138877

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000538
    Max norm: 0.000614
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001613
    Max norm: 0.087469
    Zero gradient ratio: 23.33%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.40s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00057240, max_change=0.00156080
  earth4d: mean_change=0.10349977, max_change=0.12076653
  multimodal: mean_change=0.00096381, max_change=0.03511508
  perceiver: mean_change=0.00153789, max_change=0.01194854

================================================================================
BATCH 120/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 230 tokens (44.9%)
    BioCLIP: 142 tokens (27.7%)
    Encoder_0: 137 tokens (26.8%)
    Earth4D: 3 tokens (0.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.0859375, 'data': 0.142578125, 'dataset': 0.017578125, 'modality': 0.041015625, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1384

📉 LOSS BREAKDOWN:
  Total loss: 0.138379

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000957
    data_loss: 0.002396
    dataset_loss: 0.000152
    encoder_loss: 0.000132

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000347
    bioclip_modality_loss: 0.004499
    alphaearth_modality_loss: 0.076984
    phenovision_modality_loss: 0.188443

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=44
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=73
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=21
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0111, -0.0116, -0.0390, 0.0217]
    Predicted: [-0.0027, -0.0076, -0.0257, 0.0090]
    Δ (Delta): [0.0085, 0.0040, 0.0134, -0.0128]
    MAPE: 50.85%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 7):
    Original (first 8 dims):
      [ 0.091, -0.012,  0.112, -0.053, -0.051,  0.015,  0.017,  0.054]
    Predicted:
      [ 0.105, -0.006,  0.174, -0.036, -0.047,  0.019, -0.038,  0.078]
    Δ (Delta):
      [ 0.014,  0.005,  0.063,  0.017,  0.004,  0.004, -0.055,  0.024]
    MAPE: 70.57%, RMSE: 0.0318

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.172497

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000881
    Max norm: 0.000943
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001953
    Max norm: 0.126164
    Zero gradient ratio: 22.22%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00071745, max_change=0.00170394
  earth4d: mean_change=0.10330626, max_change=0.12092794
  multimodal: mean_change=0.00097216, max_change=0.03582506
  perceiver: mean_change=0.00210428, max_change=0.01400511

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.000957 (trend: decreasing)
  data_loss: 0.002396 (trend: decreasing)
  dataset_loss: 0.000152 (trend: increasing)
  modality_loss: 0.000347 (trend: increasing)
  encoder_loss: 0.000132 (trend: decreasing)
  phenovision_modality_loss: 0.188443 (trend: decreasing)
  bioclip_modality_loss: 0.004499 (trend: decreasing)
  alphaearth_modality_loss: 0.076984 (trend: decreasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000881, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.001953, zero_ratio=22.22%

================================================================================
BATCH 121/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 262 tokens (51.2%)
    Encoder_0: 126 tokens (24.6%)
    BioCLIP: 123 tokens (24.0%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1015625, 'data': 0.154296875, 'dataset': 0.021484375, 'modality': 0.060546875, 'encoder': 0.0390625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1454

📉 LOSS BREAKDOWN:
  Total loss: 0.145425

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000896
    data_loss: 0.001120
    dataset_loss: 0.000024
    encoder_loss: 0.000263

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000234
    alphaearth_modality_loss: 0.082110
    phenovision_modality_loss: 0.200118
    bioclip_modality_loss: 0.004485

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=52
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=79
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=31
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=20

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0096, -0.0157, -0.0373, 0.0196]
    Predicted: [-0.0004, -0.0063, -0.0432, 0.0165]
    Δ (Delta): [0.0092, 0.0093, -0.0059, -0.0031]
    MAPE: 46.69%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 10):
    Original (first 8 dims):
      [ 0.13 , -0.041,  0.108, -0.049, -0.022,  0.038, -0.006,  0.045]
    Predicted:
      [ 0.095, -0.016,  0.145, -0.038, -0.059,  0.01 , -0.013,  0.039]
    Δ (Delta):
      [-0.035,  0.025,  0.037,  0.011, -0.037, -0.028, -0.006, -0.006]
    MAPE: 62.53%, RMSE: 0.0263

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.219050

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000711
    Max norm: 0.000818
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002262
    Max norm: 0.131125
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.54s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00066034, max_change=0.00150391
  earth4d: mean_change=0.10392992, max_change=0.12193144
  multimodal: mean_change=0.00089242, max_change=0.03403769
  perceiver: mean_change=0.00170441, max_change=0.01370531

================================================================================
BATCH 122/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 247 tokens (48.2%)
    Encoder_0: 146 tokens (28.5%)
    BioCLIP: 119 tokens (23.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09765625, 'data': 0.15234375, 'dataset': 0.01953125, 'modality': 0.064453125, 'encoder': 0.0546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1318

📉 LOSS BREAKDOWN:
  Total loss: 0.131777

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000923
    data_loss: 0.001651
    dataset_loss: 0.000074
    encoder_loss: 0.000243

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000252
    bioclip_modality_loss: 0.004464
    phenovision_modality_loss: 0.176532
    alphaearth_modality_loss: 0.077298

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=50
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=78
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=33
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=28

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 6):
    Original:  [-0.0094, -0.0158, -0.0369, 0.0192]
    Predicted: [-0.0087, -0.0136, -0.0458, 0.0136]
    Δ (Delta): [0.0007, 0.0022, -0.0089, -0.0056]
    MAPE: 18.72%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 1):
    Original (first 8 dims):
      [ 0.107, -0.02 ,  0.133, -0.029, -0.044,  0.02 ,  0.002,  0.096]
    Predicted:
      [ 0.089, -0.003,  0.129, -0.023, -0.053,  0.007, -0.004,  0.059]
    Δ (Delta):
      [-0.019,  0.017, -0.005,  0.006, -0.009, -0.013, -0.007, -0.037]
    MAPE: 67.12%, RMSE: 0.0173

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.124385

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000707
    Max norm: 0.000869
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001411
    Max norm: 0.096885
    Zero gradient ratio: 21.11%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00066906, max_change=0.00167228
  earth4d: mean_change=0.10475009, max_change=0.12296669
  multimodal: mean_change=0.00086752, max_change=0.03331491
  perceiver: mean_change=0.00154950, max_change=0.01453940

================================================================================
BATCH 123/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 135 tokens (26.4%)
    AlphaEarth: 240 tokens (46.9%)
    BioCLIP: 137 tokens (26.8%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.08984375, 'data': 0.138671875, 'dataset': 0.03125, 'modality': 0.05078125, 'encoder': 0.04296875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1651

📉 LOSS BREAKDOWN:
  Total loss: 0.165113

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000897
    data_loss: 0.001365
    dataset_loss: 0.000051
    encoder_loss: 0.000062

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000250
    phenovision_modality_loss: 0.235609
    bioclip_modality_loss: 0.004479
    alphaearth_modality_loss: 0.085542

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=46
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=71
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=16
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=22

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 11):
    Original:  [-0.0089, -0.0168, -0.0361, 0.0181]
    Predicted: [-0.0137, -0.0002, -0.0548, 0.0163]
    Δ (Delta): [-0.0048, 0.0165, -0.0187, -0.0018]
    MAPE: 53.71%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.05 ,  0.013,  0.02 , -0.078, -0.081, -0.011, -0.036,  0.083]
    Predicted:
      [ 0.086, -0.003,  0.129, -0.031, -0.042,  0.009, -0.004,  0.073]
    Δ (Delta):
      [ 0.036, -0.016,  0.109,  0.047,  0.039,  0.02 ,  0.032, -0.011]
    MAPE: 140.62%, RMSE: 0.0483
    🌸 Flowering Probability:
      Original:  0.050
      Predicted: 0.086
      Δ (Delta): 0.036

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.285819

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000587
    Max norm: 0.000703
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002647
    Max norm: 0.195558
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 12.99s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00065122, max_change=0.00169466
  earth4d: mean_change=0.10482869, max_change=0.12323929
  multimodal: mean_change=0.00085453, max_change=0.03328335
  perceiver: mean_change=0.00182106, max_change=0.01369422

================================================================================
BATCH 124/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 140 tokens (27.3%)
    Encoder_0: 129 tokens (25.2%)
    AlphaEarth: 242 tokens (47.3%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.111328125, 'data': 0.171875, 'dataset': 0.01953125, 'modality': 0.0390625, 'encoder': 0.048828125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1574

📉 LOSS BREAKDOWN:
  Total loss: 0.157390

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000903
    data_loss: 0.001588
    dataset_loss: 0.000049
    encoder_loss: 0.000175

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000051
    bioclip_modality_loss: 0.003674
    phenovision_modality_loss: 0.224408
    alphaearth_modality_loss: 0.081661

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=57
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=88
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=20
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=25

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 11):
    Original:  [-0.0100, -0.0144, -0.0379, 0.0203]
    Predicted: [-0.0107, 0.0061, -0.0487, 0.0225]
    Δ (Delta): [-0.0007, 0.0206, -0.0108, 0.0022]
    MAPE: 47.22%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 9):
    Original (first 8 dims):
      [ 0.092, -0.024,  0.123, -0.038, -0.039,  0.033, -0.031,  0.069]
    Predicted:
      [ 0.08 , -0.023,  0.128, -0.041, -0.02 ,  0.016, -0.003,  0.075]
    Δ (Delta):
      [-0.012,  0.001,  0.005, -0.003,  0.019, -0.017,  0.028,  0.006]
    MAPE: 28.54%, RMSE: 0.0142

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.207753

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000535
    Max norm: 0.000662
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002086
    Max norm: 0.144341
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.40s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00081945, max_change=0.00235360
  earth4d: mean_change=0.10531480, max_change=0.12377839
  multimodal: mean_change=0.00085674, max_change=0.03266780
  perceiver: mean_change=0.00163638, max_change=0.01225551

================================================================================
BATCH 125/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 260 tokens (50.8%)
    BioCLIP: 130 tokens (25.4%)
    Encoder_0: 121 tokens (23.6%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.107421875, 'data': 0.13671875, 'dataset': 0.017578125, 'modality': 0.04296875, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1246

📉 LOSS BREAKDOWN:
  Total loss: 0.124598

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000884
    data_loss: 0.001140
    dataset_loss: 0.000034
    encoder_loss: 0.000213

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000089
    bioclip_modality_loss: 0.003647
    phenovision_modality_loss: 0.160564
    alphaearth_modality_loss: 0.080870

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=55
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=70
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0090, -0.0164, -0.0364, 0.0185]
    Predicted: [-0.0094, 0.0069, -0.0452, 0.0228]
    Δ (Delta): [-0.0004, 0.0233, -0.0088, 0.0043]
    MAPE: 48.41%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.087, -0.02 ,  0.153, -0.034, -0.032,  0.003, -0.016,  0.063]
    Predicted:
      [ 0.091, -0.026,  0.127, -0.031, -0.025,  0.014, -0.013,  0.073]
    Δ (Delta):
      [ 0.004, -0.006, -0.026,  0.003,  0.007,  0.01 ,  0.003,  0.011]
    MAPE: 52.57%, RMSE: 0.0113

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.142918

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000507
    Max norm: 0.000618
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001449
    Max norm: 0.121137
    Zero gradient ratio: 21.48%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.52s
  Backward pass: 12.97s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00072745, max_change=0.00215952
  earth4d: mean_change=0.10544194, max_change=0.12370951
  multimodal: mean_change=0.00082463, max_change=0.03179496
  perceiver: mean_change=0.00152863, max_change=0.01135530

================================================================================
BATCH 126/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 131 tokens (25.6%)
    AlphaEarth: 260 tokens (50.8%)
    BioCLIP: 121 tokens (23.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.095703125, 'data': 0.126953125, 'dataset': 0.01953125, 'modality': 0.0390625, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1420

📉 LOSS BREAKDOWN:
  Total loss: 0.141957

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000906
    data_loss: 0.001332
    dataset_loss: 0.000090
    encoder_loss: 0.000049

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000210
    phenovision_modality_loss: 0.195891
    bioclip_modality_loss: 0.003738
    alphaearth_modality_loss: 0.079740

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=49
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=65
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=20
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0130, -0.0090, -0.0434, 0.0259]
    Predicted: [-0.0138, -0.0071, -0.0345, 0.0187]
    Δ (Delta): [-0.0009, 0.0018, 0.0088, -0.0072]
    MAPE: 18.86%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.18 , -0.115,  0.094, -0.089, -0.035, -0.081, -0.001, -0.061]
    Predicted:
      [ 0.094, -0.029,  0.132, -0.029, -0.041,  0.011, -0.007,  0.071]
    Δ (Delta):
      [-0.086,  0.086,  0.038,  0.059, -0.007,  0.093, -0.005,  0.132]
    MAPE: 118.76%, RMSE: 0.0757
    🌸 Flowering Probability:
      Original:  0.180
      Predicted: 0.094
      Δ (Delta): -0.086

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.322185

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000546
    Max norm: 0.000672
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002723
    Max norm: 0.251208
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.35s
  Backward pass: 12.96s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00076395, max_change=0.00204508
  earth4d: mean_change=0.10554426, max_change=0.12378272
  multimodal: mean_change=0.00081421, max_change=0.03162056
  perceiver: mean_change=0.00164733, max_change=0.01191252

================================================================================
BATCH 127/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 116 tokens (22.7%)
    Encoder_0: 147 tokens (28.7%)
    AlphaEarth: 248 tokens (48.4%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.087890625, 'data': 0.146484375, 'dataset': 0.015625, 'modality': 0.04296875, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1474

📉 LOSS BREAKDOWN:
  Total loss: 0.147401

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000998
    data_loss: 0.001030
    dataset_loss: 0.000111
    encoder_loss: 0.000174

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000124
    bioclip_modality_loss: 0.003404
    phenovision_modality_loss: 0.209413
    alphaearth_modality_loss: 0.077849

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=45
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=75
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 15):
    Original:  [-0.0134, -0.0079, -0.0439, 0.0264]
    Predicted: [-0.0097, -0.0230, -0.0190, 0.0153]
    Δ (Delta): [0.0037, -0.0151, 0.0249, -0.0111]
    MAPE: 79.80%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 10):
    Original (first 8 dims):
      [ 0.115, -0.02 ,  0.127, -0.031, -0.042,  0.026, -0.012,  0.048]
    Predicted:
      [ 0.096, -0.014,  0.12 , -0.013, -0.061, -0.001, -0.007,  0.065]
    Δ (Delta):
      [-0.019,  0.006, -0.007,  0.018, -0.019, -0.027,  0.005,  0.016]
    MAPE: 42.09%, RMSE: 0.0163

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.137127

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000615
    Max norm: 0.000741
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001429
    Max norm: 0.110287
    Zero gradient ratio: 21.85%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00073912, max_change=0.00185546
  earth4d: mean_change=0.10581188, max_change=0.12424931
  multimodal: mean_change=0.00077756, max_change=0.03209055
  perceiver: mean_change=0.00153595, max_change=0.01165851

================================================================================
BATCH 128/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 253 tokens (49.4%)
    BioCLIP: 129 tokens (25.2%)
    Encoder_0: 129 tokens (25.2%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.087890625, 'data': 0.173828125, 'dataset': 0.025390625, 'modality': 0.04296875, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1337

📉 LOSS BREAKDOWN:
  Total loss: 0.133668

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000936
    data_loss: 0.000990
    dataset_loss: 0.000117
    encoder_loss: 0.000273

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000082
    alphaearth_modality_loss: 0.077675
    phenovision_modality_loss: 0.182419
    bioclip_modality_loss: 0.003295

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=45
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=89
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=13
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 8):
    Original:  [-0.0089, -0.0164, -0.0359, 0.0179]
    Predicted: [-0.0088, -0.0146, -0.0353, 0.0005]
    Δ (Delta): [0.0001, 0.0018, 0.0007, -0.0174]
    MAPE: 27.71%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 9):
    Original (first 8 dims):
      [ 0.134, -0.013,  0.097,  0.012, -0.019,  0.013, -0.047,  0.069]
    Predicted:
      [ 0.102, -0.008,  0.123, -0.01 , -0.062, -0.012, -0.005,  0.06 ]
    Δ (Delta):
      [-0.032,  0.005,  0.027, -0.022, -0.043, -0.025,  0.042, -0.009]
    MAPE: 100.59%, RMSE: 0.0287

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.191732

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000637
    Max norm: 0.000754
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001840
    Max norm: 0.129060
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.42s
  Backward pass: 13.05s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00072327, max_change=0.00171444
  earth4d: mean_change=0.10559818, max_change=0.12405328
  multimodal: mean_change=0.00075291, max_change=0.03258777
  perceiver: mean_change=0.00169720, max_change=0.01242554

================================================================================
BATCH 129/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 244 tokens (47.7%)
    BioCLIP: 131 tokens (25.6%)
    Encoder_0: 136 tokens (26.6%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.099609375, 'data': 0.125, 'dataset': 0.025390625, 'modality': 0.056640625, 'encoder': 0.041015625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1253

📉 LOSS BREAKDOWN:
  Total loss: 0.125280

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000878
    data_loss: 0.001093
    dataset_loss: 0.000107
    encoder_loss: 0.000053

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000339
    bioclip_modality_loss: 0.003183
    alphaearth_modality_loss: 0.082481
    phenovision_modality_loss: 0.160855

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=51
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=64
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=13
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=29
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=21

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0142, -0.0036, -0.0429, 0.0263]
    Predicted: [0.0036, -0.0158, -0.0254, 0.0089]
    Δ (Delta): [0.0178, -0.0121, 0.0175, -0.0174]
    MAPE: 141.01%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.082, -0.025,  0.13 , -0.021, -0.049,  0.016, -0.028,  0.051]
    Predicted:
      [ 0.089, -0.003,  0.112, -0.023, -0.041, -0.014, -0.   ,  0.057]
    Δ (Delta):
      [ 0.007,  0.022, -0.018, -0.002,  0.009, -0.03 ,  0.028,  0.006]
    MAPE: 55.38%, RMSE: 0.0181

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.276720

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000508
    Max norm: 0.000565
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002410
    Max norm: 0.216201
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.34s
  Backward pass: 12.95s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00059165, max_change=0.00149019
  earth4d: mean_change=0.10582838, max_change=0.12466876
  multimodal: mean_change=0.00074736, max_change=0.03266967
  perceiver: mean_change=0.00152250, max_change=0.01252239

================================================================================
BATCH 130/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 241 tokens (47.1%)
    BioCLIP: 133 tokens (26.0%)
    Encoder_0: 137 tokens (26.8%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.103515625, 'data': 0.12890625, 'dataset': 0.021484375, 'modality': 0.05859375, 'encoder': 0.0546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1461

📉 LOSS BREAKDOWN:
  Total loss: 0.146124

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000842
    data_loss: 0.001100
    dataset_loss: 0.000002
    encoder_loss: 0.000025

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000230
    phenovision_modality_loss: 0.204310
    alphaearth_modality_loss: 0.081009
    bioclip_modality_loss: 0.002994

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=53
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=66
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=30
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=28

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 11):
    Original:  [-0.0107, -0.0127, -0.0393, 0.0216]
    Predicted: [-0.0157, -0.0224, -0.0276, 0.0252]
    Δ (Delta): [-0.0050, -0.0097, 0.0117, 0.0036]
    MAPE: 42.34%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.104, -0.014,  0.138,  0.001, -0.038, -0.026,  0.004,  0.061]
    Predicted:
      [ 0.081, -0.02 ,  0.1  , -0.028, -0.032,  0.006,  0.003,  0.072]
    Δ (Delta):
      [-0.023, -0.006, -0.038, -0.028,  0.006,  0.032, -0.001,  0.01 ]
    MAPE: 629.79%, RMSE: 0.0223
    🌸 Flowering Probability:
      Original:  0.104
      Predicted: 0.081
      Δ (Delta): -0.023

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.141685

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000539
    Max norm: 0.000667
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001580
    Max norm: 0.100411
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.49s
  Backward pass: 12.96s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00074616, max_change=0.00227346
  earth4d: mean_change=0.10635201, max_change=0.12473837
  multimodal: mean_change=0.00072878, max_change=0.03122033
  perceiver: mean_change=0.00142205, max_change=0.01176273

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.000842 (trend: decreasing)
  data_loss: 0.001100 (trend: decreasing)
  dataset_loss: 0.000002 (trend: decreasing)
  modality_loss: 0.000230 (trend: increasing)
  encoder_loss: 0.000025 (trend: decreasing)
  phenovision_modality_loss: 0.204310 (trend: increasing)
  bioclip_modality_loss: 0.002994 (trend: decreasing)
  alphaearth_modality_loss: 0.081009 (trend: increasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000539, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.001580, zero_ratio=20.00%

================================================================================
BATCH 131/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 272 tokens (53.1%)
    Encoder_0: 111 tokens (21.7%)
    BioCLIP: 127 tokens (24.8%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09375, 'data': 0.162109375, 'dataset': 0.02734375, 'modality': 0.03515625, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1353

📉 LOSS BREAKDOWN:
  Total loss: 0.135279

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000856
    data_loss: 0.000867
    dataset_loss: 0.000129
    encoder_loss: 0.000272

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000019
    bioclip_modality_loss: 0.002870
    phenovision_modality_loss: 0.186188
    alphaearth_modality_loss: 0.077970

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=48
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=83
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=14
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=18
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 0):
    Original:  [-0.0137, -0.0031, -0.0407, 0.0248]
    Predicted: [-0.0274, -0.0268, -0.0258, 0.0367]
    Δ (Delta): [-0.0136, -0.0237, 0.0148, 0.0118]
    MAPE: 239.89%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.084, -0.011,  0.124, -0.031, -0.065, -0.005, -0.015,  0.059]
    Predicted:
      [ 0.083, -0.032,  0.112, -0.038, -0.041,  0.012, -0.002,  0.066]
    Δ (Delta):
      [-0.001, -0.021, -0.012, -0.007,  0.024,  0.017,  0.013,  0.007]
    MAPE: 83.99%, RMSE: 0.0146

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.204190

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000661
    Max norm: 0.000800
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001979
    Max norm: 0.138403
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.35s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00070794, max_change=0.00207016
  earth4d: mean_change=0.10672224, max_change=0.12560253
  multimodal: mean_change=0.00072659, max_change=0.03019825
  perceiver: mean_change=0.00155581, max_change=0.01186504

================================================================================
BATCH 132/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 144 tokens (28.1%)
    AlphaEarth: 247 tokens (48.2%)
    Encoder_0: 121 tokens (23.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1015625, 'data': 0.146484375, 'dataset': 0.021484375, 'modality': 0.044921875, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1391

📉 LOSS BREAKDOWN:
  Total loss: 0.139062

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000916
    data_loss: 0.001017
    dataset_loss: 0.000012
    encoder_loss: 0.000019

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000426
    phenovision_modality_loss: 0.193439
    bioclip_modality_loss: 0.002932
    alphaearth_modality_loss: 0.077794

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=52
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=75
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=23
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0110, -0.0142, -0.0417, 0.0243]
    Predicted: [-0.0190, -0.0122, -0.0511, 0.0261]
    Δ (Delta): [-0.0081, 0.0020, -0.0093, 0.0018]
    MAPE: 29.25%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.091, -0.008,  0.104, -0.004, -0.054,  0.002,  0.032,  0.056]
    Predicted:
      [ 0.076, -0.036,  0.132, -0.043, -0.031,  0.014, -0.002,  0.052]
    Δ (Delta):
      [-0.014, -0.028,  0.027, -0.039,  0.023,  0.011, -0.034, -0.004]
    MAPE: 251.85%, RMSE: 0.0252
    🌸 Flowering Probability:
      Original:  0.091
      Predicted: 0.076
      Δ (Delta): -0.014

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.118294

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000633
    Max norm: 0.000897
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001247
    Max norm: 0.092878
    Zero gradient ratio: 21.48%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00068168, max_change=0.00194762
  earth4d: mean_change=0.10670861, max_change=0.12564558
  multimodal: mean_change=0.00072199, max_change=0.02973794
  perceiver: mean_change=0.00146900, max_change=0.01397852

================================================================================
BATCH 133/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 266 tokens (52.0%)
    Encoder_0: 134 tokens (26.2%)
    BioCLIP: 112 tokens (21.9%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.10546875, 'data': 0.14453125, 'dataset': 0.01953125, 'modality': 0.05859375, 'encoder': 0.0546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1474

📉 LOSS BREAKDOWN:
  Total loss: 0.147414

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000883
    data_loss: 0.000775
    dataset_loss: 0.000016
    encoder_loss: 0.000104

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000178
    bioclip_modality_loss: 0.002979
    phenovision_modality_loss: 0.209163
    alphaearth_modality_loss: 0.079311

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=54
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=74
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=30
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=28

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 13):
    Original:  [-0.0108, -0.0121, -0.0388, 0.0214]
    Predicted: [0.0015, -0.0075, -0.0547, 0.0267]
    Δ (Delta): [0.0122, 0.0045, -0.0160, 0.0054]
    MAPE: 54.39%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 6):
    Original (first 8 dims):
      [ 0.096,  0.001,  0.113, -0.019, -0.064, -0.015, -0.03 ,  0.055]
    Predicted:
      [ 0.077, -0.036,  0.111, -0.024, -0.05 ,  0.019, -0.015,  0.031]
    Δ (Delta):
      [-0.019, -0.036, -0.002, -0.005,  0.013,  0.034,  0.015, -0.024]
    MAPE: 691.15%, RMSE: 0.0219

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.268629

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000663
    Max norm: 0.000771
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002385
    Max norm: 0.204396
    Zero gradient ratio: 20.74%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 12.99s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00068384, max_change=0.00189411
  earth4d: mean_change=0.10847093, max_change=0.12776920
  multimodal: mean_change=0.00070900, max_change=0.02952015
  perceiver: mean_change=0.00156939, max_change=0.01577831

================================================================================
BATCH 134/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 134 tokens (26.2%)
    AlphaEarth: 228 tokens (44.5%)
    Encoder_0: 147 tokens (28.7%)
    Earth4D: 3 tokens (0.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.115234375, 'data': 0.14453125, 'dataset': 0.009765625, 'modality': 0.046875, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1519

📉 LOSS BREAKDOWN:
  Total loss: 0.151870

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000896
    data_loss: 0.000678
    dataset_loss: 0.000004
    encoder_loss: 0.000033

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000030
    phenovision_modality_loss: 0.223575
    bioclip_modality_loss: 0.002825
    alphaearth_modality_loss: 0.074180

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=59
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=74
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=5
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=24
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0028, -0.0304, -0.0250, -0.0062]
    Predicted: [0.0062, -0.0045, -0.0394, 0.0195]
    Δ (Delta): [0.0090, 0.0260, -0.0144, 0.0257]
    MAPE: 220.11%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 1):
    Original (first 8 dims):
      [ 0.066, -0.017,  0.126, -0.019, -0.032, -0.022,  0.026,  0.082]
    Predicted:
      [ 0.085, -0.014,  0.131, -0.012, -0.054, -0.005, -0.028,  0.048]
    Δ (Delta):
      [ 0.019,  0.003,  0.005,  0.007, -0.022,  0.017, -0.054, -0.035]
    MAPE: 60.33%, RMSE: 0.0259
    🌸 Flowering Probability:
      Original:  0.066
      Predicted: 0.085
      Δ (Delta): 0.019

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.137331

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000448
    Max norm: 0.000516
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001456
    Max norm: 0.109513
    Zero gradient ratio: 21.48%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.49s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00062546, max_change=0.00171551
  earth4d: mean_change=0.10937891, max_change=0.12898512
  multimodal: mean_change=0.00069555, max_change=0.02964079
  perceiver: mean_change=0.00149726, max_change=0.01420672

================================================================================
BATCH 135/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 255 tokens (49.8%)
    Encoder_0: 123 tokens (24.0%)
    BioCLIP: 133 tokens (26.0%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.109375, 'data': 0.1484375, 'dataset': 0.017578125, 'modality': 0.033203125, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1399

📉 LOSS BREAKDOWN:
  Total loss: 0.139866

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000881
    data_loss: 0.000949
    dataset_loss: 0.000021
    encoder_loss: 0.000018

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000200
    alphaearth_modality_loss: 0.078640
    bioclip_modality_loss: 0.002815
    phenovision_modality_loss: 0.194570

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=56
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=76
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=17
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 10):
    Original:  [-0.0146, -0.0058, -0.0463, 0.0285]
    Predicted: [0.0025, -0.0118, -0.0368, 0.0045]
    Δ (Delta): [0.0171, -0.0060, 0.0095, -0.0240]
    MAPE: 81.58%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.075, -0.025,  0.168, -0.019, -0.015, -0.026,  0.059,  0.136]
    Predicted:
      [ 0.096, -0.012,  0.125, -0.014, -0.05 , -0.006, -0.03 ,  0.074]
    Δ (Delta):
      [ 0.021,  0.013, -0.044,  0.005, -0.035,  0.019, -0.088, -0.063]
    MAPE: 78.62%, RMSE: 0.0445

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.114037

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000700
    Max norm: 0.000906
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001199
    Max norm: 0.084406
    Zero gradient ratio: 20.74%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00060107, max_change=0.00166644
  earth4d: mean_change=0.11046631, max_change=0.13056977
  multimodal: mean_change=0.00069205, max_change=0.02936877
  perceiver: mean_change=0.00136420, max_change=0.01434295

================================================================================
BATCH 136/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 250 tokens (48.8%)
    Encoder_0: 136 tokens (26.6%)
    BioCLIP: 124 tokens (24.2%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.119140625, 'data': 0.1484375, 'dataset': 0.017578125, 'modality': 0.041015625, 'encoder': 0.05859375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1537

📉 LOSS BREAKDOWN:
  Total loss: 0.153676

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000822
    data_loss: 0.000898
    dataset_loss: 0.000072
    encoder_loss: 0.000035

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000079
    alphaearth_modality_loss: 0.083240
    bioclip_modality_loss: 0.002651
    phenovision_modality_loss: 0.217983

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=61
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=76
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=21
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=30

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0092, -0.0159, -0.0370, 0.0191]
    Predicted: [-0.0181, -0.0173, -0.0347, 0.0099]
    Δ (Delta): [-0.0089, -0.0014, 0.0023, -0.0092]
    MAPE: 40.00%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.109, -0.06 ,  0.094, -0.03 , -0.029,  0.019,  0.022,  0.05 ]
    Predicted:
      [ 0.088, -0.028,  0.093, -0.018, -0.049,  0.007, -0.008,  0.073]
    Δ (Delta):
      [-0.021,  0.032, -0.001,  0.012, -0.02 , -0.012, -0.031,  0.023]
    MAPE: 53.36%, RMSE: 0.0213

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.328230

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000433
    Max norm: 0.000500
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002625
    Max norm: 0.257499
    Zero gradient ratio: 18.52%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.46s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.3x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00058908, max_change=0.00149841
  earth4d: mean_change=0.11103883, max_change=0.13088068
  multimodal: mean_change=0.00067410, max_change=0.02866106
  perceiver: mean_change=0.00137262, max_change=0.01327670

================================================================================
BATCH 137/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 124 tokens (24.2%)
    AlphaEarth: 246 tokens (48.0%)
    BioCLIP: 141 tokens (27.5%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.103515625, 'data': 0.140625, 'dataset': 0.0234375, 'modality': 0.037109375, 'encoder': 0.0703125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1461

📉 LOSS BREAKDOWN:
  Total loss: 0.146130

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000808
    data_loss: 0.000678
    dataset_loss: 0.000014
    encoder_loss: 0.000028

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000115
    bioclip_modality_loss: 0.002547
    phenovision_modality_loss: 0.197872
    alphaearth_modality_loss: 0.088837

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=53
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=72
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=19
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=36

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 5):
    Original:  [-0.0092, -0.0159, -0.0370, 0.0192]
    Predicted: [-0.0317, -0.0294, -0.0321, 0.0143]
    Δ (Delta): [-0.0225, -0.0135, 0.0049, -0.0049]
    MAPE: 92.01%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.091, -0.014,  0.121, -0.026, -0.068, -0.006,  0.009,  0.051]
    Predicted:
      [ 0.088, -0.034,  0.095, -0.03 , -0.038,  0.012,  0.02 ,  0.07 ]
    Δ (Delta):
      [-0.003, -0.02 , -0.027, -0.004,  0.03 ,  0.018,  0.011,  0.02 ]
    MAPE: 88.63%, RMSE: 0.0189

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.166148

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000464
    Max norm: 0.000592
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001516
    Max norm: 0.151781
    Zero gradient ratio: 22.22%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00059332, max_change=0.00135913
  earth4d: mean_change=0.11108581, max_change=0.13097748
  multimodal: mean_change=0.00066416, max_change=0.02803634
  perceiver: mean_change=0.00149695, max_change=0.01255776

================================================================================
BATCH 138/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 142 tokens (27.7%)
    AlphaEarth: 240 tokens (46.9%)
    BioCLIP: 129 tokens (25.2%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.099609375, 'data': 0.158203125, 'dataset': 0.03515625, 'modality': 0.056640625, 'encoder': 0.048828125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1625

📉 LOSS BREAKDOWN:
  Total loss: 0.162464

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000832
    data_loss: 0.000719
    dataset_loss: 0.000135
    encoder_loss: 0.000116

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000142
    bioclip_modality_loss: 0.002560
    phenovision_modality_loss: 0.235924
    alphaearth_modality_loss: 0.083265

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=51
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=81
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=18
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=29
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=25

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 7):
    Original:  [-0.0142, -0.0038, -0.0429, 0.0265]
    Predicted: [-0.0312, -0.0201, -0.0442, 0.0209]
    Δ (Delta): [-0.0171, -0.0163, -0.0013, -0.0056]
    MAPE: 144.09%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [ 0.072,  0.003,  0.112, -0.036, -0.043,  0.015,  0.002,  0.038]
    Predicted:
      [ 0.075, -0.041,  0.102, -0.029, -0.043,  0.005,  0.023,  0.048]
    Δ (Delta):
      [ 0.003, -0.044, -0.01 ,  0.007,  0.   , -0.01 ,  0.021,  0.01 ]
    MAPE: 334.62%, RMSE: 0.0185

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.285607

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000565
    Max norm: 0.000693
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002465
    Max norm: 0.213173
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.53s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00058015, max_change=0.00126950
  earth4d: mean_change=0.11102892, max_change=0.13077922
  multimodal: mean_change=0.00067105, max_change=0.02835760
  perceiver: mean_change=0.00152139, max_change=0.01327153

================================================================================
BATCH 139/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 257 tokens (50.2%)
    Encoder_0: 125 tokens (24.4%)
    BioCLIP: 129 tokens (25.2%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.107421875, 'data': 0.162109375, 'dataset': 0.015625, 'modality': 0.060546875, 'encoder': 0.04296875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1180

📉 LOSS BREAKDOWN:
  Total loss: 0.117971

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000847
    data_loss: 0.000805
    dataset_loss: 0.000099
    encoder_loss: 0.000023

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000079
    bioclip_modality_loss: 0.002403
    alphaearth_modality_loss: 0.074974
    phenovision_modality_loss: 0.155223

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=55
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=83
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=31
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=22

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 8):
    Original:  [-0.0138, -0.0032, -0.0409, 0.0253]
    Predicted: [-0.0223, -0.0130, -0.0365, 0.0184]
    Δ (Delta): [-0.0085, -0.0098, 0.0044, -0.0069]
    MAPE: 100.21%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 7):
    Original (first 8 dims):
      [ 0.093, -0.022,  0.129, -0.028, -0.05 ,  0.018, -0.003,  0.049]
    Predicted:
      [ 0.073, -0.029,  0.122, -0.017, -0.042, -0.003,  0.011,  0.035]
    Δ (Delta):
      [-0.02 , -0.006, -0.006,  0.011,  0.008, -0.021,  0.014, -0.014]
    MAPE: 83.80%, RMSE: 0.0138

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.359993

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000502
    Max norm: 0.000531
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002844
    Max norm: 0.268385
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.03s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00052425, max_change=0.00118834
  earth4d: mean_change=0.11158717, max_change=0.13115294
  multimodal: mean_change=0.00065964, max_change=0.02830974
  perceiver: mean_change=0.00134817, max_change=0.01249600

================================================================================
BATCH 140/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 248 tokens (48.4%)
    Encoder_0: 136 tokens (26.6%)
    BioCLIP: 128 tokens (25.0%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.12890625, 'data': 0.146484375, 'dataset': 0.009765625, 'modality': 0.056640625, 'encoder': 0.048828125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1214

📉 LOSS BREAKDOWN:
  Total loss: 0.121430

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000873
    data_loss: 0.000752
    dataset_loss: 0.000031
    encoder_loss: 0.000186

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000382
    alphaearth_modality_loss: 0.085843
    bioclip_modality_loss: 0.002187
    phenovision_modality_loss: 0.151460

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=66
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=75
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=5
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=29
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=25

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0033, -0.0305, -0.0281, 0.0000]
    Predicted: [-0.0049, -0.0034, -0.0389, 0.0132]
    Δ (Delta): [-0.0016, 0.0271, -0.0108, 0.0132]
    MAPE: 6838.67%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [ 0.081, -0.055,  0.151, -0.07 , -0.025, -0.045, -0.012, -0.01 ]
    Predicted:
      [ 0.071, -0.031,  0.116, -0.018, -0.04 ,  0.001, -0.005,  0.039]
    Δ (Delta):
      [-0.011,  0.025, -0.036,  0.052, -0.014,  0.046,  0.007,  0.049]
    MAPE: 106.68%, RMSE: 0.0343

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.225004

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000485
    Max norm: 0.000531
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002235
    Max norm: 0.143799
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00046255, max_change=0.00108928
  earth4d: mean_change=0.11311556, max_change=0.13291800
  multimodal: mean_change=0.00071420, max_change=0.02732089
  perceiver: mean_change=0.00131547, max_change=0.01220309

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.000873 (trend: increasing)
  data_loss: 0.000752 (trend: decreasing)
  dataset_loss: 0.000031 (trend: decreasing)
  modality_loss: 0.000382 (trend: increasing)
  encoder_loss: 0.000186 (trend: increasing)
  phenovision_modality_loss: 0.151460 (trend: decreasing)
  bioclip_modality_loss: 0.002187 (trend: decreasing)
  alphaearth_modality_loss: 0.085843 (trend: increasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000485, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.002235, zero_ratio=20.00%

================================================================================
BATCH 141/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 251 tokens (49.0%)
    BioCLIP: 131 tokens (25.6%)
    Encoder_0: 129 tokens (25.2%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1171875, 'data': 0.15234375, 'dataset': 0.025390625, 'modality': 0.04296875, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1454

📉 LOSS BREAKDOWN:
  Total loss: 0.145379

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000833
    data_loss: 0.000745
    dataset_loss: 0.000065
    encoder_loss: 0.000112

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000077
    phenovision_modality_loss: 0.204875
    alphaearth_modality_loss: 0.080320
    bioclip_modality_loss: 0.002355

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=60
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=78
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=13
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 2, seq 6):
    Original:  [-0.0101, -0.0144, -0.0382, 0.0206]
    Predicted: [-0.0021, -0.0095, -0.0308, 0.0114]
    Δ (Delta): [0.0080, 0.0049, 0.0074, -0.0092]
    MAPE: 44.28%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.108, -0.042,  0.084, -0.013, -0.04 , -0.011, -0.029,  0.026]
    Predicted:
      [ 0.079, -0.036,  0.121, -0.025, -0.038,  0.002, -0.019,  0.06 ]
    Δ (Delta):
      [-0.029,  0.005,  0.037, -0.011,  0.003,  0.013,  0.009,  0.034]
    MAPE: 56.71%, RMSE: 0.0217
    🌸 Flowering Probability:
      Original:  0.108
      Predicted: 0.079
      Δ (Delta): -0.029

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.144281

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000554
    Max norm: 0.000606
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001464
    Max norm: 0.116604
    Zero gradient ratio: 21.11%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00048893, max_change=0.00099882
  earth4d: mean_change=0.11355287, max_change=0.13321190
  multimodal: mean_change=0.00068650, max_change=0.02629163
  perceiver: mean_change=0.00150093, max_change=0.01264565

================================================================================
BATCH 142/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 254 tokens (49.6%)
    BioCLIP: 120 tokens (23.4%)
    Encoder_0: 138 tokens (27.0%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.099609375, 'data': 0.166015625, 'dataset': 0.02734375, 'modality': 0.072265625, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1427

📉 LOSS BREAKDOWN:
  Total loss: 0.142739

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000805
    data_loss: 0.000664
    dataset_loss: 0.000033
    encoder_loss: 0.000043

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000132
    alphaearth_modality_loss: 0.084935
    bioclip_modality_loss: 0.002142
    phenovision_modality_loss: 0.195419

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=51
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=85
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=14
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=37
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0143, -0.0038, -0.0429, 0.0264]
    Predicted: [-0.0028, -0.0081, -0.0440, 0.0293]
    Δ (Delta): [0.0115, -0.0043, -0.0011, 0.0029]
    MAPE: 52.06%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 10):
    Original (first 8 dims):
      [ 0.093, -0.045,  0.138, -0.041, -0.011, -0.005,  0.022,  0.045]
    Predicted:
      [ 0.079, -0.042,  0.103, -0.022, -0.053, -0.004, -0.023,  0.061]
    Δ (Delta):
      [-0.014,  0.004, -0.035,  0.019, -0.042,  0.002, -0.045,  0.016]
    MAPE: 94.40%, RMSE: 0.0269

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.106539

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000478
    Max norm: 0.000525
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001173
    Max norm: 0.079314
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.40s
  Backward pass: 13.03s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00044652, max_change=0.00099333
  earth4d: mean_change=0.11283648, max_change=0.13275562
  multimodal: mean_change=0.00069688, max_change=0.02602723
  perceiver: mean_change=0.00137331, max_change=0.01168096

================================================================================
BATCH 143/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 248 tokens (48.4%)
    Encoder_0: 126 tokens (24.6%)
    BioCLIP: 137 tokens (26.8%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.109375, 'data': 0.146484375, 'dataset': 0.021484375, 'modality': 0.03515625, 'encoder': 0.06640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1234

📉 LOSS BREAKDOWN:
  Total loss: 0.123406

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000808
    data_loss: 0.000621
    dataset_loss: 0.000072
    encoder_loss: 0.000043

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000591
    alphaearth_modality_loss: 0.073075
    bioclip_modality_loss: 0.002044
    phenovision_modality_loss: 0.168694

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=56
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=75
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=18
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=34

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0101, -0.0139, -0.0379, 0.0202]
    Predicted: [0.0003, -0.0223, -0.0430, 0.0346]
    Δ (Delta): [0.0104, -0.0084, -0.0052, 0.0144]
    MAPE: 62.06%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.07 , -0.026,  0.113, -0.032, -0.055, -0.029,  0.005,  0.026]
    Predicted:
      [ 0.072, -0.029,  0.109, -0.011, -0.048, -0.026, -0.026,  0.061]
    Δ (Delta):
      [ 0.002, -0.002, -0.004,  0.021,  0.007,  0.003, -0.031,  0.035]
    MAPE: 112.49%, RMSE: 0.0183

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.131103

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000563
    Max norm: 0.000659
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001416
    Max norm: 0.092577
    Zero gradient ratio: 21.11%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.52s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00059653, max_change=0.00155340
  earth4d: mean_change=0.11181599, max_change=0.13156986
  multimodal: mean_change=0.00061372, max_change=0.02605162
  perceiver: mean_change=0.00141916, max_change=0.01398950

================================================================================
BATCH 144/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 246 tokens (48.0%)
    BioCLIP: 129 tokens (25.2%)
    Encoder_0: 137 tokens (26.8%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.11328125, 'data': 0.134765625, 'dataset': 0.015625, 'modality': 0.060546875, 'encoder': 0.0390625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1417

📉 LOSS BREAKDOWN:
  Total loss: 0.141739

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000830
    data_loss: 0.000649
    dataset_loss: 0.000104
    encoder_loss: 0.000160

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000016
    phenovision_modality_loss: 0.198094
    bioclip_modality_loss: 0.001975
    alphaearth_modality_loss: 0.080393

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=58
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=69
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=31
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=20

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0143, -0.0040, -0.0430, 0.0264]
    Predicted: [-0.0103, -0.0157, -0.0420, 0.0158]
    Δ (Delta): [0.0040, -0.0116, 0.0011, -0.0107]
    MAPE: 90.19%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 1, seq 0):
    Original (first 8 dims):
      [ 8.618e-02, -4.880e-02,  1.001e-01, -1.379e-02, -3.023e-02, -3.033e-02,  2.742e-06,  5.118e-02]
    Predicted:
      [ 0.087, -0.03 ,  0.087, -0.006, -0.058, -0.012, -0.02 ,  0.06 ]
    Δ (Delta):
      [ 0.001,  0.019, -0.013,  0.008, -0.028,  0.018, -0.02 ,  0.009]
    MAPE: 92441.14%, RMSE: 0.0165
    🌸 Flowering Probability:
      Original:  0.086
      Predicted: 0.087
      Δ (Delta): 0.001

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.101730

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000397
    Max norm: 0.000478
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001097
    Max norm: 0.079124
    Zero gradient ratio: 21.85%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.33s
  Backward pass: 12.96s
  Ratio (back/fwd): 5.6x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00056851, max_change=0.00146480
  earth4d: mean_change=0.11191880, max_change=0.13133042
  multimodal: mean_change=0.00057731, max_change=0.02545729
  perceiver: mean_change=0.00138837, max_change=0.01293368

================================================================================
BATCH 145/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 252 tokens (49.2%)
    Encoder_0: 125 tokens (24.4%)
    BioCLIP: 135 tokens (26.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.083984375, 'data': 0.142578125, 'dataset': 0.01953125, 'modality': 0.0546875, 'encoder': 0.037109375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1455

📉 LOSS BREAKDOWN:
  Total loss: 0.145494

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000848
    data_loss: 0.000607
    dataset_loss: 0.000013
    encoder_loss: 0.000050

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000175
    bioclip_modality_loss: 0.002110
    alphaearth_modality_loss: 0.078480
    phenovision_modality_loss: 0.207438

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=43
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=73
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=28
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=19

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 2):
    Original:  [-0.0035, -0.0287, -0.0264, -0.0010]
    Predicted: [-0.0095, -0.0088, -0.0394, 0.0037]
    Δ (Delta): [-0.0060, 0.0199, -0.0130, 0.0047]
    MAPE: 192.23%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.096, -0.032,  0.122, -0.013, -0.06 , -0.001, -0.017,  0.064]
    Predicted:
      [ 0.089, -0.026,  0.099, -0.015, -0.061, -0.013, -0.006,  0.054]
    Δ (Delta):
      [-0.007,  0.006, -0.023, -0.001, -0.001, -0.011,  0.011, -0.011]
    MAPE: 128.54%, RMSE: 0.0110

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.120345

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000540
    Max norm: 0.000642
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001303
    Max norm: 0.087203
    Zero gradient ratio: 21.48%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.35s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00052017, max_change=0.00131002
  earth4d: mean_change=0.11100518, max_change=0.13020809
  multimodal: mean_change=0.00056287, max_change=0.02482234
  perceiver: mean_change=0.00125901, max_change=0.01235627

================================================================================
BATCH 146/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 119 tokens (23.2%)
    Encoder_0: 133 tokens (26.0%)
    AlphaEarth: 260 tokens (50.8%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1015625, 'data': 0.150390625, 'dataset': 0.013671875, 'modality': 0.0546875, 'encoder': 0.060546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1084

📉 LOSS BREAKDOWN:
  Total loss: 0.108425

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000833
    data_loss: 0.000632
    dataset_loss: 0.000061
    encoder_loss: 0.000040

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000502
    bioclip_modality_loss: 0.002052
    phenovision_modality_loss: 0.134615
    alphaearth_modality_loss: 0.077131

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=52
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=77
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=7
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=28
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=31

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0144, -0.0036, -0.0432, 0.0266]
    Predicted: [-0.0072, -0.0074, -0.0220, -0.0048]
    Δ (Delta): [0.0073, -0.0038, 0.0212, -0.0314]
    MAPE: 80.93%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.085, -0.016,  0.107, -0.017, -0.024, -0.   , -0.006,  0.061]
    Predicted:
      [ 0.089, -0.034,  0.114, -0.027, -0.038, -0.006,  0.007,  0.052]
    Δ (Delta):
      [ 0.004, -0.018,  0.007, -0.011, -0.014, -0.006,  0.013, -0.009]
    MAPE: 294.47%, RMSE: 0.0112

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.349371

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000665
    Max norm: 0.000745
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002817
    Max norm: 0.254192
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00050032, max_change=0.00117195
  earth4d: mean_change=0.11083495, max_change=0.13016431
  multimodal: mean_change=0.00056113, max_change=0.02457856
  perceiver: mean_change=0.00144354, max_change=0.01263560

================================================================================
BATCH 147/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 253 tokens (49.4%)
    Encoder_0: 133 tokens (26.0%)
    BioCLIP: 125 tokens (24.4%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.10546875, 'data': 0.119140625, 'dataset': 0.015625, 'modality': 0.046875, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1539

📉 LOSS BREAKDOWN:
  Total loss: 0.153850

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000776
    data_loss: 0.000542
    dataset_loss: 0.000025
    encoder_loss: 0.000028

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000056
    bioclip_modality_loss: 0.001952
    alphaearth_modality_loss: 0.080259
    phenovision_modality_loss: 0.222831

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=54
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=61
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=8
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=24
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 6):
    Original:  [-0.0076, -0.0203, -0.0354, 0.0170]
    Predicted: [-0.0005, -0.0149, -0.0237, 0.0203]
    Δ (Delta): [0.0071, 0.0054, 0.0117, 0.0033]
    MAPE: 43.09%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 11):
    Original (first 8 dims):
      [ 0.082, -0.019,  0.086, -0.011, -0.054, -0.023, -0.041,  0.06 ]
    Predicted:
      [ 0.081, -0.039,  0.105, -0.028, -0.044,  0.   , -0.001,  0.052]
    Δ (Delta):
      [-0.001, -0.02 ,  0.019, -0.017,  0.01 ,  0.023,  0.04 , -0.008]
    MAPE: 64.27%, RMSE: 0.0205

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.113615

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000448
    Max norm: 0.000561
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001099
    Max norm: 0.090512
    Zero gradient ratio: 20.37%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.48s
  Backward pass: 12.95s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00049005, max_change=0.00106582
  earth4d: mean_change=0.11062336, max_change=0.12956461
  multimodal: mean_change=0.00057356, max_change=0.02459583
  perceiver: mean_change=0.00147946, max_change=0.01259552

================================================================================
BATCH 148/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 243 tokens (47.5%)
    BioCLIP: 140 tokens (27.3%)
    Encoder_0: 129 tokens (25.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.0859375, 'data': 0.1484375, 'dataset': 0.0234375, 'modality': 0.04296875, 'encoder': 0.068359375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1170

📉 LOSS BREAKDOWN:
  Total loss: 0.116956

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000845
    data_loss: 0.000599
    dataset_loss: 0.000077
    encoder_loss: 0.000037

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000204
    bioclip_modality_loss: 0.001830
    alphaearth_modality_loss: 0.084219
    phenovision_modality_loss: 0.144912

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=44
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=76
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=35

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 2, seq 4):
    Original:  [-0.0088, -0.0169, -0.0362, 0.0181]
    Predicted: [-0.0037, -0.0231, -0.0287, 0.0333]
    Δ (Delta): [0.0051, -0.0062, 0.0075, 0.0152]
    MAPE: 49.91%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.062, -0.026,  0.109, -0.013, -0.052, -0.022, -0.033,  0.047]
    Predicted:
      [ 0.084, -0.048,  0.096, -0.028, -0.052,  0.015, -0.011,  0.054]
    Δ (Delta):
      [ 0.022, -0.022, -0.013, -0.016,  0.   ,  0.037,  0.022,  0.007]
    MAPE: 62.84%, RMSE: 0.0201

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.135728

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000604
    Max norm: 0.000663
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001391
    Max norm: 0.086719
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00041171, max_change=0.00096082
  earth4d: mean_change=0.11074155, max_change=0.12998323
  multimodal: mean_change=0.00054042, max_change=0.02394603
  perceiver: mean_change=0.00119354, max_change=0.01203997

================================================================================
BATCH 149/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 136 tokens (26.6%)
    AlphaEarth: 252 tokens (49.2%)
    BioCLIP: 124 tokens (24.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.111328125, 'data': 0.1328125, 'dataset': 0.021484375, 'modality': 0.03515625, 'encoder': 0.072265625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1513

📉 LOSS BREAKDOWN:
  Total loss: 0.151321

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000814
    data_loss: 0.000468
    dataset_loss: 0.000069
    encoder_loss: 0.000012

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000211
    bioclip_modality_loss: 0.001854
    alphaearth_modality_loss: 0.075832
    phenovision_modality_loss: 0.222335

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=57
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=68
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=18
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=37

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 15):
    Original:  [-0.0119, -0.0055, -0.0360, 0.0212]
    Predicted: [-0.0124, -0.0196, -0.0359, 0.0371]
    Δ (Delta): [-0.0005, -0.0141, 0.0001, 0.0159]
    MAPE: 84.61%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.089, -0.025,  0.101, -0.007, -0.024, -0.006, -0.002,  0.046]
    Predicted:
      [ 0.082, -0.028,  0.102, -0.019, -0.042,  0.003, -0.01 ,  0.066]
    Δ (Delta):
      [-0.007, -0.003,  0.002, -0.012, -0.019,  0.009, -0.008,  0.02 ]
    MAPE: 113.15%, RMSE: 0.0117

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.113561

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000516
    Max norm: 0.000617
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001146
    Max norm: 0.094796
    Zero gradient ratio: 21.48%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.35s
  Backward pass: 12.97s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00041918, max_change=0.00089611
  earth4d: mean_change=0.11129175, max_change=0.13093878
  multimodal: mean_change=0.00055638, max_change=0.02354661
  perceiver: mean_change=0.00138068, max_change=0.01152595

================================================================================
BATCH 150/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 137 tokens (26.8%)
    AlphaEarth: 231 tokens (45.1%)
    Encoder_0: 139 tokens (27.1%)
    Earth4D: 5 tokens (1.0%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.115234375, 'data': 0.12890625, 'dataset': 0.013671875, 'modality': 0.06640625, 'encoder': 0.0546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1522

📉 LOSS BREAKDOWN:
  Total loss: 0.152173

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000795
    data_loss: 0.000554
    dataset_loss: 0.000075
    encoder_loss: 0.000069

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000044
    phenovision_modality_loss: 0.221583
    bioclip_modality_loss: 0.001774
    alphaearth_modality_loss: 0.078253

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=59
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=66
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=7
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=34
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=28

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0108, -0.0123, -0.0390, 0.0216]
    Predicted: [-0.0166, -0.0132, -0.0329, 0.0195]
    Δ (Delta): [-0.0058, -0.0009, 0.0061, -0.0021]
    MAPE: 21.56%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 14):
    Original (first 8 dims):
      [ 0.073, -0.02 ,  0.11 , -0.026, -0.084, -0.016, -0.034,  0.081]
    Predicted:
      [ 0.086, -0.025,  0.095, -0.014, -0.042,  0.002, -0.005,  0.057]
    Δ (Delta):
      [ 0.013, -0.005, -0.015,  0.012,  0.042,  0.018,  0.028, -0.025]
    MAPE: 46.84%, RMSE: 0.0224
    🌸 Flowering Probability:
      Original:  0.073
      Predicted: 0.086
      Δ (Delta): 0.013

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.284783

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000472
    Max norm: 0.000529
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002325
    Max norm: 0.205832
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.33s
  Backward pass: 12.95s
  Ratio (back/fwd): 5.6x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00037218, max_change=0.00081981
  earth4d: mean_change=0.11290892, max_change=0.13264768
  multimodal: mean_change=0.00053100, max_change=0.02336277
  perceiver: mean_change=0.00125070, max_change=0.01108150

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.000795 (trend: decreasing)
  data_loss: 0.000554 (trend: decreasing)
  dataset_loss: 0.000075 (trend: increasing)
  modality_loss: 0.000044 (trend: decreasing)
  encoder_loss: 0.000069 (trend: increasing)
  phenovision_modality_loss: 0.221583 (trend: increasing)
  bioclip_modality_loss: 0.001774 (trend: decreasing)
  alphaearth_modality_loss: 0.078253 (trend: increasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000472, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.002325, zero_ratio=20.00%

================================================================================
BATCH 151/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 264 tokens (51.6%)
    BioCLIP: 123 tokens (24.0%)
    Encoder_0: 125 tokens (24.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.10546875, 'data': 0.14453125, 'dataset': 0.021484375, 'modality': 0.05859375, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1330

📉 LOSS BREAKDOWN:
  Total loss: 0.132950

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000859
    data_loss: 0.000513
    dataset_loss: 0.000147
    encoder_loss: 0.000117

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000066
    bioclip_modality_loss: 0.001872
    phenovision_modality_loss: 0.185634
    alphaearth_modality_loss: 0.075586

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=54
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=74
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=30
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 13):
    Original:  [-0.0111, -0.0133, -0.0410, 0.0234]
    Predicted: [-0.0150, -0.0087, -0.0461, 0.0088]
    Δ (Delta): [-0.0039, 0.0046, -0.0051, -0.0147]
    MAPE: 36.23%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.081, -0.033,  0.087, -0.012, -0.035, -0.027, -0.029,  0.051]
    Predicted:
      [ 0.085, -0.017,  0.115, -0.002, -0.04 , -0.017, -0.006,  0.054]
    Δ (Delta):
      [ 0.004,  0.017,  0.028,  0.01 , -0.004,  0.01 ,  0.023,  0.003]
    MAPE: 37.98%, RMSE: 0.0151

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.127209

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000583
    Max norm: 0.000665
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001155
    Max norm: 0.109531
    Zero gradient ratio: 21.48%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00038540, max_change=0.00085815
  earth4d: mean_change=0.11344654, max_change=0.13329712
  multimodal: mean_change=0.00053817, max_change=0.02335590
  perceiver: mean_change=0.00127571, max_change=0.01188076

================================================================================
BATCH 152/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 238 tokens (46.5%)
    Encoder_0: 137 tokens (26.8%)
    BioCLIP: 135 tokens (26.4%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.115234375, 'data': 0.166015625, 'dataset': 0.03515625, 'modality': 0.044921875, 'encoder': 0.068359375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1356

📉 LOSS BREAKDOWN:
  Total loss: 0.135562

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000813
    data_loss: 0.000654
    dataset_loss: 0.000002
    encoder_loss: 0.000093

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000008
    bioclip_modality_loss: 0.001598
    phenovision_modality_loss: 0.187277
    alphaearth_modality_loss: 0.079295

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=59
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=85
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=18
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=23
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=35

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 10):
    Original:  [-0.0141, -0.0041, -0.0428, 0.0263]
    Predicted: [-0.0021, -0.0024, -0.0491, -0.0020]
    Δ (Delta): [0.0120, 0.0017, -0.0063, -0.0283]
    MAPE: 62.00%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.08 , -0.035,  0.118, -0.031, -0.045, -0.016, -0.024,  0.047]
    Predicted:
      [ 0.076, -0.035,  0.118, -0.014, -0.05 , -0.019,  0.005,  0.041]
    Δ (Delta):
      [-0.004, -0.001,  0.   ,  0.017, -0.006, -0.002,  0.029, -0.006]
    MAPE: 27.68%, RMSE: 0.0123

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.180404

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000744
    Max norm: 0.000796
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001691
    Max norm: 0.142832
    Zero gradient ratio: 20.74%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.54s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00040520, max_change=0.00107230
  earth4d: mean_change=0.11398368, max_change=0.13435696
  multimodal: mean_change=0.00053559, max_change=0.02287964
  perceiver: mean_change=0.00138174, max_change=0.01315342

================================================================================
BATCH 153/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 259 tokens (50.6%)
    Encoder_0: 120 tokens (23.4%)
    BioCLIP: 133 tokens (26.0%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.078125, 'data': 0.1484375, 'dataset': 0.017578125, 'modality': 0.06640625, 'encoder': 0.02734375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1566

📉 LOSS BREAKDOWN:
  Total loss: 0.156566

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000761
    data_loss: 0.000597
    dataset_loss: 0.000150
    encoder_loss: 0.000188

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000028
    phenovision_modality_loss: 0.224701
    bioclip_modality_loss: 0.001816
    alphaearth_modality_loss: 0.083826

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=40
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=76
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=34
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=14

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 5):
    Original:  [-0.0096, -0.0154, -0.0376, 0.0199]
    Predicted: [-0.0070, -0.0141, -0.0262, 0.0171]
    Δ (Delta): [0.0026, 0.0013, 0.0114, -0.0028]
    MAPE: 20.01%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.106, -0.067,  0.052, -0.027,  0.054, -0.045,  0.007, -0.   ]
    Predicted:
      [ 0.077, -0.03 ,  0.114, -0.009, -0.056, -0.009, -0.004,  0.07 ]
    Δ (Delta):
      [-0.029,  0.037,  0.062,  0.018, -0.11 ,  0.036, -0.01 ,  0.071]
    MAPE: 2487.80%, RMSE: 0.0557
    🌸 Flowering Probability:
      Original:  0.106
      Predicted: 0.077
      Δ (Delta): -0.029

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.278791

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000644
    Max norm: 0.000772
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002412
    Max norm: 0.208841
    Zero gradient ratio: 21.11%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00037750, max_change=0.00102486
  earth4d: mean_change=0.11257609, max_change=0.13254300
  multimodal: mean_change=0.00053731, max_change=0.02273208
  perceiver: mean_change=0.00139978, max_change=0.01295853

================================================================================
BATCH 154/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 278 tokens (54.3%)
    BioCLIP: 132 tokens (25.8%)
    Encoder_0: 101 tokens (19.7%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.091796875, 'data': 0.138671875, 'dataset': 0.033203125, 'modality': 0.048828125, 'encoder': 0.052734375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1175

📉 LOSS BREAKDOWN:
  Total loss: 0.117518

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000819
    data_loss: 0.000464
    dataset_loss: 0.000156
    encoder_loss: 0.000034

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000091
    phenovision_modality_loss: 0.155909
    bioclip_modality_loss: 0.001601
    alphaearth_modality_loss: 0.074903

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=47
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=71
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=17
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=25
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=27

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 10):
    Original:  [-0.0141, -0.0041, -0.0427, 0.0261]
    Predicted: [-0.0085, -0.0151, -0.0316, 0.0289]
    Δ (Delta): [0.0056, -0.0110, 0.0111, 0.0028]
    MAPE: 85.23%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [ 0.102, -0.052,  0.109,  0.025, -0.005,  0.027, -0.048,  0.077]
    Predicted:
      [ 0.077, -0.043,  0.117, -0.008, -0.043, -0.013, -0.015,  0.068]
    Δ (Delta):
      [-0.026,  0.01 ,  0.008, -0.033, -0.039, -0.04 ,  0.034, -0.009]
    MAPE: 156.88%, RMSE: 0.0278
    🌸 Flowering Probability:
      Original:  0.102
      Predicted: 0.077
      Δ (Delta): -0.026

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.231391

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000549
    Max norm: 0.000619
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002151
    Max norm: 0.139971
    Zero gradient ratio: 20.74%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 12.99s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00063740, max_change=0.00178491
  earth4d: mean_change=0.11228811, max_change=0.13240731
  multimodal: mean_change=0.00051128, max_change=0.02202338
  perceiver: mean_change=0.00134288, max_change=0.01285575

================================================================================
BATCH 155/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 128 tokens (25.0%)
    AlphaEarth: 259 tokens (50.6%)
    Encoder_0: 125 tokens (24.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.109375, 'data': 0.142578125, 'dataset': 0.01953125, 'modality': 0.044921875, 'encoder': 0.052734375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1211

📉 LOSS BREAKDOWN:
  Total loss: 0.121108

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000782
    data_loss: 0.000498
    dataset_loss: 0.000061
    encoder_loss: 0.000090

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000026
    alphaearth_modality_loss: 0.088211
    bioclip_modality_loss: 0.001575
    phenovision_modality_loss: 0.149835

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=56
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=73
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=23
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=27

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0092, -0.0162, -0.0370, 0.0191]
    Predicted: [-0.0080, -0.0110, -0.0373, 0.0263]
    Δ (Delta): [0.0011, 0.0051, -0.0004, 0.0072]
    MAPE: 20.82%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.076, -0.006,  0.122,  0.008, -0.06 , -0.001, -0.005,  0.052]
    Predicted:
      [ 0.092, -0.046,  0.109, -0.01 , -0.04 , -0.008, -0.016,  0.067]
    Δ (Delta):
      [ 0.016, -0.04 , -0.013, -0.018,  0.02 , -0.006, -0.011,  0.015]
    MAPE: 205.77%, RMSE: 0.0198

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.362294

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000476
    Max norm: 0.000549
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002912
    Max norm: 0.279851
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.39s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00060663, max_change=0.00175999
  earth4d: mean_change=0.11201574, max_change=0.13180013
  multimodal: mean_change=0.00051400, max_change=0.02169666
  perceiver: mean_change=0.00112149, max_change=0.01205241

================================================================================
BATCH 156/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 133 tokens (26.0%)
    BioCLIP: 127 tokens (24.8%)
    AlphaEarth: 250 tokens (48.8%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09375, 'data': 0.134765625, 'dataset': 0.02734375, 'modality': 0.05078125, 'encoder': 0.04296875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1432

📉 LOSS BREAKDOWN:
  Total loss: 0.143186

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000777
    data_loss: 0.000513
    dataset_loss: 0.000173
    encoder_loss: 0.000063

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000020
    phenovision_modality_loss: 0.200726
    alphaearth_modality_loss: 0.081486
    bioclip_modality_loss: 0.001529

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=48
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=69
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=14
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=22

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0089, -0.0166, -0.0364, 0.0184]
    Predicted: [-0.0023, -0.0085, -0.0476, 0.0191]
    Δ (Delta): [0.0066, 0.0080, -0.0112, 0.0006]
    MAPE: 39.21%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 1):
    Original (first 8 dims):
      [ 0.108, -0.017,  0.122, -0.02 , -0.013, -0.005,  0.017,  0.042]
    Predicted:
      [ 0.102, -0.044,  0.105, -0.008, -0.039, -0.008, -0.021,  0.062]
    Δ (Delta):
      [-0.007, -0.027, -0.016,  0.012, -0.026, -0.002, -0.038,  0.02 ]
    MAPE: 95.16%, RMSE: 0.0215
    🌸 Flowering Probability:
      Original:  0.108
      Predicted: 0.102
      Δ (Delta): -0.007

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.164108

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000509
    Max norm: 0.000575
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001545
    Max norm: 0.117055
    Zero gradient ratio: 20.37%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.34s
  Backward pass: 12.96s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00063594, max_change=0.00175310
  earth4d: mean_change=0.11198566, max_change=0.13165382
  multimodal: mean_change=0.00051648, max_change=0.02136111
  perceiver: mean_change=0.00132005, max_change=0.01184267

================================================================================
BATCH 157/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 246 tokens (48.0%)
    BioCLIP: 133 tokens (26.0%)
    Encoder_0: 132 tokens (25.8%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.103515625, 'data': 0.154296875, 'dataset': 0.017578125, 'modality': 0.033203125, 'encoder': 0.044921875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1321

📉 LOSS BREAKDOWN:
  Total loss: 0.132120

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000776
    data_loss: 0.000412
    dataset_loss: 0.000064
    encoder_loss: 0.000042

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000151
    alphaearth_modality_loss: 0.082219
    phenovision_modality_loss: 0.178081
    bioclip_modality_loss: 0.001513

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=53
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=79
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=17
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=23

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0091, -0.0161, -0.0370, 0.0191]
    Predicted: [-0.0064, -0.0050, -0.0360, 0.0098]
    Δ (Delta): [0.0028, 0.0112, 0.0010, -0.0093]
    MAPE: 37.60%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.092, -0.049,  0.106,  0.009, -0.045,  0.004,  0.023,  0.043]
    Predicted:
      [ 0.094, -0.041,  0.099, -0.022, -0.048, -0.013, -0.006,  0.047]
    Δ (Delta):
      [ 0.001,  0.008, -0.007, -0.031, -0.003, -0.017, -0.03 ,  0.004]
    MAPE: 114.27%, RMSE: 0.0168

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.172253

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000454
    Max norm: 0.000580
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001641
    Max norm: 0.114044
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.53s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00060654, max_change=0.00157528
  earth4d: mean_change=0.11214069, max_change=0.13225929
  multimodal: mean_change=0.00053212, max_change=0.02065112
  perceiver: mean_change=0.00136002, max_change=0.01128236

================================================================================
BATCH 158/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 240 tokens (46.9%)
    Encoder_0: 117 tokens (22.9%)
    BioCLIP: 153 tokens (29.9%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.08984375, 'data': 0.146484375, 'dataset': 0.017578125, 'modality': 0.037109375, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1422

📉 LOSS BREAKDOWN:
  Total loss: 0.142221

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000787
    data_loss: 0.000447
    dataset_loss: 0.000068
    encoder_loss: 0.000230

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000037
    alphaearth_modality_loss: 0.077806
    bioclip_modality_loss: 0.001516
    phenovision_modality_loss: 0.202582

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=46
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=75
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=19
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0077, -0.0188, -0.0345, 0.0159]
    Predicted: [-0.0269, -0.0035, -0.0326, 0.0121]
    Δ (Delta): [-0.0191, 0.0154, 0.0019, -0.0038]
    MAPE: 89.74%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 6):
    Original (first 8 dims):
      [ 0.022, -0.054,  0.071,  0.011, -0.064, -0.025,  0.015, -0.003]
    Predicted:
      [ 0.08 , -0.022,  0.119, -0.008, -0.047, -0.031, -0.007,  0.054]
    Δ (Delta):
      [ 0.058,  0.032,  0.048, -0.019,  0.017, -0.006, -0.022,  0.057]
    MAPE: 317.76%, RMSE: 0.0373

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.125916

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000482
    Max norm: 0.000518
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001204
    Max norm: 0.101332
    Zero gradient ratio: 20.74%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.35s
  Backward pass: 12.99s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00056877, max_change=0.00141965
  earth4d: mean_change=0.11116662, max_change=0.13088676
  multimodal: mean_change=0.00051378, max_change=0.02096597
  perceiver: mean_change=0.00115687, max_change=0.01081050

================================================================================
BATCH 159/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 139 tokens (27.1%)
    BioCLIP: 116 tokens (22.7%)
    AlphaEarth: 256 tokens (50.0%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.103515625, 'data': 0.15234375, 'dataset': 0.01953125, 'modality': 0.05859375, 'encoder': 0.037109375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1366

📉 LOSS BREAKDOWN:
  Total loss: 0.136620

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000815
    data_loss: 0.000536
    dataset_loss: 0.000254
    encoder_loss: 0.000026

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000047
    alphaearth_modality_loss: 0.078533
    bioclip_modality_loss: 0.001458
    phenovision_modality_loss: 0.190482

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=53
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=78
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=30
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=19

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0090, -0.0164, -0.0367, 0.0187]
    Predicted: [-0.0247, -0.0163, -0.0324, 0.0262]
    Δ (Delta): [-0.0158, 0.0001, 0.0043, 0.0075]
    MAPE: 57.27%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.066, -0.008,  0.109, -0.002, -0.042, -0.018,  0.003,  0.099]
    Predicted:
      [ 0.078, -0.039,  0.107, -0.014, -0.059, -0.019, -0.008,  0.047]
    Δ (Delta):
      [ 0.012, -0.03 , -0.002, -0.012, -0.018, -0.001, -0.011, -0.052]
    MAPE: 183.53%, RMSE: 0.0233

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.153228

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000523
    Max norm: 0.000608
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001403
    Max norm: 0.102728
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.42s
  Backward pass: 13.03s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00050133, max_change=0.00125050
  earth4d: mean_change=0.11164217, max_change=0.13105376
  multimodal: mean_change=0.00050050, max_change=0.02036254
  perceiver: mean_change=0.00124826, max_change=0.01060272

================================================================================
BATCH 160/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 269 tokens (52.5%)
    BioCLIP: 119 tokens (23.2%)
    Encoder_0: 123 tokens (24.0%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.087890625, 'data': 0.181640625, 'dataset': 0.01953125, 'modality': 0.0703125, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1404

📉 LOSS BREAKDOWN:
  Total loss: 0.140421

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000822
    data_loss: 0.000405
    dataset_loss: 0.000103
    encoder_loss: 0.000018

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000142
    bioclip_modality_loss: 0.001515
    phenovision_modality_loss: 0.195481
    alphaearth_modality_loss: 0.081340

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=45
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=93
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=36
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0107, -0.0139, -0.0407, 0.0230]
    Predicted: [-0.0132, -0.0285, -0.0280, 0.0307]
    Δ (Delta): [-0.0024, -0.0146, 0.0127, 0.0077]
    MAPE: 48.02%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.074, -0.032,  0.093, -0.002, -0.057, -0.017, -0.024,  0.073]
    Predicted:
      [ 0.071, -0.034,  0.107, -0.004, -0.052, -0.019, -0.006,  0.057]
    Δ (Delta):
      [-0.003, -0.002,  0.013, -0.002,  0.005, -0.002,  0.018, -0.016]
    MAPE: 35.40%, RMSE: 0.0099

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.130364

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000465
    Max norm: 0.000613
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001147
    Max norm: 0.119443
    Zero gradient ratio: 23.70%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.43s
  Backward pass: 13.05s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00079185, max_change=0.00270961
  earth4d: mean_change=0.11120319, max_change=0.13082317
  multimodal: mean_change=0.00047975, max_change=0.01950457
  perceiver: mean_change=0.00135448, max_change=0.01009983

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.000822 (trend: increasing)
  data_loss: 0.000405 (trend: decreasing)
  dataset_loss: 0.000103 (trend: decreasing)
  modality_loss: 0.000142 (trend: increasing)
  encoder_loss: 0.000018 (trend: decreasing)
  phenovision_modality_loss: 0.195481 (trend: decreasing)
  bioclip_modality_loss: 0.001515 (trend: decreasing)
  alphaearth_modality_loss: 0.081340 (trend: decreasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000465, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.001147, zero_ratio=23.70%

================================================================================
BATCH 161/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 253 tokens (49.4%)
    Encoder_0: 124 tokens (24.2%)
    BioCLIP: 134 tokens (26.2%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.123046875, 'data': 0.17578125, 'dataset': 0.013671875, 'modality': 0.046875, 'encoder': 0.05859375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1309

📉 LOSS BREAKDOWN:
  Total loss: 0.130894

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000826
    data_loss: 0.000408
    dataset_loss: 0.000212
    encoder_loss: 0.000036

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000060
    phenovision_modality_loss: 0.177826
    bioclip_modality_loss: 0.001466
    alphaearth_modality_loss: 0.079966

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=63
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=90
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=7
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=24
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=30

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0133, -0.0073, -0.0434, 0.0259]
    Predicted: [0.0028, -0.0267, -0.0350, 0.0239]
    Δ (Delta): [0.0162, -0.0194, 0.0084, -0.0020]
    MAPE: 102.97%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.081, -0.002,  0.126, -0.016, -0.037, -0.013, -0.017,  0.043]
    Predicted:
      [ 0.071, -0.047,  0.104, -0.015, -0.043, -0.016,  0.001,  0.047]
    Δ (Delta):
      [-0.01 , -0.045, -0.023,  0.001, -0.006, -0.003,  0.018,  0.004]
    MAPE: 262.37%, RMSE: 0.0195
    🌸 Flowering Probability:
      Original:  0.081
      Predicted: 0.071
      Δ (Delta): -0.010

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.145359

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000574
    Max norm: 0.000734
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001462
    Max norm: 0.097024
    Zero gradient ratio: 20.74%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.56s
  Backward pass: 13.05s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00078084, max_change=0.00247831
  earth4d: mean_change=0.11353769, max_change=0.13358799
  multimodal: mean_change=0.00046826, max_change=0.01966990
  perceiver: mean_change=0.00127725, max_change=0.01264663

================================================================================
BATCH 162/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 126 tokens (24.6%)
    AlphaEarth: 268 tokens (52.3%)
    BioCLIP: 117 tokens (22.9%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.08984375, 'data': 0.1171875, 'dataset': 0.013671875, 'modality': 0.056640625, 'encoder': 0.029296875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1281

📉 LOSS BREAKDOWN:
  Total loss: 0.128084

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000827
    data_loss: 0.000428
    dataset_loss: 0.000303
    encoder_loss: 0.000016

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000055
    bioclip_modality_loss: 0.001422
    phenovision_modality_loss: 0.172597
    alphaearth_modality_loss: 0.079562

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=46
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=60
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=7
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=29
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=15

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0093, -0.0159, -0.0374, 0.0196]
    Predicted: [0.0080, -0.0238, -0.0315, 0.0220]
    Δ (Delta): [0.0173, -0.0080, 0.0059, 0.0024]
    MAPE: 65.96%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 1, seq 8):
    Original (first 8 dims):
      [ 0.085, -0.027,  0.103, -0.002, -0.039, -0.044, -0.003,  0.051]
    Predicted:
      [ 0.086, -0.021,  0.115, -0.003, -0.036, -0.02 , -0.013,  0.061]
    Δ (Delta):
      [ 6.104e-05,  6.027e-03,  1.227e-02, -8.678e-04,  3.571e-03,  2.415e-02, -1.035e-02,  9.247e-03]
    MAPE: 66.81%, RMSE: 0.0110

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.239320

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000474
    Max norm: 0.000540
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002040
    Max norm: 0.179385
    Zero gradient ratio: 19.26%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.32s
  Backward pass: 12.93s
  Ratio (back/fwd): 5.6x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00074321, max_change=0.00224455
  earth4d: mean_change=0.11323505, max_change=0.13314883
  multimodal: mean_change=0.00047816, max_change=0.02009824
  perceiver: mean_change=0.00141123, max_change=0.01253722

================================================================================
BATCH 163/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 246 tokens (48.0%)
    Encoder_0: 130 tokens (25.4%)
    BioCLIP: 136 tokens (26.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.087890625, 'data': 0.13671875, 'dataset': 0.033203125, 'modality': 0.048828125, 'encoder': 0.0546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1361

📉 LOSS BREAKDOWN:
  Total loss: 0.136107

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000801
    data_loss: 0.000517
    dataset_loss: 0.000063
    encoder_loss: 0.000025

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000150
    bioclip_modality_loss: 0.001325
    alphaearth_modality_loss: 0.077341
    phenovision_modality_loss: 0.190864

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=45
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=70
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=17
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=25
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=28

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 2, seq 0):
    Original:  [-0.0113, -0.0075, -0.0363, 0.0207]
    Predicted: [-0.0059, -0.0254, -0.0333, 0.0284]
    Δ (Delta): [0.0054, -0.0178, 0.0030, 0.0077]
    MAPE: 82.52%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 11):
    Original (first 8 dims):
      [ 0.088, -0.045,  0.132,  0.026, -0.047, -0.009,  0.02 ,  0.067]
    Predicted:
      [ 0.108, -0.036,  0.107,  0.003, -0.05 , -0.012, -0.03 ,  0.066]
    Δ (Delta):
      [ 0.019,  0.01 , -0.025, -0.023, -0.003, -0.003, -0.051, -0.001]
    MAPE: 56.12%, RMSE: 0.0229

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.146457

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000609
    Max norm: 0.000793
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001455
    Max norm: 0.087297
    Zero gradient ratio: 21.48%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00069936, max_change=0.00208901
  earth4d: mean_change=0.11438914, max_change=0.13434987
  multimodal: mean_change=0.00047586, max_change=0.01940942
  perceiver: mean_change=0.00132945, max_change=0.01257489

================================================================================
BATCH 164/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 245 tokens (47.9%)
    BioCLIP: 149 tokens (29.1%)
    Encoder_0: 117 tokens (22.9%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.130859375, 'data': 0.140625, 'dataset': 0.01953125, 'modality': 0.056640625, 'encoder': 0.052734375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1687

📉 LOSS BREAKDOWN:
  Total loss: 0.168721

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000765
    data_loss: 0.000534
    dataset_loss: 0.000208
    encoder_loss: 0.000004

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000031
    phenovision_modality_loss: 0.258084
    bioclip_modality_loss: 0.001282
    alphaearth_modality_loss: 0.075430

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=67
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=72
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=29
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=27

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 6):
    Original:  [-0.0142, -0.0037, -0.0430, 0.0263]
    Predicted: [-0.0144, -0.0113, -0.0482, 0.0157]
    Δ (Delta): [-0.0002, -0.0076, -0.0051, -0.0107]
    MAPE: 64.36%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [ 0.088, -0.016,  0.093,  0.015, -0.094, -0.008, -0.009,  0.032]
    Predicted:
      [ 0.102, -0.04 ,  0.112, -0.006, -0.05 , -0.014, -0.018,  0.06 ]
    Δ (Delta):
      [ 0.014, -0.023,  0.019, -0.021,  0.044, -0.005, -0.009,  0.027]
    MAPE: 76.48%, RMSE: 0.0233
    🌸 Flowering Probability:
      Original:  0.088
      Predicted: 0.102
      Δ (Delta): 0.014

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.496626

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000498
    Max norm: 0.000578
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.003523
    Max norm: 0.389464
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.35s
  Backward pass: 12.97s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00063786, max_change=0.00188112
  earth4d: mean_change=0.11535572, max_change=0.13548008
  multimodal: mean_change=0.00047471, max_change=0.01907963
  perceiver: mean_change=0.00134688, max_change=0.01152739

================================================================================
BATCH 165/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 139 tokens (27.1%)
    AlphaEarth: 259 tokens (50.6%)
    Encoder_0: 114 tokens (22.3%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.076171875, 'data': 0.1640625, 'dataset': 0.01953125, 'modality': 0.0625, 'encoder': 0.048828125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1256

📉 LOSS BREAKDOWN:
  Total loss: 0.125562

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000738
    data_loss: 0.000430
    dataset_loss: 0.000223
    encoder_loss: 0.000260

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000013
    phenovision_modality_loss: 0.169117
    bioclip_modality_loss: 0.001413
    alphaearth_modality_loss: 0.078159

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=39
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=84
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=32
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=25

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0012, -0.0285, -0.0171, -0.0118]
    Predicted: [-0.0155, -0.0099, -0.0356, 0.0181]
    Δ (Delta): [-0.0143, 0.0186, -0.0185, 0.0298]
    MAPE: 411.48%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.087, -0.029,  0.095, -0.002, -0.071, -0.029, -0.025,  0.043]
    Predicted:
      [ 0.091, -0.051,  0.099, -0.025, -0.055, -0.007,  0.004,  0.043]
    Δ (Delta):
      [ 0.004, -0.021,  0.004, -0.023,  0.016,  0.022,  0.029, -0.   ]
    MAPE: 216.18%, RMSE: 0.0181
    🌸 Flowering Probability:
      Original:  0.087
      Predicted: 0.091
      Δ (Delta): 0.004

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.147877

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000530
    Max norm: 0.000604
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001474
    Max norm: 0.095041
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.03s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00062730, max_change=0.00169815
  earth4d: mean_change=0.11403416, max_change=0.13402171
  multimodal: mean_change=0.00047752, max_change=0.01971294
  perceiver: mean_change=0.00141529, max_change=0.01174849

================================================================================
BATCH 166/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 257 tokens (50.2%)
    Encoder_0: 120 tokens (23.4%)
    BioCLIP: 133 tokens (26.0%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.10546875, 'data': 0.13671875, 'dataset': 0.02734375, 'modality': 0.037109375, 'encoder': 0.05859375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1268

📉 LOSS BREAKDOWN:
  Total loss: 0.126754

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000764
    data_loss: 0.000434
    dataset_loss: 0.000034
    encoder_loss: 0.000031

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000038
    bioclip_modality_loss: 0.001402
    phenovision_modality_loss: 0.164363
    alphaearth_modality_loss: 0.085327

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=54
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=70
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=14
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=19
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=30

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 3):
    Original:  [-0.0098, -0.0146, -0.0379, 0.0204]
    Predicted: [-0.0125, -0.0159, -0.0293, 0.0060]
    Δ (Delta): [-0.0027, -0.0013, 0.0086, -0.0144]
    MAPE: 32.47%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.095, -0.033,  0.115, -0.008, -0.042, -0.02 , -0.   ,  0.056]
    Predicted:
      [ 0.087, -0.032,  0.121, -0.014, -0.035, -0.024,  0.007,  0.049]
    Δ (Delta):
      [-0.008,  0.001,  0.006, -0.006,  0.006, -0.004,  0.008, -0.007]
    MAPE: 248.78%, RMSE: 0.0062

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.184959

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000525
    Max norm: 0.000630
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001754
    Max norm: 0.131044
    Zero gradient ratio: 21.11%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.49s
  Backward pass: 12.97s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00054090, max_change=0.00154099
  earth4d: mean_change=0.11539068, max_change=0.13617015
  multimodal: mean_change=0.00046467, max_change=0.01921475
  perceiver: mean_change=0.00126626, max_change=0.01176416

================================================================================
BATCH 167/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 254 tokens (49.6%)
    BioCLIP: 136 tokens (26.6%)
    Encoder_0: 122 tokens (23.8%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1015625, 'data': 0.169921875, 'dataset': 0.025390625, 'modality': 0.037109375, 'encoder': 0.037109375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1291

📉 LOSS BREAKDOWN:
  Total loss: 0.129116

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000764
    data_loss: 0.000426
    dataset_loss: 0.000291
    encoder_loss: 0.000005

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000023
    alphaearth_modality_loss: 0.072903
    bioclip_modality_loss: 0.001301
    phenovision_modality_loss: 0.181585

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=52
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=87
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=13
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=19
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=19

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0087, -0.0169, -0.0361, 0.0183]
    Predicted: [-0.0067, -0.0105, -0.0286, 0.0021]
    Δ (Delta): [0.0020, 0.0064, 0.0075, -0.0162]
    MAPE: 42.60%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.12 , -0.006,  0.108, -0.019, -0.051, -0.001, -0.015,  0.034]
    Predicted:
      [ 0.082, -0.034,  0.107, -0.018, -0.041, -0.026,  0.015,  0.047]
    Δ (Delta):
      [-0.038, -0.029, -0.001,  0.001,  0.009, -0.024,  0.03 ,  0.012]
    MAPE: 328.58%, RMSE: 0.0223

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.222379

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000558
    Max norm: 0.000781
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001935
    Max norm: 0.154017
    Zero gradient ratio: 20.37%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.05s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00052945, max_change=0.00142165
  earth4d: mean_change=0.11565624, max_change=0.13704796
  multimodal: mean_change=0.00047992, max_change=0.02056107
  perceiver: mean_change=0.00145509, max_change=0.01191289

================================================================================
BATCH 168/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 247 tokens (48.2%)
    BioCLIP: 134 tokens (26.2%)
    Encoder_0: 131 tokens (25.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.103515625, 'data': 0.154296875, 'dataset': 0.021484375, 'modality': 0.048828125, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1408

📉 LOSS BREAKDOWN:
  Total loss: 0.140777

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000753
    data_loss: 0.000389
    dataset_loss: 0.000017
    encoder_loss: 0.000081

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000071
    phenovision_modality_loss: 0.197767
    alphaearth_modality_loss: 0.080217
    bioclip_modality_loss: 0.001252

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=53
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=79
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=25
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0113, -0.0076, -0.0362, 0.0207]
    Predicted: [-0.0194, -0.0238, -0.0224, 0.0194]
    Δ (Delta): [-0.0081, -0.0162, 0.0138, -0.0013]
    MAPE: 82.42%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.088, -0.04 ,  0.117,  0.001, -0.046, -0.025,  0.012,  0.05 ]
    Predicted:
      [ 0.084, -0.029,  0.108,  0.001, -0.046, -0.023, -0.007,  0.063]
    Δ (Delta):
      [-0.004,  0.011, -0.01 ,  0.   , -0.   ,  0.002, -0.02 ,  0.012]
    MAPE: 30.53%, RMSE: 0.0098
    🌸 Flowering Probability:
      Original:  0.088
      Predicted: 0.084
      Δ (Delta): -0.004

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.155399

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000415
    Max norm: 0.000451
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001504
    Max norm: 0.108620
    Zero gradient ratio: 20.37%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00051892, max_change=0.00130754
  earth4d: mean_change=0.11516213, max_change=0.13614118
  multimodal: mean_change=0.00044911, max_change=0.01974528
  perceiver: mean_change=0.00130282, max_change=0.01177650

================================================================================
BATCH 169/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 256 tokens (50.0%)
    Encoder_0: 132 tokens (25.8%)
    BioCLIP: 124 tokens (24.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.091796875, 'data': 0.166015625, 'dataset': 0.009765625, 'modality': 0.0625, 'encoder': 0.03515625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1420

📉 LOSS BREAKDOWN:
  Total loss: 0.142014

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000816
    data_loss: 0.000529
    dataset_loss: 0.000171
    encoder_loss: 0.000019

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000015
    bioclip_modality_loss: 0.001229
    alphaearth_modality_loss: 0.077707
    phenovision_modality_loss: 0.202360

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=47
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=85
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=5
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=32
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=18

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 4):
    Original:  [-0.0107, -0.0116, -0.0381, 0.0209]
    Predicted: [-0.0177, -0.0189, -0.0252, 0.0203]
    Δ (Delta): [-0.0069, -0.0074, 0.0129, -0.0006]
    MAPE: 41.34%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [ 0.085, -0.028,  0.102,  0.007, -0.044, -0.021, -0.01 ,  0.04 ]
    Predicted:
      [ 0.089, -0.031,  0.097,  0.004, -0.053, -0.002, -0.022,  0.073]
    Δ (Delta):
      [ 0.004, -0.003, -0.005, -0.003, -0.008,  0.019, -0.012,  0.033]
    MAPE: 46.65%, RMSE: 0.0147

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.301797

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000578
    Max norm: 0.000673
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002440
    Max norm: 0.231762
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.39s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00048888, max_change=0.00124139
  earth4d: mean_change=0.11615666, max_change=0.13732560
  multimodal: mean_change=0.00042557, max_change=0.01822853
  perceiver: mean_change=0.00127618, max_change=0.01218937

================================================================================
BATCH 170/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 299 tokens (58.4%)
    Encoder_0: 108 tokens (21.1%)
    BioCLIP: 103 tokens (20.1%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.123046875, 'data': 0.16796875, 'dataset': 0.025390625, 'modality': 0.037109375, 'encoder': 0.05859375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1207

📉 LOSS BREAKDOWN:
  Total loss: 0.120690

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000781
    data_loss: 0.000445
    dataset_loss: 0.000093
    encoder_loss: 0.000158

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000120
    alphaearth_modality_loss: 0.076227
    bioclip_modality_loss: 0.001276
    phenovision_modality_loss: 0.161351

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=63
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=86
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=13
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=19
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=30

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0089, -0.0164, -0.0361, 0.0182]
    Predicted: [-0.0022, -0.0139, -0.0276, 0.0183]
    Δ (Delta): [0.0066, 0.0025, 0.0085, 0.0002]
    MAPE: 28.63%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.115, -0.062,  0.084, -0.003, -0.018, -0.026, -0.   ,  0.056]
    Predicted:
      [ 0.088, -0.029,  0.113,  0.008, -0.062, -0.016, -0.021,  0.071]
    Δ (Delta):
      [-0.027,  0.033,  0.028,  0.011, -0.044,  0.009, -0.02 ,  0.015]
    MAPE: 675.54%, RMSE: 0.0261

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.119429

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000586
    Max norm: 0.000652
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001109
    Max norm: 0.097625
    Zero gradient ratio: 22.22%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.57s
  Backward pass: 13.04s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00061817, max_change=0.00179367
  earth4d: mean_change=0.11721340, max_change=0.13888861
  multimodal: mean_change=0.00041500, max_change=0.01858176
  perceiver: mean_change=0.00142596, max_change=0.01254714

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.000781 (trend: increasing)
  data_loss: 0.000445 (trend: increasing)
  dataset_loss: 0.000093 (trend: increasing)
  modality_loss: 0.000120 (trend: increasing)
  encoder_loss: 0.000158 (trend: increasing)
  phenovision_modality_loss: 0.161351 (trend: decreasing)
  bioclip_modality_loss: 0.001276 (trend: decreasing)
  alphaearth_modality_loss: 0.076227 (trend: decreasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000586, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.001109, zero_ratio=22.22%

================================================================================
BATCH 171/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 258 tokens (50.4%)
    Encoder_0: 113 tokens (22.1%)
    BioCLIP: 140 tokens (27.3%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.10546875, 'data': 0.15234375, 'dataset': 0.0234375, 'modality': 0.03515625, 'encoder': 0.033203125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1457

📉 LOSS BREAKDOWN:
  Total loss: 0.145657

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000784
    data_loss: 0.000436
    dataset_loss: 0.000025
    encoder_loss: 0.000033

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000076
    bioclip_modality_loss: 0.001246
    alphaearth_modality_loss: 0.081482
    phenovision_modality_loss: 0.206118

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=54
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=78
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=18
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=17

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 8):
    Original:  [-0.0141, -0.0041, -0.0427, 0.0260]
    Predicted: [0.0050, -0.0041, -0.0515, 0.0174]
    Δ (Delta): [0.0191, -0.0000, -0.0087, -0.0086]
    MAPE: 47.28%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [ 0.104, -0.039,  0.085, -0.003, -0.057,  0.006,  0.007,  0.02 ]
    Predicted:
      [ 0.092, -0.04 ,  0.125, -0.006, -0.044, -0.02 , -0.015,  0.051]
    Δ (Delta):
      [-0.012, -0.001,  0.039, -0.003,  0.013, -0.026, -0.022,  0.031]
    MAPE: 138.08%, RMSE: 0.0224

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.165545

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000412
    Max norm: 0.000483
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001512
    Max norm: 0.128826
    Zero gradient ratio: 21.48%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00070253, max_change=0.00220550
  earth4d: mean_change=0.11760386, max_change=0.13909009
  multimodal: mean_change=0.00041411, max_change=0.01693257
  perceiver: mean_change=0.00135531, max_change=0.01315759

================================================================================
BATCH 172/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 144 tokens (28.1%)
    AlphaEarth: 250 tokens (48.8%)
    Encoder_0: 118 tokens (23.0%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.091796875, 'data': 0.13671875, 'dataset': 0.02734375, 'modality': 0.076171875, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1277

📉 LOSS BREAKDOWN:
  Total loss: 0.127675

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000814
    data_loss: 0.000465
    dataset_loss: 0.000154
    encoder_loss: 0.000086

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000008
    phenovision_modality_loss: 0.178801
    bioclip_modality_loss: 0.001275
    alphaearth_modality_loss: 0.072668

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=47
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=70
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=14
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=39
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 9):
    Original:  [-0.0095, -0.0153, -0.0370, 0.0193]
    Predicted: [0.0027, -0.0070, -0.0449, 0.0169]
    Δ (Delta): [0.0122, 0.0083, -0.0079, -0.0024]
    MAPE: 54.12%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 0):
    Original (first 8 dims):
      [ 0.1  , -0.047,  0.089, -0.003, -0.046, -0.   ,  0.013,  0.062]
    Predicted:
      [ 0.092, -0.043,  0.119, -0.009, -0.04 , -0.026,  0.004,  0.038]
    Δ (Delta):
      [-0.008,  0.005,  0.03 , -0.006,  0.006, -0.026, -0.009, -0.024]
    MAPE: 1839.26%, RMSE: 0.0172
    🌸 Flowering Probability:
      Original:  0.100
      Predicted: 0.092
      Δ (Delta): -0.008

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.154976

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000522
    Max norm: 0.000643
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001458
    Max norm: 0.093783
    Zero gradient ratio: 21.11%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 12.97s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00066572, max_change=0.00208019
  earth4d: mean_change=0.11718069, max_change=0.13882147
  multimodal: mean_change=0.00040999, max_change=0.01731664
  perceiver: mean_change=0.00136038, max_change=0.01294027

================================================================================
BATCH 173/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 132 tokens (25.8%)
    Encoder_0: 130 tokens (25.4%)
    AlphaEarth: 248 tokens (48.4%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.0859375, 'data': 0.126953125, 'dataset': 0.029296875, 'modality': 0.0546875, 'encoder': 0.044921875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1209

📉 LOSS BREAKDOWN:
  Total loss: 0.120930

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000759
    data_loss: 0.000358
    dataset_loss: 0.000093
    encoder_loss: 0.000023

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000148
    alphaearth_modality_loss: 0.076187
    bioclip_modality_loss: 0.001238
    phenovision_modality_loss: 0.162147

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=44
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=65
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=15
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=28
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=23

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0091, -0.0163, -0.0367, 0.0189]
    Predicted: [-0.0175, -0.0137, -0.0339, 0.0154]
    Δ (Delta): [-0.0084, 0.0026, 0.0028, -0.0035]
    MAPE: 33.66%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 1, seq 0):
    Original (first 8 dims):
      [ 0.118, -0.048,  0.095, -0.019, -0.034,  0.022, -0.026,  0.051]
    Predicted:
      [ 0.083, -0.052,  0.114, -0.019, -0.036, -0.027,  0.009,  0.046]
    Δ (Delta):
      [-0.036, -0.004,  0.019,  0.   , -0.003, -0.049,  0.035, -0.005]
    MAPE: 54.40%, RMSE: 0.0258

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.351917

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000493
    Max norm: 0.000545
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002609
    Max norm: 0.273441
    Zero gradient ratio: 20.37%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.33s
  Backward pass: 12.95s
  Ratio (back/fwd): 5.6x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00060926, max_change=0.00188199
  earth4d: mean_change=0.11680629, max_change=0.13852678
  multimodal: mean_change=0.00041301, max_change=0.01718766
  perceiver: mean_change=0.00136122, max_change=0.01231008

================================================================================
BATCH 174/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 123 tokens (24.0%)
    AlphaEarth: 259 tokens (50.6%)
    Encoder_0: 128 tokens (25.0%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.08203125, 'data': 0.140625, 'dataset': 0.0234375, 'modality': 0.04296875, 'encoder': 0.044921875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1400

📉 LOSS BREAKDOWN:
  Total loss: 0.140013

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000766
    data_loss: 0.000384
    dataset_loss: 0.000044
    encoder_loss: 0.000096

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000010
    phenovision_modality_loss: 0.202497
    bioclip_modality_loss: 0.001212
    alphaearth_modality_loss: 0.073989

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=42
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=72
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=23

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 1, seq 2):
    Original:  [-0.0108, -0.0139, -0.0403, 0.0228]
    Predicted: [-0.0200, -0.0150, -0.0336, 0.0144]
    Δ (Delta): [-0.0092, -0.0011, 0.0067, -0.0083]
    MAPE: 36.81%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 13):
    Original (first 8 dims):
      [ 0.101, -0.03 ,  0.113, -0.001, -0.039, -0.017, -0.022,  0.055]
    Predicted:
      [ 0.089, -0.046,  0.105, -0.014, -0.044, -0.026, -0.002,  0.068]
    Δ (Delta):
      [-0.012, -0.016, -0.008, -0.012, -0.005, -0.009,  0.02 ,  0.012]
    MAPE: 146.52%, RMSE: 0.0126
    🌸 Flowering Probability:
      Original:  0.101
      Predicted: 0.089
      Δ (Delta): -0.012

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.095372

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000510
    Max norm: 0.000662
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.000928
    Max norm: 0.079791
    Zero gradient ratio: 21.48%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.51s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00054203, max_change=0.00170006
  earth4d: mean_change=0.11559829, max_change=0.13728136
  multimodal: mean_change=0.00041183, max_change=0.01618481
  perceiver: mean_change=0.00118843, max_change=0.01185736

================================================================================
BATCH 175/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 260 tokens (50.8%)
    Encoder_0: 131 tokens (25.6%)
    BioCLIP: 121 tokens (23.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.087890625, 'data': 0.12890625, 'dataset': 0.025390625, 'modality': 0.04296875, 'encoder': 0.044921875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1387

📉 LOSS BREAKDOWN:
  Total loss: 0.138735

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000695
    data_loss: 0.000418
    dataset_loss: 0.000048
    encoder_loss: 0.000140

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000048
    alphaearth_modality_loss: 0.079283
    bioclip_modality_loss: 0.001156
    phenovision_modality_loss: 0.194755

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=45
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=66
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=13
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=23

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 5):
    Original:  [-0.0100, -0.0144, -0.0380, 0.0205]
    Predicted: [-0.0105, -0.0184, -0.0324, 0.0249]
    Δ (Delta): [-0.0005, -0.0040, 0.0056, 0.0044]
    MAPE: 17.32%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 1, seq 6):
    Original (first 8 dims):
      [ 0.13 , -0.01 ,  0.096, -0.008, -0.066, -0.017, -0.017,  0.048]
    Predicted:
      [ 0.09 , -0.042,  0.092, -0.009, -0.05 , -0.015, -0.005,  0.063]
    Δ (Delta):
      [-0.04 , -0.031, -0.003, -0.001,  0.017,  0.002,  0.012,  0.014]
    MAPE: 61.47%, RMSE: 0.0201

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.125347

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000470
    Max norm: 0.000598
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001193
    Max norm: 0.102707
    Zero gradient ratio: 22.22%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.34s
  Backward pass: 12.97s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00050646, max_change=0.00150180
  earth4d: mean_change=0.11487180, max_change=0.13643314
  multimodal: mean_change=0.00042085, max_change=0.01597170
  perceiver: mean_change=0.00118135, max_change=0.01119227

================================================================================
BATCH 176/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 150 tokens (29.3%)
    AlphaEarth: 234 tokens (45.7%)
    BioCLIP: 126 tokens (24.6%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.08984375, 'data': 0.150390625, 'dataset': 0.033203125, 'modality': 0.05859375, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1444

📉 LOSS BREAKDOWN:
  Total loss: 0.144430

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000749
    data_loss: 0.000469
    dataset_loss: 0.000045
    encoder_loss: 0.000024

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000015
    bioclip_modality_loss: 0.001148
    phenovision_modality_loss: 0.206072
    alphaearth_modality_loss: 0.079187

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=46
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=77
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=17
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=30
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 2, seq 6):
    Original:  [-0.0142, -0.0040, -0.0428, 0.0261]
    Predicted: [-0.0039, -0.0220, -0.0341, 0.0363]
    Δ (Delta): [0.0103, -0.0180, 0.0087, 0.0102]
    MAPE: 146.02%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 11):
    Original (first 8 dims):
      [ 0.08 , -0.03 ,  0.101, -0.   , -0.058, -0.013, -0.021,  0.049]
    Predicted:
      [ 0.083, -0.035,  0.084, -0.01 , -0.055, -0.009, -0.003,  0.058]
    Δ (Delta):
      [ 0.002, -0.004, -0.017, -0.009,  0.003,  0.003,  0.017,  0.009]
    MAPE: 473.27%, RMSE: 0.0100

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.177382

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000424
    Max norm: 0.000487
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001590
    Max norm: 0.120779
    Zero gradient ratio: 21.48%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.36s
  Backward pass: 12.98s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00050292, max_change=0.00140824
  earth4d: mean_change=0.11476913, max_change=0.13579758
  multimodal: mean_change=0.00043369, max_change=0.01724015
  perceiver: mean_change=0.00116732, max_change=0.01079038

================================================================================
BATCH 177/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 112 tokens (21.9%)
    Encoder_0: 135 tokens (26.4%)
    AlphaEarth: 265 tokens (51.8%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.080078125, 'data': 0.158203125, 'dataset': 0.005859375, 'modality': 0.04296875, 'encoder': 0.037109375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1284

📉 LOSS BREAKDOWN:
  Total loss: 0.128427

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000713
    data_loss: 0.000366
    dataset_loss: 0.000036
    encoder_loss: 0.000077

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000012
    phenovision_modality_loss: 0.174650
    bioclip_modality_loss: 0.001187
    alphaearth_modality_loss: 0.078834

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=41
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=81
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=3
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=19

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 9):
    Original:  [-0.0107, -0.0141, -0.0404, 0.0229]
    Predicted: [-0.0026, -0.0095, -0.0454, 0.0304]
    Δ (Delta): [0.0081, 0.0047, -0.0050, 0.0075]
    MAPE: 38.37%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.072, -0.056,  0.093, -0.004, -0.06 , -0.027, -0.006,  0.038]
    Predicted:
      [ 0.09 , -0.03 ,  0.099, -0.001, -0.057, -0.014, -0.007,  0.051]
    Δ (Delta):
      [ 0.018,  0.026,  0.007,  0.003,  0.003,  0.013, -0.001,  0.013]
    MAPE: 32.87%, RMSE: 0.0132
    🌸 Flowering Probability:
      Original:  0.072
      Predicted: 0.090
      Δ (Delta): 0.018

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.181929

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000439
    Max norm: 0.000503
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001565
    Max norm: 0.134196
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00047981, max_change=0.00127160
  earth4d: mean_change=0.11423229, max_change=0.13530408
  multimodal: mean_change=0.00041997, max_change=0.01690366
  perceiver: mean_change=0.00114742, max_change=0.01083294

================================================================================
BATCH 178/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 267 tokens (52.1%)
    BioCLIP: 123 tokens (24.0%)
    Encoder_0: 122 tokens (23.8%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.099609375, 'data': 0.138671875, 'dataset': 0.01171875, 'modality': 0.048828125, 'encoder': 0.052734375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1270

📉 LOSS BREAKDOWN:
  Total loss: 0.127003

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000693
    data_loss: 0.000452
    dataset_loss: 0.000045
    encoder_loss: 0.000167

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000040
    bioclip_modality_loss: 0.001129
    phenovision_modality_loss: 0.169813
    alphaearth_modality_loss: 0.080725

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=51
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=71
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=6
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=25
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=27

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 5):
    Original:  [-0.0132, -0.0101, -0.0449, 0.0269]
    Predicted: [-0.0073, -0.0116, -0.0385, 0.0220]
    Δ (Delta): [0.0058, -0.0015, 0.0063, -0.0049]
    MAPE: 22.87%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 1, seq 1):
    Original (first 8 dims):
      [ 0.086, -0.043,  0.109,  0.013, -0.046, -0.028,  0.011,  0.05 ]
    Predicted:
      [ 0.086, -0.034,  0.102, -0.009, -0.054, -0.017,  0.003,  0.051]
    Δ (Delta):
      [-0.   ,  0.01 , -0.007, -0.023, -0.007,  0.011, -0.008,  0.001]
    MAPE: 40.73%, RMSE: 0.0106

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.172877

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000381
    Max norm: 0.000450
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001601
    Max norm: 0.120186
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.03s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00045381, max_change=0.00119179
  earth4d: mean_change=0.11367720, max_change=0.13417248
  multimodal: mean_change=0.00045424, max_change=0.01941126
  perceiver: mean_change=0.00115900, max_change=0.01014265

================================================================================
BATCH 179/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 129 tokens (25.2%)
    AlphaEarth: 256 tokens (50.0%)
    BioCLIP: 127 tokens (24.8%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.091796875, 'data': 0.13671875, 'dataset': 0.01953125, 'modality': 0.052734375, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1330

📉 LOSS BREAKDOWN:
  Total loss: 0.133020

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000727
    data_loss: 0.000403
    dataset_loss: 0.000033
    encoder_loss: 0.000195

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000013
    bioclip_modality_loss: 0.001137
    alphaearth_modality_loss: 0.081359
    phenovision_modality_loss: 0.181235

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=47
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=70
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=10
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=27
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 6):
    Original:  [-0.0143, -0.0038, -0.0429, 0.0261]
    Predicted: [-0.0178, -0.0201, -0.0302, 0.0180]
    Δ (Delta): [-0.0036, -0.0162, 0.0126, -0.0081]
    MAPE: 127.13%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 1):
    Original (first 8 dims):
      [ 0.098, -0.01 ,  0.088, -0.021, -0.068, -0.024,  0.001,  0.057]
    Predicted:
      [ 0.085, -0.047,  0.119, -0.009, -0.052, -0.022,  0.011,  0.041]
    Δ (Delta):
      [-0.014, -0.037,  0.031,  0.012,  0.016,  0.002,  0.01 , -0.016]
    MAPE: 194.26%, RMSE: 0.0202

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.158143

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000417
    Max norm: 0.000483
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001478
    Max norm: 0.113627
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.35s
  Backward pass: 12.99s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00079241, max_change=0.00286196
  earth4d: mean_change=0.11311108, max_change=0.13334876
  multimodal: mean_change=0.00045832, max_change=0.02124875
  perceiver: mean_change=0.00110280, max_change=0.00953839

================================================================================
BATCH 180/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 262 tokens (51.2%)
    Encoder_0: 122 tokens (23.8%)
    BioCLIP: 128 tokens (25.0%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.10546875, 'data': 0.16015625, 'dataset': 0.021484375, 'modality': 0.033203125, 'encoder': 0.052734375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1386

📉 LOSS BREAKDOWN:
  Total loss: 0.138567

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000738
    data_loss: 0.000413
    dataset_loss: 0.000126
    encoder_loss: 0.000011

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000006
    bioclip_modality_loss: 0.001150
    alphaearth_modality_loss: 0.076920
    phenovision_modality_loss: 0.196733

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=54
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=82
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=17
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=27

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 8):
    Original:  [-0.0098, -0.0146, -0.0376, 0.0199]
    Predicted: [-0.0213, -0.0140, -0.0421, 0.0056]
    Δ (Delta): [-0.0115, 0.0006, -0.0045, -0.0143]
    MAPE: 51.19%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 9):
    Original (first 8 dims):
      [ 0.085, -0.056,  0.091, -0.025, -0.033, -0.029,  0.007,  0.047]
    Predicted:
      [ 0.092, -0.051,  0.098, -0.016, -0.055, -0.02 ,  0.004,  0.047]
    Δ (Delta):
      [ 0.007,  0.006,  0.007,  0.009, -0.022,  0.01 , -0.002, -0.   ]
    MAPE: 24.38%, RMSE: 0.0100

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.101288

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000370
    Max norm: 0.000439
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.000990
    Max norm: 0.081821
    Zero gradient ratio: 24.07%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.40s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00073235, max_change=0.00258466
  earth4d: mean_change=0.11481497, max_change=0.13528153
  multimodal: mean_change=0.00042580, max_change=0.01893328
  perceiver: mean_change=0.00116164, max_change=0.00915967

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.000738 (trend: decreasing)
  data_loss: 0.000413 (trend: decreasing)
  dataset_loss: 0.000126 (trend: increasing)
  modality_loss: 0.000006 (trend: decreasing)
  encoder_loss: 0.000011 (trend: decreasing)
  phenovision_modality_loss: 0.196733 (trend: decreasing)
  bioclip_modality_loss: 0.001150 (trend: increasing)
  alphaearth_modality_loss: 0.076920 (trend: decreasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000370, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.000990, zero_ratio=24.07%

================================================================================
BATCH 181/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 137 tokens (26.8%)
    AlphaEarth: 246 tokens (48.0%)
    BioCLIP: 127 tokens (24.8%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09765625, 'data': 0.150390625, 'dataset': 0.017578125, 'modality': 0.037109375, 'encoder': 0.060546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1303

📉 LOSS BREAKDOWN:
  Total loss: 0.130345

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000734
    data_loss: 0.000383
    dataset_loss: 0.000010
    encoder_loss: 0.000288

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000080
    alphaearth_modality_loss: 0.079963
    bioclip_modality_loss: 0.001109
    phenovision_modality_loss: 0.177309

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=50
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=77
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=19
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=31

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0086, -0.0174, -0.0359, 0.0179]
    Predicted: [-0.0241, -0.0038, -0.0440, 0.0123]
    Δ (Delta): [-0.0155, 0.0136, -0.0081, -0.0056]
    MAPE: 78.36%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 7):
    Original (first 8 dims):
      [ 0.063, -0.047,  0.089, -0.002, -0.017, -0.015,  0.027,  0.087]
    Predicted:
      [ 0.09 , -0.056,  0.098, -0.014, -0.042, -0.017, -0.005,  0.045]
    Δ (Delta):
      [ 0.027, -0.009,  0.009, -0.012, -0.025, -0.002, -0.032, -0.042]
    MAPE: 128.51%, RMSE: 0.0236

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.117653

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000456
    Max norm: 0.000540
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001127
    Max norm: 0.094129
    Zero gradient ratio: 22.22%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00071433, max_change=0.00255445
  earth4d: mean_change=0.11577845, max_change=0.13652840
  multimodal: mean_change=0.00040331, max_change=0.01711365
  perceiver: mean_change=0.00113946, max_change=0.00972029

================================================================================
BATCH 182/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 132 tokens (25.8%)
    Encoder_0: 128 tokens (25.0%)
    AlphaEarth: 252 tokens (49.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.087890625, 'data': 0.162109375, 'dataset': 0.009765625, 'modality': 0.041015625, 'encoder': 0.052734375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1411

📉 LOSS BREAKDOWN:
  Total loss: 0.141084

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000702
    data_loss: 0.000374
    dataset_loss: 0.000129
    encoder_loss: 0.000139

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000111
    alphaearth_modality_loss: 0.072530
    phenovision_modality_loss: 0.206248
    bioclip_modality_loss: 0.001160

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=45
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=83
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=5
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=21
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=27

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0086, -0.0172, -0.0358, 0.0178]
    Predicted: [-0.0077, 0.0012, -0.0402, 0.0146]
    Δ (Delta): [0.0009, 0.0185, -0.0044, -0.0032]
    MAPE: 37.09%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.109, -0.057,  0.121, -0.033, -0.02 , -0.035, -0.003,  0.027]
    Predicted:
      [ 0.085, -0.039,  0.101, -0.011, -0.039, -0.014, -0.007,  0.052]
    Δ (Delta):
      [-0.024,  0.018, -0.02 ,  0.022, -0.019,  0.022, -0.004,  0.026]
    MAPE: 64.14%, RMSE: 0.0202

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.102717

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000446
    Max norm: 0.000553
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001007
    Max norm: 0.084110
    Zero gradient ratio: 20.37%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.42s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00072791, max_change=0.00231223
  earth4d: mean_change=0.11540986, max_change=0.13597111
  multimodal: mean_change=0.00039235, max_change=0.01578047
  perceiver: mean_change=0.00127169, max_change=0.01037088

================================================================================
BATCH 183/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 257 tokens (50.2%)
    BioCLIP: 124 tokens (24.2%)
    Encoder_0: 129 tokens (25.2%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09765625, 'data': 0.15234375, 'dataset': 0.0234375, 'modality': 0.044921875, 'encoder': 0.052734375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1531

📉 LOSS BREAKDOWN:
  Total loss: 0.153102

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000707
    data_loss: 0.000431
    dataset_loss: 0.000142
    encoder_loss: 0.000076

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000017
    bioclip_modality_loss: 0.001089
    phenovision_modality_loss: 0.215532
    alphaearth_modality_loss: 0.087260

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=50
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=78
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=23
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=27

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 7):
    Original:  [-0.0108, -0.0140, -0.0405, 0.0230]
    Predicted: [0.0054, -0.0150, -0.0250, 0.0202]
    Δ (Delta): [0.0163, -0.0009, 0.0155, -0.0027]
    MAPE: 51.77%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.101, -0.034,  0.108, -0.014, -0.052,  0.013,  0.004,  0.064]
    Predicted:
      [ 0.082, -0.032,  0.089, -0.012, -0.047, -0.016, -0.008,  0.061]
    Δ (Delta):
      [-0.018,  0.002, -0.019,  0.002,  0.005, -0.029, -0.012, -0.003]
    MAPE: 74.60%, RMSE: 0.0147

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.243923

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000467
    Max norm: 0.000553
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.002125
    Max norm: 0.177316
    Zero gradient ratio: 21.48%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.53s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.2x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00068488, max_change=0.00221913
  earth4d: mean_change=0.11552483, max_change=0.13618501
  multimodal: mean_change=0.00040178, max_change=0.01631600
  perceiver: mean_change=0.00116330, max_change=0.01006338

================================================================================
BATCH 184/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 138 tokens (27.0%)
    AlphaEarth: 234 tokens (45.7%)
    Encoder_0: 137 tokens (26.8%)
    Earth4D: 3 tokens (0.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09765625, 'data': 0.166015625, 'dataset': 0.0078125, 'modality': 0.0390625, 'encoder': 0.056640625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1321

📉 LOSS BREAKDOWN:
  Total loss: 0.132066

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000740
    data_loss: 0.000408
    dataset_loss: 0.000010
    encoder_loss: 0.000229

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000036
    phenovision_modality_loss: 0.185893
    alphaearth_modality_loss: 0.074804
    bioclip_modality_loss: 0.001084

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=50
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=85
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=4
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=20
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=29

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 5):
    Original:  [-0.0093, -0.0159, -0.0371, 0.0194]
    Predicted: [0.0000, -0.0159, -0.0347, 0.0216]
    Δ (Delta): [0.0094, 0.0000, 0.0024, 0.0022]
    MAPE: 29.58%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 0, seq 11):
    Original (first 8 dims):
      [ 0.111, -0.031,  0.085, -0.02 , -0.07 , -0.012,  0.014,  0.049]
    Predicted:
      [ 0.081, -0.038,  0.099, -0.018, -0.054, -0.029,  0.007,  0.054]
    Δ (Delta):
      [-0.03 , -0.007,  0.015,  0.002,  0.016, -0.017, -0.007,  0.006]
    MAPE: 38.24%, RMSE: 0.0150
    🌸 Flowering Probability:
      Original:  0.111
      Predicted: 0.081
      Δ (Delta): -0.030

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.137455

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000508
    Max norm: 0.000567
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001295
    Max norm: 0.088010
    Zero gradient ratio: 21.11%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.39s
  Backward pass: 13.02s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00067882, max_change=0.00215018
  earth4d: mean_change=0.11541421, max_change=0.13578732
  multimodal: mean_change=0.00039164, max_change=0.01470678
  perceiver: mean_change=0.00125498, max_change=0.01073008

================================================================================
BATCH 185/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    Encoder_0: 128 tokens (25.0%)
    AlphaEarth: 247 tokens (48.2%)
    BioCLIP: 136 tokens (26.6%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09765625, 'data': 0.13671875, 'dataset': 0.021484375, 'modality': 0.048828125, 'encoder': 0.0625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1499

📉 LOSS BREAKDOWN:
  Total loss: 0.149925

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000712
    data_loss: 0.000371
    dataset_loss: 0.000066
    encoder_loss: 0.000069

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000202
    phenovision_modality_loss: 0.217238
    bioclip_modality_loss: 0.001092
    alphaearth_modality_loss: 0.079288

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=50
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=70
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=11
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=25
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=32

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 7):
    Original:  [-0.0108, -0.0139, -0.0402, 0.0227]
    Predicted: [-0.0192, -0.0147, -0.0432, 0.0258]
    Δ (Delta): [-0.0084, -0.0008, -0.0030, 0.0032]
    MAPE: 26.39%

  🎨 PHENOVISION EMBEDDING RECONSTRUCTION (sample 1, seq 2):
    Original (first 8 dims):
      [ 0.075, -0.061,  0.093,  0.006, -0.058,  0.008,  0.019,  0.052]
    Predicted:
      [ 0.084, -0.045,  0.113, -0.008, -0.047, -0.035,  0.003,  0.044]
    Δ (Delta):
      [ 0.01 ,  0.016,  0.021, -0.014,  0.011, -0.043, -0.016, -0.008]
    MAPE: 123.78%, RMSE: 0.0203
    🌸 Flowering Probability:
      Original:  0.075
      Predicted: 0.084
      Δ (Delta): 0.010

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.126316

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000397
    Max norm: 0.000454
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001119
    Max norm: 0.095234
    Zero gradient ratio: 20.74%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.32s
  Backward pass: 12.97s
  Ratio (back/fwd): 5.6x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00063442, max_change=0.00200751
  earth4d: mean_change=0.11516522, max_change=0.13528730
  multimodal: mean_change=0.00038500, max_change=0.01453860
  perceiver: mean_change=0.00122512, max_change=0.01035951

================================================================================
BATCH 186/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 252 tokens (49.2%)
    Encoder_0: 126 tokens (24.6%)
    BioCLIP: 133 tokens (26.0%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.115234375, 'data': 0.154296875, 'dataset': 0.02734375, 'modality': 0.05078125, 'encoder': 0.060546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1440

📉 LOSS BREAKDOWN:
  Total loss: 0.144011

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000728
    data_loss: 0.000372
    dataset_loss: 0.000052
    encoder_loss: 0.000171

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000018
    bioclip_modality_loss: 0.001109
    phenovision_modality_loss: 0.196967
    alphaearth_modality_loss: 0.087698

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=59
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=79
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=14
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=26
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=31

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 5):
    Original:  [-0.0107, -0.0124, -0.0387, 0.0213]
    Predicted: [-0.0198, -0.0072, -0.0459, 0.0264]
    Δ (Delta): [-0.0090, 0.0052, -0.0072, 0.0050]
    MAPE: 42.04%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.068, -0.049,  0.109,  0.01 , -0.037, -0.028,  0.016,  0.066]
    Predicted:
      [ 0.086, -0.055,  0.086, -0.009, -0.043, -0.009,  0.009,  0.034]
    Δ (Delta):
      [ 0.018, -0.006, -0.022, -0.019, -0.005,  0.019, -0.006, -0.032]
    MAPE: 52.09%, RMSE: 0.0181

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.165828

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000459
    Max norm: 0.000549
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001607
    Max norm: 0.133648
    Zero gradient ratio: 22.59%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00085844, max_change=0.00317296
  earth4d: mean_change=0.11701042, max_change=0.13774817
  multimodal: mean_change=0.00042691, max_change=0.01876193
  perceiver: mean_change=0.00135114, max_change=0.01146418

================================================================================
BATCH 187/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 274 tokens (53.5%)
    Encoder_0: 124 tokens (24.2%)
    BioCLIP: 114 tokens (22.3%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.08984375, 'data': 0.158203125, 'dataset': 0.025390625, 'modality': 0.0546875, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1421

📉 LOSS BREAKDOWN:
  Total loss: 0.142123

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000729
    data_loss: 0.000374
    dataset_loss: 0.000032
    encoder_loss: 0.000190

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000022
    bioclip_modality_loss: 0.001083
    alphaearth_modality_loss: 0.071467
    phenovision_modality_loss: 0.209442

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=46
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=81
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=13
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=28
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 3):
    Original:  [-0.0088, -0.0169, -0.0360, 0.0183]
    Predicted: [-0.0147, -0.0028, -0.0424, 0.0083]
    Δ (Delta): [-0.0059, 0.0141, -0.0064, -0.0100]
    MAPE: 55.66%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 4):
    Original (first 8 dims):
      [ 0.095, -0.033,  0.124,  0.026, -0.034, -0.017,  0.022,  0.064]
    Predicted:
      [ 0.091, -0.055,  0.105, -0.02 , -0.044, -0.007,  0.016,  0.039]
    Δ (Delta):
      [-0.003, -0.022, -0.019, -0.046, -0.009,  0.011, -0.007, -0.025]
    MAPE: 52.60%, RMSE: 0.0220

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.203343

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000488
    Max norm: 0.000539
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001827
    Max norm: 0.133644
    Zero gradient ratio: 20.00%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.54s
  Backward pass: 13.03s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00080439, max_change=0.00286290
  earth4d: mean_change=0.11709850, max_change=0.13788001
  multimodal: mean_change=0.00041936, max_change=0.01937903
  perceiver: mean_change=0.00133315, max_change=0.01161185

================================================================================
BATCH 188/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 252 tokens (49.2%)
    Encoder_0: 133 tokens (26.0%)
    BioCLIP: 126 tokens (24.6%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.076171875, 'data': 0.15625, 'dataset': 0.017578125, 'modality': 0.04296875, 'encoder': 0.041015625}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1199

📉 LOSS BREAKDOWN:
  Total loss: 0.119888

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000727
    data_loss: 0.000408
    dataset_loss: 0.000018
    encoder_loss: 0.000254

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000129
    bioclip_modality_loss: 0.001066
    alphaearth_modality_loss: 0.076566
    phenovision_modality_loss: 0.159794

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=39
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=80
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=21

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 6):
    Original:  [-0.0137, -0.0085, -0.0449, 0.0269]
    Predicted: [-0.0068, -0.0136, -0.0313, 0.0026]
    Δ (Delta): [0.0069, -0.0051, 0.0135, -0.0243]
    MAPE: 57.66%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 1):
    Original (first 8 dims):
      [ 0.074, -0.033,  0.093, -0.021, -0.051, -0.015, -0.005,  0.06 ]
    Predicted:
      [ 0.088, -0.033,  0.108, -0.009, -0.048, -0.014,  0.013,  0.055]
    Δ (Delta):
      [ 0.013, -0.   ,  0.015,  0.012,  0.003,  0.001,  0.018, -0.005]
    MAPE: 61.47%, RMSE: 0.0107

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.188873

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000549
    Max norm: 0.000622
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001682
    Max norm: 0.121798
    Zero gradient ratio: 20.74%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.00s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00074296, max_change=0.00258208
  earth4d: mean_change=0.11733901, max_change=0.13795029
  multimodal: mean_change=0.00040156, max_change=0.01869084
  perceiver: mean_change=0.00117723, max_change=0.01208766

================================================================================
BATCH 189/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 264 tokens (51.6%)
    Encoder_0: 129 tokens (25.2%)
    BioCLIP: 118 tokens (23.0%)
    Earth4D: 1 tokens (0.2%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.1015625, 'data': 0.154296875, 'dataset': 0.0234375, 'modality': 0.044921875, 'encoder': 0.05859375}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1271

📉 LOSS BREAKDOWN:
  Total loss: 0.127107

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000681
    data_loss: 0.000354
    dataset_loss: 0.000028
    encoder_loss: 0.000119

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000054
    alphaearth_modality_loss: 0.074988
    bioclip_modality_loss: 0.001084
    phenovision_modality_loss: 0.176032

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=52
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=79
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=23
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=30

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 0):
    Original:  [-0.0111, -0.0113, -0.0385, 0.0214]
    Predicted: [-0.0083, -0.0180, -0.0250, 0.0065]
    Δ (Delta): [0.0028, -0.0067, 0.0135, -0.0149]
    MAPE: 47.25%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 5):
    Original (first 8 dims):
      [ 0.117, -0.043,  0.073, -0.002, -0.035, -0.011,  0.02 ,  0.039]
    Predicted:
      [ 0.094, -0.039,  0.104, -0.005, -0.049, -0.021,  0.01 ,  0.051]
    Δ (Delta):
      [-0.023,  0.005,  0.031, -0.003, -0.014, -0.011, -0.009,  0.012]
    MAPE: 50.56%, RMSE: 0.0160

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.250573

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000434
    Max norm: 0.000467
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001937
    Max norm: 0.180024
    Zero gradient ratio: 20.37%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.37s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00070298, max_change=0.00233745
  earth4d: mean_change=0.11719062, max_change=0.13724956
  multimodal: mean_change=0.00040921, max_change=0.01894690
  perceiver: mean_change=0.00139920, max_change=0.01189842

================================================================================
BATCH 190/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 142 tokens (27.7%)
    AlphaEarth: 267 tokens (52.1%)
    Encoder_0: 103 tokens (20.1%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.107421875, 'data': 0.1640625, 'dataset': 0.0234375, 'modality': 0.044921875, 'encoder': 0.046875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1277

📉 LOSS BREAKDOWN:
  Total loss: 0.127683

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000701
    data_loss: 0.000330
    dataset_loss: 0.000004
    encoder_loss: 0.000073

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000051
    bioclip_modality_loss: 0.001079
    alphaearth_modality_loss: 0.076270
    phenovision_modality_loss: 0.175931

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=55
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=84
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=12
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=23
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=24

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 1):
    Original:  [-0.0141, -0.0042, -0.0425, 0.0257]
    Predicted: [-0.0115, -0.0159, -0.0454, 0.0201]
    Δ (Delta): [0.0026, -0.0117, -0.0028, -0.0056]
    MAPE: 81.15%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 1):
    Original (first 8 dims):
      [ 0.112, -0.033,  0.118, -0.009, -0.054,  0.003, -0.003,  0.064]
    Predicted:
      [ 0.091, -0.043,  0.091,  0.   , -0.057, -0.023,  0.002,  0.042]
    Δ (Delta):
      [-0.021, -0.009, -0.026,  0.01 , -0.003, -0.026,  0.005, -0.022]
    MAPE: 148.17%, RMSE: 0.0177

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.148467

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000387
    Max norm: 0.000454
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.001346
    Max norm: 0.104260
    Zero gradient ratio: 20.74%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.39s
  Backward pass: 13.03s
  Ratio (back/fwd): 5.4x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00063207, max_change=0.00211922
  earth4d: mean_change=0.11711938, max_change=0.13713440
  multimodal: mean_change=0.00039642, max_change=0.01891748
  perceiver: mean_change=0.00116859, max_change=0.01070934

================================================================================
SUMMARY (last 10 batches)
================================================================================

📊 LOSS TRENDS:
  spacetime_loss: 0.000701 (trend: decreasing)
  data_loss: 0.000330 (trend: decreasing)
  dataset_loss: 0.000004 (trend: decreasing)
  modality_loss: 0.000051 (trend: increasing)
  encoder_loss: 0.000073 (trend: decreasing)
  phenovision_modality_loss: 0.175931 (trend: decreasing)
  bioclip_modality_loss: 0.001079 (trend: decreasing)
  alphaearth_modality_loss: 0.076270 (trend: decreasing)

🏥 GRADIENT HEALTH:
  ✅ earth4d: norm=0.000387, zero_ratio=0.00%
  ✅ multimodal_fusion: norm=0.001346, zero_ratio=20.74%

================================================================================
BATCH 191/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 255 tokens (49.8%)
    BioCLIP: 136 tokens (26.6%)
    Encoder_0: 121 tokens (23.6%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09765625, 'data': 0.16015625, 'dataset': 0.017578125, 'modality': 0.04296875, 'encoder': 0.060546875}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1512

📉 LOSS BREAKDOWN:
  Total loss: 0.151152

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000722
    data_loss: 0.000365
    dataset_loss: 0.000001
    encoder_loss: 0.000361

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000093
    bioclip_modality_loss: 0.001025
    phenovision_modality_loss: 0.220251
    alphaearth_modality_loss: 0.078763

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=50
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=82
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=22
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=31

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 2):
    Original:  [-0.0141, -0.0042, -0.0423, 0.0255]
    Predicted: [-0.0087, -0.0143, -0.0556, 0.0282]
    Δ (Delta): [0.0054, -0.0101, -0.0133, 0.0027]
    MAPE: 80.48%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 1, seq 0):
    Original (first 8 dims):
      [ 0.109, -0.04 ,  0.104, -0.011, -0.055, -0.016, -0.007,  0.048]
    Predicted:
      [ 0.089, -0.053,  0.077, -0.011, -0.054, -0.013,  0.002,  0.039]
    Δ (Delta):
      [-0.019, -0.013, -0.027,  0.   ,  0.001,  0.003,  0.009, -0.009]
    MAPE: 32.42%, RMSE: 0.0136

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.097964

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000529
    Max norm: 0.000572
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.000892
    Max norm: 0.081864
    Zero gradient ratio: 21.48%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.38s
  Backward pass: 13.03s
  Ratio (back/fwd): 5.5x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00063389, max_change=0.00199672
  earth4d: mean_change=0.11779106, max_change=0.13784747
  multimodal: mean_change=0.00038542, max_change=0.01774810
  perceiver: mean_change=0.00121788, max_change=0.01093767

================================================================================
BATCH 192/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    AlphaEarth: 235 tokens (45.9%)
    Encoder_0: 139 tokens (27.1%)
    BioCLIP: 136 tokens (26.6%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.09765625, 'data': 0.15625, 'dataset': 0.017578125, 'modality': 0.0546875, 'encoder': 0.048828125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1394

📉 LOSS BREAKDOWN:
  Total loss: 0.139352

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000665
    data_loss: 0.000360
    dataset_loss: 0.000007
    encoder_loss: 0.000062

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000063
    alphaearth_modality_loss: 0.072496
    bioclip_modality_loss: 0.001028
    phenovision_modality_loss: 0.203103

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=50
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=80
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=9
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=28
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=25

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 12):
    Original:  [-0.0090, -0.0165, -0.0361, 0.0181]
    Predicted: [-0.0064, -0.0156, -0.0398, 0.0298]
    Δ (Delta): [0.0026, 0.0009, -0.0038, 0.0117]
    MAPE: 27.13%

  🎨 ALPHAEARTH EMBEDDING RECONSTRUCTION (sample 0, seq 3):
    Original (first 8 dims):
      [ 0.146, -0.001,  0.096, -0.01 , -0.071, -0.027, -0.034,  0.048]
    Predicted:
      [ 0.078, -0.046,  0.084, -0.012, -0.052, -0.018,  0.01 ,  0.045]
    Δ (Delta):
      [-0.068, -0.046, -0.012, -0.002,  0.019,  0.009,  0.044, -0.004]
    MAPE: 1164.22%, RMSE: 0.0341

⬅️ BACKWARD PASS:
  Gradient norm (before clipping): 0.093396

📐 GRADIENT FLOW:
  earth4d:
    Mean norm: 0.000383
    Max norm: 0.000448
    Zero gradient ratio: 0.00%
  multimodal_fusion:
    Mean norm: 0.000854
    Max norm: 0.080937
    Zero gradient ratio: 24.07%

⏱️ TIMING BREAKDOWN:
  Forward pass:  2.55s
  Backward pass: 13.01s
  Ratio (back/fwd): 5.1x

🔧 PARAMETER UPDATES:
  other: mean_change=0.00061807, max_change=0.00180710
  earth4d: mean_change=0.11723435, max_change=0.13799497
  multimodal: mean_change=0.00037865, max_change=0.01682900
  perceiver: mean_change=0.00132380, max_change=0.01069677

================================================================================
BATCH 193/4351
================================================================================

📊 BATCH COMPOSITION:
  Shape: torch.Size([32, 16, 4]) (batch_size × context_window × dims)
  Encoder usage in batch:
    BioCLIP: 118 tokens (23.0%)
    AlphaEarth: 256 tokens (50.0%)
    Encoder_0: 136 tokens (26.6%)
    Earth4D: 2 tokens (0.4%)

🔄 FORWARD PASS:

[Forward Pass] Batch size: 32, Sequence length: 16
  Encoded tokens: torch.Size([32, 16, 256])
  Generated training masks
  Applied masks: {'spacetime': 0.087890625, 'data': 0.130859375, 'dataset': 0.013671875, 'modality': 0.072265625, 'encoder': 0.05078125}
  Encoded latents: torch.Size([32, 64, 128])
  Computed training losses: 0.1450

📉 LOSS BREAKDOWN:
  Total loss: 0.144958

  Universal Space Losses (256D token space):
    spacetime_loss: 0.000689
    data_loss: 0.000400
    dataset_loss: 0.000031
    encoder_loss: 0.000252

  Modality Space Losses (original dimensions via PerceiverProjector decoders):
    modality_loss: 0.000146
    bioclip_modality_loss: 0.001066
    alphaearth_modality_loss: 0.077298
    phenovision_modality_loss: 0.209287

🔍 RECONSTRUCTION EXAMPLES:
  DEBUG - Output keys: ['latents', 'loss', 'reconstructed', 'original', 'masks', 'component_losses', 'modality_reconstructions']
  DEBUG - Reconstructed shape: torch.Size([32, 16, 256])
  DEBUG - Original shape: torch.Size([32, 16, 256])
  DEBUG - Mask types: ['spacetime', 'data', 'dataset', 'modality', 'encoder']
  DEBUG - spacetime mask: shape=torch.Size([32, 16]), any_masked=True, count=45
  DEBUG - data mask: shape=torch.Size([32, 16]), any_masked=True, count=67
  DEBUG - dataset mask: shape=torch.Size([32, 16]), any_masked=True, count=7
  DEBUG - modality mask: shape=torch.Size([32, 16]), any_masked=True, count=37
  DEBUG - encoder mask: shape=torch.Size([32, 16]), any_masked=True, count=26

  📍 SPACETIME (X,Y,Z,T) RECONSTRUCTION (sample 0, seq 10):
    Original:  [-0.0141, -0.0043, -0.0426, 0.0256]
    Predicted: [-0.0043, -0.0090, -0.0277, 0.0166]
    Δ (Delta): [0.0098, -0.0047, 0.0149, -0.0090]
    MAPE: 62.32%

  🎨 BIOCLIP EMBEDDING RECONSTRUCTION (sample 0, seq 2):
    Original (first 8 dims):
      [ 0.065, -0.033,  0.085, -0.017, -0.064, -0.008, -0.016,  0.027]
    Predicted:
      [ 0.082, -0.055,  0.102, -0.02 , -0.05 , -0.012,  0.008,  0.053]
    Δ (Delta):
      [ 0.016, -0.021,  0.017, -0.004,  0.014, -0.004,  0.025,  0.026]
    MAPE: 56.77%, RMSE: 0.0179

⬅️ BACKWARD PASS:
